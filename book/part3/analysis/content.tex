\chapter{Introduction
  \Author{M. Schordan \andAuthor F. Rastello}}
\label{chapter:analysis}
\inputpath{part3}{analysis}
\inputprogress

The goal of this chapter is to provide an overview of the benefits of
SSA form for program analysis and optimization. We illustrate how SSA
form makes an analysis more convenient because of its similarity to
functional programs. Technically, the def-use chains explicitly
expressed through the SSA graph, but also the static-single
information property, is what makes SSA so convenient.

\ifconstantprop

\subsection{The Propagation Engine}

There are several analyses that propagate information through the SSA
graph.  The Chapter \ref{chapter:constant_propagation_is_easier} gives a
general description of the mechanism. It shows how SSA form
facilitates the design and implementation of analyses equivalent to
traditional data flow analyses and how the SSA property allows to
reduce analysis time and memory consumption.  The presented {\em
  Propagation Engine} is an extension of the well-known approach by
Wegman and Zadeck for sparse conditional constant propagation. The
basic algorithm is not limited to constant propagation and allows to
solve a large class of data-flow problems more efficiently than the
iterative work list algorithm for solving data flow equations. The
basic idea is to directly propagate information computed at the unique
definition of a variable to all its uses.

\fi

As already mentioned in Chapter~\ref{chapter:properties_and_flavors},
SSA form can come in different flavors. The vanilla one is strict SSA, or said
equivalently, SSA form with the dominance property. The most common SSA
construction algorithm exploits this dominance property by two means:
First it allows to compute join sets for $\phi$-placement in a very
efficient way using the dominance frontier.  Second it allows variable
renaming using a folding scheme along the dominance tree.  The notion
of dominance and dominance frontier are two structural properties that
make SSA form singular for compiler analysis and transformations.

Those two aspects are illustrated in this part through two chapters:
Chapter~\ref{chapter:ssa_tells_nothing_of_liveness} shows how loop
nesting forest and dominance property can be exploited to devise very
efficient liveness analysis; Chapter~\ref{chapter:pre_not_helped}
shows how the dominance frontier that allows to insert a minimal number of
\phifun\ for SSA construction can also be used to minimize redundant
computations.

\paragraph{Faster Liveness Analysis with SSA form}
A data-flow analysis iterates potentially up to as many times as the
maximum loop depth of a given program until it stabilizes and
terminates. In contrast, the properties of SSA form allow to
accelerate liveness analysis without the requirement of any iteration
to reach a fixed point: it only requires at most two passes over the
CFG. Also an extremely simple liveness check is possible by providing
a query system to answer questions such as ``is variable $v$ live at
location $q$?''.

We first present an algorithm for computing liveness information on
reducible CFGs and continue with deriving an algorithm that can also
be applied to irreducible graphs. This can be achieved by transforming
an irreducible graph into a reducible graph such that the liveness in
both graphs is equivalent. In practice we do not modify the graph, but
adapt the algorithm for reducible graphs such that it simulates the
modification of some edges on the fly.

\paragraph{Relation between Partial Redundancy Elimination and SSA form}

The elimination of redundant computations is an important compiler
optimization. In chapter \ref{chapter:pre_not_helped} we show that the
dominance frontiers that define where \phifuns\ are inserted
in SSA form also form the ``frontier'' between full redundancy and
partial redundancy. A computation is {\em fully redundant} if it has
also occurred earlier regardless of control flow, a computation is {\em
  partially redundant} if it is has occurred earlier only on certain
paths. Following the program flow, once we are past the dominance
frontiers, any further occurrence of a redundant expression is
partially redundant, whereas any occurrence before the dominance
frontier is fully redundant. The difference for the optimization is
that fully redundant expressions can be deleted, whereas for
(strictly) partially redundant expressions insertions are required to
eliminate the redundancy. Thus, since partial redundancies start at
dominance frontiers they are related to SSA's $\phi$ statements. In
chapter \ref{chapter:pre_not_helped} on SSAPRE we present an
algorithm, named SSAPRE, performing partial redundancy elimination
(PRE) by efficiently taking advantage of the use-def information
inherent in its input conventional SSA form. SSAPRE builds redundancy
edges between redundant expressions (in SSA form) and has the effect
of factoring the redundancy edges at merge pointer in the CFG. The
resulting {\em factored redundancy graph (FRG)} can be regarded as the
SSA form for expressions. We also discuss speculative code motion
which suppresses redundancy in some path at the expense of another
path where the computation is added but the result is unused.

This PRE algorithm is not capable of recognizing redundant
computations among lexically different expressions that yield the same
value. Therefore we also discuss redundancy elimination based on value
analysis, which can determine redundancy whenever two computations
yield the same value. A technique for such a value analysis is {\em
  value numbering}. In this case the benefit of SSA form is that it
enables value numbering to be easily extended to the global scope,
called {\em global value numbering}, because each SSA version of a
variable corresponds to at most one static value for the variable.

\paragraph{Determining Induction Variables}

Chapter \ref{chapter:loop_tree} illustrates how capturing properties
of the SSA graph itself (circuits) can be used to determine a very
specific subset of program variables: induction variables. The
induction variable analysis is based on the detection of self
references in the SSA representation and the extraction of the loop
tree, which can be performed on the SSA graph as well. The presented algorithm
translates the SSA representation into a representation of polynomial
functions, describing the sequence of values that SSA variables hold
during the execution of loops. The number of iterations is computed as
the minimum solution of a polynomial inequality with integer
solutions, also called Diophantine inequality.
