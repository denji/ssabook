\section{Introduction}
Chapter~\ref{chapter:classical_construction_algorithm} provides a basic algorithm for destructing SSA that suffers from several limitations and drawbacks: first, it works under implicit assumptions that are not necessarily fulfilled at machine level; second, it must rely on subsequent phases to remove the numerous copy operations it inserts; finally, it increases subsequently the size of the intermediate representation, thus making it not suitable for just-in-time compilation.   

{\bf Correctness}
SSA at machine level complicates the process of destruction that can potentially lead to bugs if not performed carefully. The algorithm described in Section~\ref{sec:classical_construction_algorithm:destruction} involves the splitting of every critical edges\index{critical edge}. Unfortunately, because of specific architectural constraints, region boundaries, or exception handling code, the compiler might not allow the splitting of a given edge. Fortunately, as we will see further, this obstacle can easily be overcame. But then it becomes essential to be able to append a copy operation at the very end of a basic block which might neither be possible. Also, care must be taken with duplicated edges, i.e., when the same basic block appears twice in the list of predecessors.
This can occur after control flow graph structural optimizations like
dead code elimination or empty block elimination.
In such case, the edges should be considered as critical and then split.

SSA imposes a strict discipline on variable naming: every ``name'' must be associated to only one definition which is obviously most of the time not compatible with the instruction set of the targeted architecture. As an example, a two-address mode instruction, such as auto-increment ($x=x+1$) would enforce its definition to use the same resource than one of its arguments (defined elsewhere), thus imposing two different definitions for the same temporary variable. This is why some prefer using, for SSA construction, the notion of versioning\index{variable version} in place of renaming. Implicitly, two versions of the same original variable should not interfere, while two names can. The former simplifies the SSA destruction phase, while the latter simplifies and allows more transformations to be performed under SSA. Apart from dedicated registers for which optimizations are usually very careful in managing there live-range, register constraints related to calling conventions or instruction set architecture might be handled by the register allocation phase. However, as we will see, enforcement of register constraints impacts the register pressure as well as the number of copy operations. For those reasons we may want those constraints to be expressed earlier (such as for the pre-pass scheduler), in which case the SSA destruction phase might have to cope with them.

{\bf Code quality}
The natural way of lowering \phifuns and expressing register constraints is through the insertion of copies (when edge-splitting is not mandatory as discussed above). If done carelessly, the resulting code will contain many temporary-to-temporary copy operations. In theory, reducing the amount of these copies is the role of the coalescing during the register allocation phase.
A few memory and time-consuming existing coalescing heuristics mentioned in Chapter~\ref{chapter:register_allocation} are quite effective in practice. The difficulty comes both from the size of the interference graph (the information of colorability is spread out) and the presence of many overlapping live-ranges that carry the same value (so non-interfering).
Coalescing can also, with less effort, be performed prior to the register allocation phase. As opposed to a (so-called conservative) coalescing during register allocation, this aggressive coalescing would not cope with the interference graph colorability. As we will see, strict SSA form is really helpful for both computing and representing equivalent variables. This makes the SSA destruction phase the good candidate for eliminating (or not inserting) those copies.

{\bf Speed and Memory Footprint} 
The cleanest and simplest way to perform SSA destruction with good code quality is to first insert copy instructions to make the SSA form conventional, then take advantage of the SSA form to run efficiently aggressive coalescing (without breaking the conventional property), before eventually renaming \phiwebs\index{\phiweb} and getting rid of \phifuns. Unfortunately this approach will lead, in a transitional stage, to an intermediate representation with a substantial number of variables: the size of the liveness sets and interference graph classically used to perform coalescing become prohibitively large for dynamic compilation. To overcome this difficulty one can compute liveness and interference on demand which, as we already mentioned, is made simpler by the use of SSA form. Remains the process of copy insertion itself that might still take a substantial amount of time. To fulfill memory and time constraints imposed by just-in-time compilation, one idea is to \emph{virtually} insert those copies, and only \emph{effectively} insert the non-coalesced ones.    

This chapter addresses those three issues: handling of machine level constraints, code quality (elimination of copies), and algorithm efficiency (speed and memory footprint). The layout falls into three corresponding sections.
