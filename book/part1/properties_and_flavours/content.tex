%% \chapter{Properties and flavours \Author{P. Brisk}\\\progressbar[0.4\textwidth]{writing}{80}}
\chapter{Properties and flavours \Author{P. Brisk}}
\inputprogress
\label{chap:properties_and_flavours}


\section{Preliminaries}

Any operation of the form $v \leftarrow \ldots$ that assigns a value
to variable $v$ is called a \emph{definition} of $v$. Any operation
of the form $\ldots \leftarrow v$ that reads a value of $v$ is called
a \emph{use} of $v$. One of the key properties of SSA Form is that
each variable is defined only once; however, renaming each variable 
alone is insufficient, and $\phi$-functions are needed to merge
the disparate redefinitions of each variable. The remaining 
sections of this chapter provide the theoretical foundations that
describe precisely how $\phi$-functions are inserted, and then
derives important properties of the SSA Form that result. 

\section{Dominance}


The concepts of \emph{dominance}, introduced in this section, and 
\emph{post-dominance}, introduced in the following section, form the
foundations of a collection of compiler techniques called 
\emph{control flow analysis}. Intuitively dominance and post-dominance
respectively tell us which basic blocks are \emph{guaranteed} to
execute before or after others. Historically, this information has
been used to identify hierarchies of nested loops. Although this
problem may seem trivial, given the prevalence of programming 
constructs to express loops, such as \emph{for}, \emph{while} and
\emph{do-while}, the problem becomes significantly more complicated
when trying to account for \emph{irreducible loops}, which can be
constructed using arbitrary \emph{goto} statements
~\cite{RamlingamSep02}. Our interest, however, is SSA Form
rather than loop analysis; as we shall see, extensions to these
basic definitions, in the sections that follow, are necessary to
understand the criteria for instantiating $\phi$-functions 
during the construction of SSA Form. 

In a CFG, basic block $n_{1}$ \emph{dominates} basic block $n_{2}$
if every path in the CFG from the entry point to $n_{2}$ includes
$n_{1}$. By convention, every basic block in a CFG dominates itself. Basic 
block $n_{1}$ \emph{strictly dominates} $n_{2}$ if $n_{1}$ dominates
$n_{2}$ and $n_{1} \neq n_{2}$. We use the symbols $n_{1} dom n_{2}$
and $n_{1} sdom n_{2}$ to denote dominance and strict dominance 
respectively.

The \emph{immediate dominator} of a basic block $n_{2}$, denoted by
$idom(n_{2})$, is the basic block $n_{1}$ such that $n_{1} sdom n_{2}$, and
there does not exist any basic block $n_{3}$ such that 
$n_{1} sdom n_{3} sdom n_{2}$. Intuitively, this means that $n_{1}$ is the
closest basic block to $n_{2}$ that strictly dominates $n_{2}$. 
Every basic block other than the entry point has exactly one immediate 
dominator. 

The \emph{dominator tree} is a data structure that links every basic block
to its immediate dominator. The entry point of the CFG, itself a basic block,
is the root of the dominator tree. If $N$ is the set of basic blocks in 
the CFG and $r$ represents the entry point, then the
dominator tree is the graph $T = (N, E_{T})$, where 
$E_{T} = \{(n, idom(n))| n \in N \wedge n \neq r \}$. In other words,
each edge in the dominator tree points from each basic block, other than $r$,
to its immediate dominator. 

\section{Post-dominance}

Post-dominance is similar in principle to dominance, as we will see below.
In fact, all of the post-dominance information described below can be 
constructed using the basic methods required to compute dominance 
information. It suffices to reverse every edge in the CFG, and then
swap the roles of the entry and exit node. The dominance information
computed for the resulting CFG is wholly equivalent to the post-dominance
information for the original CFG. 

In a CFG, basic block $n_{2}$ \emph{post-dominates} basic block $n_{1}$
if every path in the CFG from $n_{1}$ to the exit point includes
$n_{2}$. By convention, every basic block in the CFG post-dominates itself. 
Basic block $n_{2}$ \emph{strictly post-dominates} $n_{1}$ if $n_{2}$ 
post-dominates $n_{1}$ and $n_{2} \neq n_{1}$. We use the symbols 
$n_{2} pdom n_{1}$ and $n_{2} spdom n_{1}$ to denote post-dominance and 
strict post-dominance respectively.

The \emph{immediate post-dominator} of a basic block $n_{1}$, denoted by
$ipdom(n_{1})$, is the basic block $n_{2}$ such that $n_{2} spdom n_{1}$, and
there does not exist any basic block $n_{3}$ such that
$n_{2} spdom n_{3} spdom n_{1}$. Intuitively, this means that $n_{2}$ is the
closest basic block to $n_{1}$ that strictly post-dominates $n_{1}$.
Every basic block other than the exit point has exactly one immediate 
post-dominator.

The \emph{post-dominator tree} is a data structure that links every
basic block to its immediate post-dominator. The exit point of the CFG is
the root of the post-dominator tree. If $N$ is the set of basic blocks
in the CFG, and $t$ is the unique exit point, then the post-dominator 
tree is the graph $P = (N, E_{P})$, where
$E_{P} = \{(n, ipdom(n)), n \in N \wedge n \neq t\}$. In other words,
each edge in the post-dominator tree points from each basic block, other 
than $t$, to its immediate post-dominator.  

\section{Join Sets and Dominance Frontiers}

Let $n_{1}$ and $n_{2}$ be distinct basic blocks in a CFG. A basic block
$n_{3}$, which may or may not be distinct from $n_{1}$ or $n_{2}$, is 
a \emph{join node} of $n_{1}$ and $n_{2}$ if there exist at least two
non-empty paths, i.e., paths containing at least one CFG edge, from 
$n_{1}$ to $n_{3}$ and from $n_{2}$ to $n_{3}$, respectively, such that
$n_{3}$ is the only basic block that occurs on both of the paths. In
other words, the two paths converge at $n_{3}$ and no other CFG node. 
Given a set $S$ of basic blocks, $n_{3}$ is a join node of $S$ if it
is the join node of at least two basic blocks in $S$. The set of join
nodes of set $S$ is denoted by the set $\J(S)$. 

Intuitively, a join set corresponds to the placement of $\phi$-functions.
In other words, if $n_{1}$ and $n_{2}$ are basic blocks that both
contain the definition of a variable $v$, then we ought to instantiate
$\phi$-functions for $v$ at every basic block in $\J(n_{1}, n_{2})$. 
Generalizing this statement, if $S$ is the set of basic blocks containing
definitions of $v$, then $\phi$-functions should be instantiated in
every basic block block in $\J(S)$. 

That being said, we still require an efficient method to compute join sets. 
In principle, we may require join sets for every subset of basic blocks
in the CFG, although in principle, we are likely to require fewer. For 
this reason, we will turn, briefly, to an alternative structure based
on dominance, rather than paths in a graph.

The \emph{dominance frontier} of a basic block $n_{1}$ in a CFG,
denoted $DF(n_{1})$, is the set of basic blocks $X$, such that
$n_{1}$ \emph{does not} strictly dominate $X$, but \emph{does} 
dominate at least one predecessor block of $X$. The dominance 
frontier of a set $S$ of basic blocks, denoted $DF(S)$, is the union
of the dominance frontiers of the elements of $S$. 

The following section will make a connection between join sets
and dominance frontiers.

\section{Iterated Join Sets and Dominance Frontiers}

Let $S$ be a set of basic blocks, and let $\J^{0}(S) = \J(S)$. 
Now, let us recursively define 
$\J^{i}(S) = \J(S \cup \J^{i \minus 1}(S))$. By
construction, it is straightforward to see that
$\J^{i}(S) \supseteq \J^{i-1}(S)$ for all $i$, as there is
no process to facilitate vertex removal. If we compute these sets
iteratively, we will eventually converge to a situation where
$\J^{i}(S) = \J^{i-1}(S)$ in a finite number of steps, 
as each CFG contains a finite number of basic blocks. The resulting
set, denoted by $\J^{+}(S)$, is called the 
\emph{iterated join set} of $S$. 

We can do something similar with dominance frontiers. Given a set
$S$ of basic blocks, let $DF^{0}(S) = DF(S)$, and let
$DF^{i}(S) = DF(S \cup DF^{i-1}(S))$. Once again, we can iteratively
compute $DF^{i}(S)$ from $DF^{i-1}(S)$, stopping when the two sets
are equal. The resulting set, denoted by $DF^{+}(S)$ is called
the \emph{iterated dominance frontier} of $S$. 

There is, in fact, a connection between iterated join sets and 
iterated dominance frontiers. Let $X$ be any set of CFG nodes
that contains the CFG entry node. Cytron et al.~\cite{CytronOct91}
proved that $\J^{+}(X) = DF^{+}(X)$. Weiss~\cite{WeissJun92}
proved a slightly stronger version, that $\J(X) = DF^{+}(X)$.
Weiss also conjectured that $\J^{+}(S) = \J(S)$ for any 
set of CFG nodes $S$, and this conjecture was formally proven
by Wolfe~\cite{WolfeJul94}. 

For reasons that we will discuss in greater detail below, each
variable in an SSA Form program has an \emph{implicit} definition
at the CFG entry point. Based on the results above, either the
join set or iterated dominance frontier could be used, in principle
to instantiate $\phi$-functions. Cytron et al.~\cite{CytronOct91} 
compute iterated dominance frontiers using a worklist algorithm. 
As noted by Wolfe~\cite{WolfeJul94}, join and iterated join sets 
are too complicated to compute directly, and no algorithms exist
that call for their explicit use, outside of any context where
iterated dominance frontiers suffice.  

Intuitively, dominance frontiers are not enough, which is why
iterated dominance frontiers are needed. Ignoring the subtle
differences between dominance frontiers and join sets, consider
a variable $v$ defined in basic blocks $n_{1}$ and $n_{2}$. Initially,
we will instantiate a $\phi-$function for $v$ in some basic block
$n_{3} \in DF(n_{1}, n_{2})$; however, before renaming, this
$\phi$-function itself is a new definition of $v$, so we will need
to instantiate additional $\phi$-functions in $DF(n_{3})$ as well.
Taken ad-infinitum, this line of reasoning is precisely why we
use iterated dominance frontiers instead of dominance frontiers. 

\section{Split Sets and (Iterated) Post-dominance Frontiers}

We have already noted the existence of a duality between the 
dominance and post-dominance relations. The \emph{dual} of
a join set is called a \emph{split set}: given two distinct
basic blocks $n_{2}$ and $n_{3}$, basic block $n_{1}$ belongs
to their split set, denoted $\S(n_{1}, n_{2})$ if there 
are non-empty paths from $n_{3}$ to $n_{1}$ and $n_{2}$
respectively, such that the only basic block common to both
paths is $n_{3}$. Clearly, if all of the CFG edges are
reversed, $n_{3}$ would belong to the join set of $n_{1}$
and $n_{2}$. Based on Wolfe's~\cite{WolfeJul94} proof,
there is no need to define an \emph{iterated split set}, 
as it is equivalent to the split set. 

Similarly, the dominance fronter can be generalized 
to the \emph{post-dominance frontier} by replacing the
dominance with the post dominance relation in the definition.
For a set of basic blocks, $S$, we let $PDF(S)$ denote
the post-dominance frontier of $S$. Furthermore, we can
define an \emph{iterated post-dominance frontier} in a 
manner that is exactly similar to the iterated dominance
frontier. The iterated post-dominance frontier of a set
$S$ of vertices is denoted $PDF^{+}(S)$. 

\section{Philip's comment}

From the outline, I budgeted 2 pages for the text of
the preceding sections, and 2 pages for illustrations.

\section{SSA Form, Def-Use, and Use-Def Chains}

A procedure is defined to be in SSA Form if it satisfies
two properties. Firstly, each variable can only be defined
once. This property is easily achieved: assuming that varible
$v$ is defined $k$ times in the procedure, rename each of the
definites to $v_{1}, v_{2}, \cdots, v_{k}$ respectively. 
The second property is that each use of $v$ must now be
renamed to correspond to precisely one definition. Without
loss of generality, if there is a use of $v$ in a basic block
that belongs to the join set of two (or more) definitions
$v_{i}$ and $v_{j}$, then the second property is implicitly
unsatisfied: renaming this use of $v$ to be a use of $v_{i}$ or
$v_{j}$ will not maintain correct program semantics. In order
to rectify the situation, a $\phi$-function must be instantiated
to merge the definitions of $v_{i}$ and $v_{j}$ into a new 
variable $v_{l}$, and the use of $v$ is replaced with a use
of $v_{l}$. 

Note: insert a figure here to illustrate. 

Two important data structures that are used in compilers are
\emph{Definition-Use (DU) Chains} and 
\emph{Use-Definition (UD) Chains}. A definition $D$ of variable
$v$ reaches a use $U$ of $v$ if there is a path in the CFG
from $D$ to $U$ that does not include another definition of $v$. 
A DU Chain connects $D$ to all uses of $v$ that are reachable
from $D$; a UD Chain connects a use $U$ of $v$ to all of the 
definition of $v$ from which $U$ is reachable. 

Under SSA Form, as mentioned above, each variable is defined 
once and each use corresponds to a single definition. Therefore,
there is exactly one DU Chain per variable (the definition points
to all of the uses), and one UD Chain per use, which points to the
single definition. Thus, the UD chains are explicit, and do not
need to be represented.

Note: Insert a figure here to illustrate what happens to DU
and UD chains by the conversion to SSA Form.  

\section{Dead Code Elimination Under SSA}

Note: In an email on October 19, 2009, Fabrice suggested that
I include this section. If I choose to write it and include it
here, I will base the discussion on The SSA DCE algorithm from
Robert Morgan's textbook, which was used in MachSUIF (among 
other places). Alternativly, I could use the DCE algorithm
from the classic Cytron et al. 1991 paper. 

I have not yet written this section, because I am not sure
whether an introductory chapter is the appropriate place
to insert a discussion of DCE, or whether it is better moved
to a chapter on SSA-based optimizations. 

\section{SSA with dominance property}

A procedure is defined to be \emph{strict} if every variable
is defined before it is used along every path from the entry
to exit point~\cite{BudimlicJun02}; otherwise, it is \emph{non-strict}. 
Some languages, such as Java, impose strictness as part of the language
definition; others, such as C/C++, impose no such restrictions. 

It is possible to define SSA Form in a manner that ensures strictness,
even if the original procedure itself is non-strict; it is also possible
to define SSA Form in a way that does not ensure strictness. The designer
of an SSA-based compiler must determine which definition is most appropriate.

Any existing SSA construction algorithm can ensure strictness: it suffices
to add a pseudo-definition of each variable at the entry point of the 
procedure. This pseudo-definition does not affect the live range of the
variable: it is only used to guide the placement of $\phi$-functions.
Any SSA construction algorithm that uses iterated dominance frontiers,
as we will shortly show, uses this implicit definition. 

Let $v_{0}$ denote the pseudo-definition corresponding to variable $v$ in
a pre-SSA procedure. Any instruction that uses $v_{0}$ indicates a likely
programmer error. The instruction will use a non-initialized variable; in
the general case, this leads to unpredictable program behavior whenever
this instruction is executed.
Similarly, a $\phi$-function may also contain $v_{0}$ as a parameter. 
If the corresponding path is taken into the basic block containing the
$\phi$-function, then an uninitialized variable (denoted by the pseudo-
definition) will be copied to the variable defined by the $\phi$-function.
As these programs are still legal, depending on the language definition,
the best that a compiler can do is to present a warning to the user in
order to alert him or her of the situation. 

Let $S_{v}$ be the set of basic blocks containing definitions of variable 
$v$, and let $n_{0}$ denote the entry point of the CFG. Conceptually, 
non-strict SSA Form can be constructed by placing $\phi$-functions at the
entry points of each basic block belonging to $J(S_{v}) = J^{+}(S_{v})$, 
i.e., using join sets. Using iterated dominance frontiers, in contrast, 
ensures strict SSA Form, since $DF^{+}(S_{v}) = J(S_{v} \cup n_{0})$.  

As there are no known efficient algorithms to compute join sets, the
most straightforward way to compute non-strict SSA Form is to first
built strict SSA Form and then remove $\phi$-functions that are 
otherwise extraneous. Consider, for example, a $\phi$-function that
defines variable $v_{j}$: one of the parameters is another definition of
$v$, denoted by $v_{i}$, while \emph{all} of the remaining parameters 
are $v_{0}$: the implicit definition. It is possible, in this case, to
eliminate this $\phi$-function by replacing all uses of $v_{j}$ with
uses of $v_{i}$ instead. Although this destroys the dominance property,
it does reduce the number of $\phi$-functions in the program. 

Note: Insert an example here, as described in an email from Fabrice:
'example of a double diamond with a def and used on the left and b def 
and used on the right: no interference'. I also have some decent examples
from a paper published at IWLS 2007 (no official proceedings). 

There are certainly situations where one would want the dominance property
as well. Most of these situations exploit properties relating to liveness
and interference, which are briefly outlined in the following sections 
of this chapter, and are described in much greater detail in subsequence
chapters. 

It is also important to note that many program transformations may
cause strict SSA Form to become non-strict. Examples of such transformations
include copy propagation and spilling. Techniques to convert non-strict
to strict SSA Form without explicitly destroying and rebuilding SSA 
Form will be discussed in Section ...

(requires coordination with Sebastian).
(possibly put an example here showing how copy propagation destroys
strictness property; spilling works too). 

\section{Minimal SSA Form}

\emph{Minimal SSA Form} is computed in a straightforward manner,
given the preceding discussion. Let $S_{v}$ be the set of
basic blocks containing definitions of variable $v$. Then
$\phi$-functions for $v$ are instantiated in every basic
block in $DF^{+}(S_{v})$. This is followed by a pass over
the program that renames each variable so that it has a
unique definition, and renames each use to correspond
precisely to one definition. Detailed pseudocode for these
procedures can be found in ... 

As we will soon see, Minimal SSA Form is hardly minimal, as
other algorithms that insert considerably fewer $\phi$-functions
have been developed; however, Minimal SSA Form does insert
fewer $\phi$-functions compared to the naive alternative,
which is to instantiate a $\phi$-function for each variable
at the entry point of each basic block in the program that
has more than one predecessor. 

\section{Semi-pruned SSA Form}

\emph{Semi-pruned SSA From}, introduced by Briggs et al.
~\cite{BriggsJul98}, was based on the observation that
many variables are never used outside of the basic blocks
where they are defined. Consequently, there is no need 
to instantiate any $\phi$-functions for any of them. 
Specifically, if every use of variable $v$ occurs after
a definition of $v$ in the same basic block, then no
$\phi$-functions are necessary, although each definition
and use must still be renamed. All of these variables
are filtered out, and the algorithm to construct
Minimal SSA Form is then applied to the remaining 
variables. For many applications, the vast majority 
of variables are filtered, which significantly reduces
the number of $\phi$-functions that are instantiated,
and avoids the corresponding increase in the size of the
symbol table to accommmodate variables that would otherwise
be defined by $\phi$-functions. 


\section{Liveness and Liveness Analysis}

Now, we turn our attention to variables and their lifetimes. By
exploiting this information, we can further reduce the 
number of $\phi$-functions instantiated during the conversion
to SSA Form. 

Variable $v$ is \emph{live} at some point $p$ in a program if:
\begin{enumerate}
\item There is a path from some definition of $v$ to $p$, and
\item There is a use of $v$ reachable from $p$, meaning that
there is a path from from $p$ to the use of $v$ that does not
include a definition of $v$. 
\end{enumerate}

The \emph{live range} of a variable is the set of points
at which it is live. A variable must reside in a storage 
location, in a register, in memory or both, throughout
its live range.  

\emph{Liveness Analysis} is the process of computing the
live range of each variable in a procedure. The bulk of
the work is performed by an iterative data flow analysis
that computes either the \emph{Live-In} or the 
\emph{Live-Out} sets of each basic blocks, i.e., the set
of variables that are live at the entry or exit of
each basic block in the program. Given this information,
a linear traversal of each basic block yields the 
exact set of variables that are live at each program
point. In general, the sets of program points that
constitute each variable's live range are not computed.

Here, we will show how to compute the Live-Out set of
each basic block. $LIVEOUT[n]$ will denote the Live-Out
set of basic block $n$. Each of the sets is represented
as a bit-vector, with one bit corresponding to each
variable. If $V$ is the set of variables in the procedure,
then each bit-vector contains $|V|$ bits. To simplify
notation, we will associate each variable $v_{i}$ with
its index $i$, so $LIVEOUT[n][i] = 1$ if $v_{i}$ is live
at the exit point of basic block $n$, and 
$LIVEOUT[n][i] = 0$ otherwise. 

To perform liveness analysis, we need two other sets for
each basic block. $UEVAR[n]$ is defined to be the set of
\emph{upwards exposed} variables in basic block $n$, i.e.,
those variables that are used in $n$, but do not have a 
definition preceding them in $n$. Clearly, the variables
belonging to $UEVAR[n]$ are live at the entry point of $n$;
Liveness analysis is a backward dataflow analysis, as
variable lifetimes are propagated backwards from
$UEVAR[n]$ and into the predecessors of $n$. 

$VARKILL[n]$ contains the set of variables that are defined
in basic block $n$. As liveness analysis is a backward
data flow analysis, the backward propagation of liveness
information ends with the definition of a variable. 
The $VARKILL$ sets effectively stop the backward 
propagation of variables that are live at their definition
points.

Initially, the sets $LIVEOUT[n]$ are empty for each basic 
block, and a linear traversal of each basic block yields
the sets $UEVAR[n]$ and $VARKILL[n]$. Once the initial sets
are established, the following data flow equations are
repeatedly computed for each basic block $n$; this is 
called an \emph{iteration}. The process repeats until an 
entire iteration fails to change any of the $LIVEOUT$ sets.
At this point, stability is reached, and the information
contained in the $LIVEOUT$ sets is wholly accurate.

[Insert liveness equations here]

The results of liveness analysis are generally used in
three contexts: the construction of Pruned SSA Form,
as described in the following section; register allocation;
and the translation out of SSA Form. 

\section{Pruned SSA Form}

Minimal SSA Form does not take variable liveness information
into account when instantiating $\phi$-functions. Semi-pruned
SSA recognizes that many variables are never live across
the boundaries of the basic blocks that contain their 
definition, and does not instantiate any $\phi$-functions
for these variables. Under both Minimal and Semi-pruned
SSA Form, the $\phi$-functions that are instantiated are
done so on the basis of iterated dominance frontiers alone,
and do not account for any additional liveness information
about the corresponding variable. Therefore, $\phi$-functions
are often instantiated for variables at the entry point of
basic blocks where the variable is not actually live. 
\emph{Pruned SSA Form}, in contrast, requires that 
$\phi$-functions are only instantiated for a variable $v$
at the entry point of basic blocks where $v$ is live
~\cite{ChoiJan91}.  

There are two possible methods to construct Pruned SSA Form.
The direct method is to perform liveness analysis upfront, and
check whether a variable is live at the entry point of a 
basic block before instantiating a $\phi$-function for that
variable. This approach is straightforward, but requires
performing liveness analysis upfront. 

Briggs et al.~\cite{BriggsJul98}, who introduced Semi-pruned
SSA Form, compared the runtimes of constructing Minimal SSA,
Semi-pruned SSA, and the direct method of constructing Pruned
SSA Form. Minimal SSA Form instantiated an exorbitant number
of $\phi$-functions, and, as a consequence, it had the slowest
runtime. Semi-pruned SSA Form was the fastest to construct,
because Pruned SSA Form required liveness analysis in advance.
Although Semi-pruned SSA Form instantiated significantly 
more $\phi$-functions than Pruned SSA Form, the overhead of
liveness analysis was significantly greater than the overhead
of instantiating the extra $\phi$-functions. 

It is also possible to construction Pruned SSA by first
building Semi-Pruned SSA and then using a simplified dead code
elimination algorithm on $\phi$-functions to convert the
resulting program into Pruned SSA Form; in fact, a more
algorithm that eliminates dead code as well as dead $\phi$
functions also suffices. To the best of our knowledge, such
an algorithm has not been formally published; however, Choi
et al.~\cite{ChoiJan91} suggested that such an algorithm
may exist, and the method has even been patented
[I'm not sure how to cite this]. As long as the runtime of
the dead code elimination step plus the cost of instantiating
the extra $\phi$-functions, which are later removed, is 
cheaper than performing liveness analysis, this latter
method will run faster. 

\section{Live Range Insersection and Interference}
\label{part1:ssaprop:subtrees}

Liveness information also plays an important role in
register allocation. Any variable must reside in a 
storage location: a register, in memory, or both, at each
point when it is live; otherwise, its value will be lost.
The register allocator, therefore, must partition the
variables between register and memory at each point in
the program, and make sure that at most one variable is
allocated to each register at any point. 

The live ranges of two variables \emph{intersect} if
there is at least one point in a procedure where both
variables are live. In general, two variables whose live
ranges intersect cannot share the same storage location;
however, there is one exception: Chaitin et al.
~\cite{Chaitin81,ChaitinJun82} suggested that two variables
whose live ranges overlap, but are known to always hold
the same value, can share the same register; unfortunately,
testing whether or not two computations produce the same
result is undecideable, as it is a generalization of
the \emph{Halting Problem}. Therefore, Chaitin's more
general notion of live range \emph{interference} is often 
simplified to live range intersection, or intersection
with provisions to account for constant values. 

Many register allocators build an auxiliary data structure
called an \emph{interference graph} to assist with the
allocation process. An interference graph is denoted by
the tuple $G = (V, E, A)$, where $V$ is a set of vertices,
one for each variable in the procedure, $E$ is a set of
\emph{interference edges}, and $A$ is a set of 
\emph{affinity edges}. An interference edge $(v_{i}, v_{j})$
is placed between every pair of variables $v_{i}$ and 
$v_{j}$ that interfere with one another; in principle,
any definition of interference that at least includes
intersection can be used. An affinity edge $(v_{k}, v_{l})$
is placed between two variables that are repectively 
defined and used by a copy operation, i.e., 
$v_{k} \leftarrow v_{l}$ or $v_{l} \leftarrow v_{k}$;
in this case, assigning $v_{k}$ and $v_{l}$ to the
same register eliminates the copy operation. More details
on this process will be provided in section ...

An SSA Form program that is also strict satisfies a very important
property: the definition point of each variable dominates each use
~\cite{BudimlicJun02}; from there, it is fairly straightforward to
show that the definition point not only dominates each use, but 
the entire live range of the resulting variable. In other words,
each live range is a subtree of the dominator tree. The resulting
interference graph, therefore, is the intersection graph of a set
of subtrees of a tree, which is precisely the definition of the 
class of \emph{chordal} graphs. Chordal graphs are significant because
several problems that are NP-complete for general graphs have efficient
polynomial-time solutions for chordal graphs, including graph coloring,
which is often used as a model for register allocation. This theory
will be presented in much greater detail in future chapters of this book.

\section{Philip's comments}

Detailed example (hopefully, corresponding to
the procedure illustrated in the preceding 
section. 

Here, I anticipate 1 page of text and 1 page
for the illustration.

\section{Conventional and Transformed SSA Form}

The conversion to SSA form replaces each variable $v$ in the pre-SSA
program with a set of variables $v_{1}, v_{2}, \ldots, v_{k}$, which
perfectly partition $v$. At every point in the procedure where $v$ is
live, \emph{exactly} one variable $v_{i}$ is also live; and none of
the $v_{i}$ are live at any point where $v$ is not. 

Based on this observation, we can partition the variables in a 
program that has been converted to SSA Form into congruence classes. 
We say that $x$ and $y$ are \emph{$\phi$-related} to one another
if they are referenced by the same $\phi$-function, i.e., 
either both $x$ and $y$ are parameters of the $\phi$-function, or,
without loss of generality, $x$ is a parameter, and $y$ is defined by
the $\phi$-function. This relation is
\begin{enumerate}
\item \emph{reflexive}: $x$ is $\phi$-related to $x$;
\item \emph{symmetric}: $x$ is $\phi$-related to $y$ if and only if
$y$ is $\phi$-related to $x$; and
\item \emph{transitive}: if $x$ is $\phi$-related to $y$ and 
$y$ is $\phi$-related to $z$, then $x$ is $\phi$-related to $z$.
\end{enumerate}
Therefore, this notion of $\phi$-relationship is itself and equivalence
relation, meaning that the transitive closure of the relation partitions
the variables defined locally in the procedure into equivalence classes. 
By the reflexive property, each variable that is not involved in a 
$\phi$-function belongs to a singleton class. 
Let \emph{$\phi$-CongruenceClass${x}$} denote the equivalence class 
containing variable $x$. 

The conversion to SSA Form, as described above, ensures that all varibles
$v_{i}$ introduced for pre-SSA variable $v$ belong to the same $\phi$-
congruence class. The program can be immediately translated out of SSA
Form by replacing each definition or use of $v_{i}$ with $v$ instead, 
and deleting the $\phi$-functions. This simplistic reverse transformation
is not always possible after SSA-based program transformations and 
optimizations such as dead code elimination, copy folding and propagation,
and various forms of code motion are applied~\cite{BriggsJul98}. 

In particular, transformations applied to an SSA Form program may
merge the $\phi$-congruence classes of distinct variables
$u_{i}$ and $v_{j}$, which respectively correspond to pre-SSA variables
$u$ and $v$ respectively. Moreover, $u_{i}$ and $v_{j}$ may interfere,
making it impossible to replace all of the variables in a single
$\phi$-congruence class with a single variable. 

Note: A figure showing an example here would be very nice.

For example, suppose
that the lifetimes of $u_{i}$ and $v_{j}$ overlap, and we replace all
of the variables in their $\phi$-congruence class with a new variable $x$
during the translation out of SSA Form. Then when the program executes,
there will be a point where $x$ is live, and it will contain one value: 
either the value corresponding to $u_{i}$ or $v_{j}$, but not both. 
This is incorrect, as both values are required, because they will be used
at some point later in the program as per the definition of liveness.
As a consequence, the semantics of the program have not been preserved
during the translation out of SSA Form. 

Sreedhar et al.~\cite{SreedharSep99} defines a program to be in 
\emph{Conventional SSA (CSSA)} if it is possible to replace all occurrences
of variables in a $\phi$-congruence class with a single variable without
affecting program correctness; in other words, none of the variables in
a $\phi$-congruence class interfere with one another under CSSA Form. 
The basic SSA construction algorithms all build CSSA Form; however, CSSA
Form is not preserved after numerous transformations and optimizations
are applied. A program is defined to be \emph{Transformed SSA (TSSA)}
Form if at least two variables in a $\phi$-congruence class interfere. 
The conversion from TSSA to CSSA Form is an important and required 
step in order to translate out of SSA Form. Efficient and effective algorithms
to translate out of SSA Form will be presented in Sections ... 

