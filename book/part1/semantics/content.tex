\chapter{Semantics \Author{L. Beringer}}
\numberofpages{10}
\newcommand{\pdec}[3]{\ensuremath{\mathtt{proc}\ } {#1}\ \mathtt{(}{#2}\mathtt{)\ =} \ {#3}}
\newcommand{\letin}[3]{\ensuremath{\mathtt{let}\ {#1}\ \mathtt{=}\ {#2}\ 
                       \mathtt{in}\ {#3}\ \mathtt{end}}}
\newcommand{\letinD}[3]{\ensuremath{\mathtt{let}\ {#1}\ \mathtt{=}\ {#2}\ 
                       \mathtt{in}\ {#3}\ }}
\newcommand{\ite}[3]{\ensuremath{\mathtt{if}\ {#1}\ \mathtt{then}\ {#2}\
                        \mathtt{else}\ {#3}}}
\newcommand{\letrec}[2]{\ensuremath{\mathtt{fun}\ {#1}\ \mathtt{in}\ {#2}\ \mathtt{end}}}
\newcommand{\call}[2]{\ensuremath{{#1}\mathtt{(}{#2}\mathtt{)}}}
\newcommand{\decl}[0]{\ensuremath{\mathit{decl}}}
\newcommand{\uopsymbol}[0]{\ensuremath{\mathit{unop}}}
\newcommand{\bopsymbol}[0]{\ensuremath{\mathit{binop}}}
\newcommand{\uop}[1]{\ensuremath{\uopsymbol\ {#1}}}
\newcommand{\bop}[2]{\ensuremath{\bopsymbol\ {#1}\ {#2}}}
\newcommand{\simplejudge}[2]{{#1} \vdash {#2}}
\newcommand{\closenode}[3]{\mathtt{close}_{{#1}}({#2},{#3})}
\newcommand{\loopnode}[3]{\mathtt{loop}_{{#1}}({#2},{#3})}

%\section{Introduction}
\label{section:Part1:Semantics:Intro}
In this chapter we discuss models that underpin the SSA discipline and
some of the concepts associated with SSA, or embed SSA in alternative
program representations. Besides complementing the intuitive meaning
of ``unimplementable'' $\phi$-instructions, the models are motivated
by the aim
\begin{itemize}
\item to make syntactic conditions and semantic invariants that are
  implicit in the definition of SSA more explicit. The introduction of
  SSA itself was motivated by a similar goal: to represent aspects of
  program structure, namely the def-use relationships, explicitly in
  syntax, by enforcing a particular naming discipline. In a similar
  way, representations of SSA in other formalisms explicitly enforce
  invariants such as ``all $\phi$-functions in a block must be of the
  same arity'', ``the variables assigned to by the $\phi$-functions in
  a block must be distinct'', or ``$\phi$-functions are only allowed
  to occur at the beginning of of a basic block''. Reasoning using
  semantic models helps us to understand why such conditions are
  required and to identify opportunities for relaxing them.
\item to have formal criteria with respect to which SSA-based code
  transformations may be proven correct. For example,
  Glesner~\cite{DBLP:conf/asm/Glesner04} uses a representation of SSA
  in terms of abstract state machines to prove the correctness of a
  code generation transformation, while Chakravarty et
  al.~\cite{ChakravartyKZ:COCV03} prove the correctness of a
  functional representation of Wegmann and Zadeck's SSA-based sparse
  conditional constant propagation
  algorithm~\cite{WegmannZ:Toplas1991}.
\item to facilitate the implementation of interpreters operating at
  SSA level. This enables the compiler developer to experimentally
  validate SSA-based analyses and transformations at their genuine
  language level, before out-of-SSA-translation is performed
\item to provide a formal basis for comparing and integrating variants
  of SSA (such as the variants discussed elsewhere in this book), for
  translating between these variants, and for translating into and out
  of SSA
\item to provide conceptual support for the appeal of SSA by relating
  core concepts to principles well-understood in other domains of
  compiler and programming language research
\end{itemize}
%As the variations on SSA discussed in this book highlights, a compiler
%writer has considerable elbow room even after subscribing to ``the''
%SSA discipline. A cross-cutting goal of the semantic models is to
%provide orientation in this design space.

We categorize the models into two groups. The first group concerns
representations that emphasize control-flow aspects of SSA by adhering
to the control flow successor relation inside basic blocks or the
dominator structure between basic blocks. We discuss aspects of the
Appel-Kelsey analogy between SSA and functional representatations in
some detail, and summarize a recent type-based representation due to
Matsuno and Ohori~\cite{DBLP:conf/ppdp/MatsunoO06}.

The second group of models concerns representations that emphasize
data flow over control flow. The first model, due to Glesner,
disregards control flow indside basic block but retains it at basic
block boundaries, and is phrased in terms of abstract state
machines~\cite{DBLP:journals/tocl/Gurevich00}.  The second model, due
to Pop et al.~\cite{PopJS2007}, dispenses with control flow entirely
and instead views programs as sets of equations that model the
assignment of values to variables in a style reminiscent of partial
recursive functions.

We conclude with some brief pointers to additional literature.

%\begin{enumerate}
%\item In Section~\ref{section:Part1:Semantics:GlesnerSemantics} we discuss representations of SSA using abstract state machines, term graphs, and partial orders developed
%  by Glesner. Avoiding the explicit use of variable names altogether,
%  programs are represented as an overlay of the control flow structure
%  over the data dependence graph given by the def-use
%  relationships. The execution of each basic block maintains the phase
%  distinction between the evaluation of the $\phi$-nodes and the
%  execution of non-$\phi$-instructions. The latter proceeds in a
%  non-deterministic order governed purely by the availability of
%  operands.
%\item Section~\ref{section:Part1:Semantics:FunctionalLanguages} outlines a correspondence of SSA to restricted functional languages, which was
%  pioneered by O'Donnell, Kelsey, and Appel. Control flow is either
%  represented in continuation-passing-style (CPS) or by 
%  mutually tail-recursive first-order functions. Exploiting the
%  conceptual similarity between points of definition of imperative
%  variables and binding of functional names, and between dominance and
%  static scope, the particular appeal of this model lies in the fact
%  that its target formalism is well-known to many programmers,
%  directly supported by interpreters and compilers, and firmly
%  grounded in mathematical logic by being based on the
%  $\lambda$-calculus. These features also make the formalism a natural
%  common representation for integrating compiler front-ends for
%  different languages.
%\item Section~\ref{section:Part1:Semantics:PopSemantics} finally summarizes a denotational model developed by Pop et al.~\cite{PopJS2007}. Motivated in part by recent proposals to require loop-closing $\phi$-nodes~\cite{}, the authors formally relate SSA to imperative code in non-SSA form, complementing the formal relationships between SSA and functional languages, and extend the SSA discipline to a full declarative language. 
%\end{enumerate}
%Similar to the task of implementing one programming language in
%another, each model embeds SSA into a more ``basic'' host
%formalism. Reasoning steps performed in these host formalism are
%considered already understood, without requiring further
%justification.  This is not to say that their notational and sometimes
%conceptual complexity may not itself occasionally be
%considerable. Indeed, for the sake of readability, we gloss over
%numerous technical details in the following sections and refer the
%reader to the literature for more in-depth treatments of the models.

\section{Control-flow-based representations}

\subsection{Functional interpretations}
\label{section:Part1:Semantics:FunctionalLanguages}

Our first model of SSA is provided by functional programming
languages. This interpretation rooted in the observation that the
central goal of SSA, namely to provide each use of a variable with a
unique point of definition, is obtained for free from the concepts of
binding and lexical scoping. Like the binding of a variable in
$\lambda$-calculus, a let-binding $\letin x e {e'}$ in a functional
language binds the result of $e$ to name $x$ for the duration (scope)
$e'$.  Contrary to assignments to imperative variables, such a binding
\emph{shadows} earlier bindings of $x$ throughout the evaluation
of $e'$, but does not overwrite them\footnote{In order to avoid
confusion, we refer to identifiers in the functional world as
\emph{names}, reserving the term \emph{variable} for the imperative
regime.}. Indeed, the choice of name $x$ is arbitrary and the
result of $e'$ is not altered if we capture-avoidingly replace
(``$\alpha$-rename'') $x$ by some other fresh name $y$ that does not
occur free in $e'$. As mentioned in Chapter~\ref{Section1.3}, this
notion of \emph{referential transparency} is shared between SSA and
functional languages.

O'Donnell~\cite{ODonnellPhD} and Kelsey~\cite{Kelsey95} observed that
the correspondence between name binding and point of variable
definition extends to other aspects of program structure. The
dominance-based control flow structure of SSA corresponds in a precise
way to \emph{continuation-passing-style} (CPS), a program
representation routinely used in compilers for functional languages
\cite{DBLP:journals/lisp/SussmanS98a,Appel:CWC}.

Programs in CPS explicitly communicate code fragments (``continuation
terms'') that stipulate where evaluation should continue once the
execution of the current fragment has terminated. In this respect,
continuations are similar to return addresses in procedure
calls. Being particular higher-order functions, continuations may
create, apply, and communicate further continuations, enabling the
efficient representation of arbitrary control flow.  Intra-procedural
merge points correspond to invocations of identical (local)
continuations, albeit with possibly differing actual arguments.  Each
formal data parameter of such a continuation plays exactly the same
role as a single $\phi$-function at the beginning of a code block: to
unify the arguments stemming from various calls sites and bind them to
a unique name for the duration of the ensuing code fragment.

An alternative to the use of anonymous continuation terms is to
represent a procedure as a set of mutually tail-recursive functions --
a representation occasionally referred to as \emph{direct
style}~\cite{Reynolds1974}. For this variant, which we use below, the
correspondence was popularized by Appel~\cite{Appel98:SSA}. The
granularity of functions is roughly that of basic blocks, enabling one
to model jumps as function invocations. However, deviations from this
discipline are possible, for example by inlining functions that have
only a single invocation site, i.e.~considering extended basic blocks.
The tail-recursiveness of functions implies that no invocation stack
needs to be maintained for such local function calls. In contrast,
procedure invocations obey the usual frame stack discipline and may be
represented as function calls that occur in non-tail position.

The correspondence to SSA is most pronounced for direct style or CPS
programs that are in \emph{let-normal-form}: each intermediate result
must be explicitly named, and function arguments must be names or
constants. Syntactically, let-normal-form isolates basic instructions
in a separate category of primitive terms $a$ and then requires
let-bindings to be of the form $\letin x a {e'}$.  In particular,
neither jumps (conditional or unconditional) nor let-bindings are
primitive. As a consequence of these restrictions, the conversion of
unrestricted code into let-normal form fixes an evaluation order, in a
similar way as the linearization of a data- and control
flow-dependence graph.

%Furthermore, as $a$ cannot contain bindings
%and all uses of $x$ in $e$ are in the scope of the innermost binding
%of $x$, any outer binding of $x$ becomes unreachable. Thus, ANF allows
%us to avoid any stack-based implementation of shadowing - in precise
%correspondence to SSA where variables originating from the same
%original program identifiers \footnote{what's the terminology we use
%for such variables?} not only shadow each other but make each other
%unreachable for the remainder of the code.

Occasionally, \emph{direct style} refers to the combination of
tail-recursive functions and let-normal form. Variations of this
discipline include
\emph{administrative normal form} (A-normal form, ANF~\cite{DBLP:conf/pldi/FlanaganSDF93}), B-form~\cite{DBLP:conf/pldi/TarditiMCSHL96}, and SIL~\cite{DBLP:journals/jfp/TolmachO98}. 

%Before illustrating the correspondence in more detail we briefly
%discuss the target representation.

\subsubsection{Translation into and out of SSA}

The correspondence between SSA and functional languages may be broken
down into the correspondence pairs shown in
Table~\ref{tableFunctionalCorrespondencesI}. We discuss some of these
aspects by considering the translation into SSA, using the program in
Figure~\ref{fig:FunctionalCorrespondenceRunningExample} as a running
example.

\begin{table}
\begin{tabular}{|c|c|}
  \hline Functional concept & Imperative/SSA concept\\ 
  \hline \hline
  name-binding in let & assignment (point of definition)\\
  name-binding in function parameter & assignment by $\phi$-function
  (point of definition)\\ 
  $\alpha$-renaming & clash-avoiding variable renaming\\
  unique association of binding occurrences to uses & unique
  association of defs to uses\\ 
  lexical scope of name & dominance
  region of variable\\ 
  subterm relationship & control flow successor relationship\\
  free name & live-in variable (least
  solution)\\ 
  arity of function $f_i$ & number of
  $\phi$-functions at beginning of $b_i$\\ 
  distinctness of formal
  parameters of $f_i$ & distinctness of LHS-variables in the
  $\phi$-block of $b_i$\\ 
  number of call sites of function $f_i$ &
  arity of $\phi$-functions in block $b_i$\\ 
  parameter lifting/dropping & addition/removal of $\phi$-function\\ 
  block floating/sinking & reordering according to dominator tree
  structure\\
  potential nesting structure 
  %of fully $\lambda$-lifted function definitions 
  & dominator tree\\
  nesting level & maximal
  level index in dominator tree\\
  conversion to GNF & out-of-SSA translation\\
  \hline
\end{tabular}
\caption{\label{tableFunctionalCorrespondencesI}
  Correspondence pairs between functional form and SSA: program structure}
\end{table}

\begin{figure}
\begin{tabular}{rlcrlcrll}
$\mathtt{b_1:}$ & $\mathtt{v:=1}$ & \quad &
  $\mathtt{b_2:}$ & $\mathtt{x:=5+y}$ & \quad &
  $\mathtt{b_3:}$ & $\mathtt{w:=y+v}$\\
& $\mathtt{z:=8}$ & & & $\mathtt{y=y*z}$ & & & $\mathtt{return\ w}$\\
& $\mathtt{y:=4}$ & & & $\mathtt{x:=x-1}$\\
& $\mathtt{goto}\ \mathtt{b_2}$ & & & $\mathtt{if\ x=0\ b_3\ b_2}$
\end{tabular}
\caption{\label{fig:FunctionalCorrespondenceRunningExample} Correspondence between SSA and functional representation: running example}
\end{figure}

A simple way to represent this program in a functional language is to
introduce one function $f_i$ for each basic block $b_i$. The body of
each $f_i$ arises by introducing one let-binding for each assignment
and converting jumps into function calls. In order to determine the
formal parameters of these functions we perform a liveness
analysis. For each basic block $b_i$, we choose an arbitrary
enumeration of its live-in variables. We then use this enumeration as
the list of formal parameters in the declaration of the function
$f_i$, and also as the list of actual arguments list in calls to
$f_i$. We organize all function definitions in a single block of
mutually tail-recursive functions at the top of the functional
program: 
$$\begin{array}{l}
\begin{array}{lcl}
  \mathtt{fun}\ f_1() & = &
   \letin v 1 {
              \letin z 8 {
                 \letin y 4 {\call {f_2} {v,z,y}}
              }
            }
\end{array}\\
\begin{array}{lcl}
\mathtt{and}\ f_2(v,z,y) & = & 
  \letinD x {5+y} {
              \letinD y {x*z} {
                 \letinD x {x-1} {}}}\\ & & 
   \ite {x=0} {\call {f_3} {y,v}} 
              {\call {f_2} {v,z,y}}\ \mathtt{end}\ \mathtt{end}\ \mathtt{end}
\end{array}\\
\begin{array}{lcl}
  \mathtt{and}\ f_3(y,v) & = & \letin w {y+v} w\\
  \mathtt{in}\ f_1()\ \mathtt{end}
\end{array} 
\end{array}
$$ 
The resulting programs has the following properties:
\begin{itemize}
\item all function declarations are closed, i.e.~the free variables of their bodies are contained in their formal parameter lists
\item names are not unique, but the program obeys a core property
of SSA: each use of a name is associated with a unique binding, namely
the unique \emph{innermost} binding in whose scope the use occurs.
\item inside each function the subterm relationship captures the control flow successor relation in the corresponding basic block.
\end{itemize}
The correspondence between liveness and free occurrences of names
manifests itself in the striking structural similarity between the
liveness-equation for assignments $$\mathsf{LV}([x:=a]^i) =
\mathsf{Use}(a) \cup (\mathsf{LV}(\mathit{succ}(i)) \setminus \mathsf{Defs}(i))$$ and the clause for the corresponding expression form %$\letin x a e$ 
in the definition of free variables,
$$\mathsf{FV}(\letin x a e) = \mathsf{Use}(a) \cup (\mathsf{FV}(e) \setminus
\{x\})).$$ In fact, the
\emph{least} solution to the liveness equations cannot only be used to determine the formal parameters of functions but in fact assigns each program point (even
intermediate ones) exactly the free variables of the corresponding
subexpression in the functional program.

We may $\alpha$-convert (i.e.~replace a bound name by a fresh one, and
substitute the latter for all free occurrences of the former in its
scope) this program to make names globally unique. As no function
declaration is nested inside another, the renamings are independent
from each other.  The resulting program is an SSA-program in disguise:
each formal parameter of a function $f_i$ represents the target
variable of one $\phi$-function for the corresponding block $b_i$. The
arguments of these $\phi$-functions are the arguments in the
corresponding positions in the calls to $f_i$. As the number of
arguments in each call to $f_i$ coincides with the number of $f_i$'s
formal parameters, the $\phi$-functions in $b_i$ are all of the same
arity, namely the number of call sites to $f$. In order to coordinate
the relative positioning of the arguments of the $\phi$-functions, we
choose an arbitrary enumeration of these call sites. 

Under this perspective, the above construction of parameter lists
amounts to equipping each $b_i$ with $\phi$-functions for all its
live-in variables, with subsequent renaming of the
variables\footnote{In principle, the parameter lists can be
constructed from any solution to the
liveness-\emph{in}equations. These arise by replacing $=$ with
$\supseteq$ in the dataflow equations. Using inequations rather than
equations allows functions to have more formal parameters than
strictly necessary. Requiring all parameter lists to be chosen
according to the
\emph{same} solution prevents (ill-defined) functions whose bodies contain free
variables that are not amongst the formal parameters.  In particular,
including all variables in all parameter lists constitutes a solution
to the inequations but not necessarily one to the equations.}. Thus,
the above method corresponds to the construction of \emph{pruned}
SSA--see Chapter~\ref{ChapterPrunedSSA}.

While resulting in a legal SSA program, the construction clearly
introduces more $\phi$-functions than necessary. A closer inspection
reveals that each superfluous $\phi$-function corresponds to some
function $f_i$ passing one of its arguments on to some other function
$f_j$ without modifying it. The technique for eliminating such
arguments is called
\emph{lambda-dropping}~\cite{DBLP:journals/tcs/DanvyS00} and is the inverse of
\emph{lambda-lifting}~\cite{DBLP:conf/fpca/Johnsson85}.

Lambda-dropping may be performed before or after variable names are
made distinct, and first analyses the static invocation graph to
identify when function definitions may be moved inside each other
(\emph{block sinking}). As the above construction yields closed
functions, block-sinking is always legal. In our example, $f_3$ is
only invoked from within $f_2$, and $f_2$ is only called in the bodies
of $f_2$ and $f_1$.  We may thus move the definition of $f_3$ into
that of $f_2$, and the latter one into $f_1$.

Several options exist as to where the function definitions should be
inserted in their host functions.  Inserting them near the beginning yields the
program 
$$
\begin{array}{l}
  \begin{array}{lcl}\mathtt{fun}\ f_1() & = & \end{array}\\
  \qquad
   \begin{array}[t]{l} 
     \begin{array}{lcl}\mathtt{fun}\ f_2(v,z,y) & = & \end{array}\\
     \qquad \begin{array}{l}  
             \begin{array}{lcl}\mathtt{fun}\ f_3(y,v) & = & \letin w {y+v} w\end{array}\\
             \, \mathtt{in}\
              \begin{array}[t]{l}
                \letinD x {5+y} {
                  \letinD y {x*z} {
                   \letinD x {x-1} {}}}\\ 
                  \ite {x=0} {\call {f_3} {y,v}}
                     {\call {f_2} {v,z,y}}\ \mathtt{end}\ \mathtt{end}\ \mathtt{end}
              \end{array}\\
             \mathtt{end}
           \end{array}\\
     \, \mathtt{in}\ \letin v 1 {
              \letin z 8 {
                 \letin y 4 {\call {f_2} {v,z,y}}
              }
            }
   \end{array}\\
\begin{array}{lcl}
  \mathtt{in}\ f_1()\ \mathtt{end}
\end{array} 
\end{array}
$$ 
An alternative is to insert them near the end of their host
functions, directly prior to their use: 
$$
\begin{array}{ll}
\mathtt{fun}\ f_1()\ = & \mathtt{let}\ v = 1 \ 
                  \mathtt{in\ let}\ z = 8 \ 
                  \mathtt{in\ let}\ y = 4\ \mathtt{in}\\
&\! \begin{array}{ll}
     \mathtt{fun}\ f_2(v,z,y) = &
     \mathtt{let}\ x = 5+y\
     \mathtt{in\ let}\ y = x*z\
     \mathtt{in\ let}\ x = x-1\ \mathtt{in}\\
   & \mathtt{if}\ x=0\\ 
   &  \mathtt{then}\ 
      \begin{array}[t]{l}
         \mathtt{fun}\ f_3(y,v) = 
         \mathtt{let}\ w = y+v\ \mathtt{in}\ w\ \mathtt{end}\\
         \mathtt{in}\ f_3(y,v)\ \mathtt{end}
      \end{array}\\
   & \mathtt{else}\ f_2(v,z,y)\ \mathtt{end\ end\ end}
  \end{array}\\
& \mathtt{in}\ f_2(v,z,y)\ \mathtt{end\ end\ end\ end}\\
\mathtt{in}\ f_1()\  \mathtt{end}
\end{array}
$$
The latter placement enables the second phase of
lambda-dropping, called
\emph{parameter dropping}, to remove variables from the lists of
formal parameters based on the syntactic scope. In our example, both
parameters of $f_3$ can be removed; of the three parameters of $f_2$,
only $y$ survives:
$$
\begin{array}{ll}
\mathtt{fun}\ f_1()\ = & \mathtt{let}\ v = 1 \ 
                  \mathtt{in\ let}\ z = 8 \ 
                  \mathtt{in\ let}\ y = 4\ \mathtt{in}\\
&\! \begin{array}{ll}
     \mathtt{fun}\ f_2(y) = &
     \mathtt{let}\ x = 5+y\
     \mathtt{in\ let}\ y = x*z\
     \mathtt{in\ let}\ x = x-1\ \mathtt{in}\\
   & \mathtt{if}\ x=0\\ 
   &  \mathtt{then}\ 
      \begin{array}[t]{l}
         \mathtt{fun}\ f_3() = 
         \mathtt{let}\ w = y+v\ \mathtt{in}\ w\ \mathtt{end}\\
         \mathtt{in}\ f_3()\ \mathtt{end}
      \end{array}\\
   & \mathtt{else}\ f_2(y)\ \mathtt{end\ end\ end}
  \end{array}\\
& \mathtt{in}\ f_2(y)\ \mathtt{end\ end\ end\ end}\\
\mathtt{in}\ f_1()\  \mathtt{end}
\end{array}
$$
Interpreting this program back in SSA yields code that
contains a single $\phi$-function, for variable $y$ at the beginning
of block $b_2$. The reason that this $\phi$-function can't be
eliminated (it is redefined in the loop) is precisely the reason why
$y$ survives $\lambda$-dropping: the innermost scope in force at $y$'s
use in the recursive call to $f_2$ is the one introduced by the
binding of $y$ in $f_2$'s body, and is thus different from the scope
in force at the point where $f_2$ is declared.

Analyzing whether function definitions may be nested inside one
another is tantamount to analyzing the imperative dominance structure:
function $f_i$ may be moved inside $f_j$ exactly if all calls to $f_i$
come from within $f_j$ exactly if all paths from the initial program
point to block $b_i$ traverse $b_j$ exactly if $b_j$ dominates
$b_i$. The optimal nesting structure is thus given by the dominator
tree: the maximal level at which a function may occur is its level
(counting from the root) in the dominator tree.

The choice as to where functions are placed corresponds to variants of
SSA. For example, the recently introduced loop-closed SSA form~\cite{}
requires the insertion of $\phi$-nodes for all variables that are
modified in a loop, in order to merge copies of these variables that
arise when the loop is unrolled.  In the functional setting, this
amounts to inserting functions $f$ that are called from within a
recursive function $g$ at the same level as $g$. In our example
program, $f_3$ is thus placed at the same level as
$f_2$, but the placement of $f_2$ is left unaltered.
$$
\begin{array}{ll}
\mathtt{fun}\ f_1()\ = & \mathtt{let}\ v = 1 \ 
                  \mathtt{in\ let}\ z = 8 \ 
                  \mathtt{in\ let}\ y = 4\ \mathtt{in}\\
& \mathtt{fun}\ f_3(y,v) = 
   \mathtt{let}\ w = y+v\ \mathtt{in}\ w\ \mathtt{end}\\
&\! \begin{array}{ll}
     \mathtt{fun}\ f_2(v,z,y) = &
     \mathtt{let}\ x = 5+y\
     \mathtt{in\ let}\ y = x*z\
     \mathtt{in\ let}\ x = x-1\ \mathtt{in}\\
   & \mathtt{if}\ x=0\ \mathtt{then}\ f_3(y,v)\
     \mathtt{else}\ f_2(v,z,y)\ \mathtt{end\ end\ end}
  \end{array}\\
& \mathtt{in}\ f_2(v,z,y)\ \mathtt{end\ end\ end\ end}\\
\mathtt{in}\ f_1()\  \mathtt{end}
\end{array}
$$
As a consequence, any parameter of the invoked function that is
rebound in the loop cannot be $\lambda$-dropped. In the example,
$y$ is not deleted from the parameter list of $f_3$.
$$
\begin{array}{ll}
\mathtt{fun}\ f_1()\ = & \mathtt{let}\ v = 1 \ 
                  \mathtt{in\ let}\ z = 8 \ 
                  \mathtt{in\ let}\ y = 4\ \mathtt{in}\\
& \mathtt{fun}\ f_3(y) = 
   \mathtt{let}\ w = y+v\ \mathtt{in}\ w\ \mathtt{end}\\
&\! \begin{array}{ll}
     \mathtt{fun}\ f_2(y) = &
     \mathtt{let}\ x = 5+y\
     \mathtt{in\ let}\ y = x*z\
     \mathtt{in\ let}\ x = x-1\ \mathtt{in}\\
   & \mathtt{if}\ x=0\ \mathtt{then}\ f_3(y)\
     \mathtt{else}\ f_2(y)\ \mathtt{end\ end\ end}
  \end{array}\\
& \mathtt{in}\ f_2(y)\ \mathtt{end\ end\ end\ end}\\
\mathtt{in}\ f_1()\  \mathtt{end}
\end{array}
$$ 
Subsequently, loop unrolling replaces the recursive
call to $f_2$ by the body of $f_2$. The resulting code (here in the
form where the unrolling is not isolated as a separate function
declaration) 
$$
\begin{array}{ll}
\mathtt{fun}\ f_1()\ = & \mathtt{let}\ v = 1 \ 
                  \mathtt{in\ let}\ z = 8 \ 
                  \mathtt{in\ let}\ y = 4\ \mathtt{in}\\
& \mathtt{fun}\ f_3(y) = 
   \mathtt{let}\ w = y+v\ \mathtt{in}\ w\ \mathtt{end}\\
&\! \begin{array}{ll}
     \mathtt{fun}\ f_2(y) = &
     \mathtt{let}\ x = 5+y\
     \mathtt{in\ let}\ y = x*z\
     \mathtt{in\ let}\ x = x-1\ \mathtt{in}\\
   & \mathtt{if}\ x=0\ \mathtt{then}\ f_3(y)\\
   & \mathtt{else} 
       \begin{array}[t]{l} 
         \mathtt{let}\ x = 5+y\
         \mathtt{in\ let}\ y = x*z\
         \mathtt{in\ let}\ x = x-1\ \mathtt{in}\\
         \mathtt{if}\ x=0\ \mathtt{then}\ f_3(y)\
         \mathtt{else}\ f_2(y)\ \mathtt{end\ end\ end}
       \end{array}\\ 
   & \mathtt{end\ end\ end}
  \end{array}\\
& \mathtt{in}\ f_2(y)\ \mathtt{end\ end\ end\ end}\\
\mathtt{in}\ f_1()\  \mathtt{end}
\end{array}
$$ 
contains two invocations sites for $f_3$, in precise correspondence to
the control flow arcs in the unrolled SSA program. Each call passes
the correct value to the ``loop closing'' parameter $y$ of $f_3$.

The above example code excerpts exhibit a further feature: the
argument list of any call coincides with the list of formal parameters
of the invoked function. This discipline is not enjoyed by functional
programs in general, and is destroyed by the renaming phase that makes
names globally distinct. Optimizing program transformations also
destroy this discipline, in a similar way as the relationship between
SSA variables originating from the same program variable is destroyed
by imperative program manipulations--see
Chapter~\ref{ChapterTransformations}. On the other hand, programs
satisfying this discipline can be immediately converted to $\phi$-free
imperative form - all $\phi$-functions are trivial. Thus, the task of
translating out of SSA amounts to converting a functional program with
arbitrary argument lists into one where argument lists and formal
parameter lists coincide for each
function. Beringer~\cite{DBLP:journals/entcs/Beringer07} calls this
transformation GNF-conversion and presents a simple local algorithm
that considers each call site individually. A single additional name
suffices for performing all necessary permutations, in line with the
results of~\cite{May}. Rideau et
al.~\cite{DBLP:journals/jar/RideauSL08} present an in-depth study of
this conversion problem, backed-up by a formalization in a theorem
prover.

\subsubsection{Program analyses}
Program analyses for functional languages are typically formulated as
\emph{type systems}. Table~\ref{tableFunctionalCorrespondencesII} collects some
correspondence pairs that relate concepts from type systems to notions
from dataflow analysis frameworks.
\begin{table}
\begin{center}
\begin{tabular}{|c|c|}
  \hline Functional concept & Imperative/SSA concept\\ 
  \hline
  type systems & dataflow frameworks\\
  typing context & scope-aware symbol table\\
  typing
  rules & dataflow (in-)equations/transfer functions\\
  subtype relationship & merge operator\\
  typing
  type inference & fixed point iteration\\
  derivations & solutions to dataflow equations\\
  \hline
\end{tabular}
\end{center}
\caption{\label{tableFunctionalCorrespondencesII}
  Correspondence pairs between functional form and SSA: program analyses}
\end{table}
 A typical type judgement $\Gamma
\vdash e:\tau$ 
associates a type $\tau$ to an expression $e$, based on typing
assumptions in context $\Gamma$. Usually, contexts track the types of
(at least) the free names of $e$, similarly to a symbol table in an
imperative analysis. Thus, almost any type system is an extension of
the concept of free variables, turning the above relationship between
liveness and free variables into an instance of the given
analogies. The distinctness of formal parameters, the distinctness of
function names in function declaration blocks, and similar syntactic
restrictions, may be easily enforced by equipping the corresponding
typing rules with additional side-conditions, and are in any case
enforced by many functional languages as part of the language
definition.

A major benefit of SSA for dataflow analyses is the avoidance of
variables that have several unrelated uses but happen to be
identically named.  Even in the absence of globally unique names, this
property is enjoyed by type systems, as the adaptation of type
contexts in the rule for let-bindings is compatible with referential
transparency.

Imperative analysis frameworks employ transfer functions for relating
the information associated with adjacent program points. In accordance
with the correspondence between the control flow successor relation and
the subterm relationship, this role is in type systems played by
syntax-directed typing rules. Merge operators at control flow merge
points correspond to appropriate notions of subtyping.

The correspondent to fixed point algorithms for obtaining dataflow
solutions is type inference. Both tasks proceed algorithmically in a
structurally equal fashion, along the control flow
successor-/predecessor relationship or sub-/superterm relationship.
%,typically strike a balance between analysis time and precision. 
Finally, \emph{solutions} of dataflow analyses arise when
all constraints are met -- in type systems, the corresponding notion is
that of a successful typing derivation.

As many functional languages support high-order functions, type
systems are particularly well suited for formulating inter-procedural
analyses\footnote{Maybe the author of the chapter on inter-procedural
analyses can briefly take up this point, allowing me to insert a
forward-reference here?}.

\subsection{Type-based representations}
The above functional representations recast the SSA discipline
\emph{syntactically}. An alternative proposed by Matsuno and
Ohori~\cite{DBLP:conf/ppdp/MatsunoO06} is to leave the syntax of
programs in non-SSA form and to model the SSA discipline at the level
of types. In their analysis, each base type represents the definition
point of a variable. Contexts $\Gamma$ associate program variables
with sets of types, modeling the collection of reaching definitions
for the variables available at a given program point. Noting that the
sets of definitions reaching the use of a variable form a tree where
the paths are the control flows from the definitions to the use, the
authors admit types to be formulated over type variables whose
introduction and use corresponds to the introduction of $\phi$-nodes
in SSA.
 
Formulated for a language of unstructured control flow, the analysis
is phrased as a proof theory in sequential sequent calculus
style~\cite{DBLP:journals/toplas/Ohori07}. Judgements take the form
$\mathcal{M}, \mathcal{C},
\Gamma \vdash B$ where context $\Gamma$ associates the free program
variables of code sequence $B$ (a basic block) with their types,
$\mathcal{C}$ contains typing specifications for all targets of jumps
in $B$, and $\mathcal{M}$ contains the (possibly recursive) unfolding
definitions of all type variables occurring in $\Gamma$ and
$\mathcal{C}$.

Similar to type systems for functional languages, the hypotheses of
typing rules concern the immediate successor of the head instruction
of the rules' conclusion. The leaves of a typing derivation are formed
by axioms that represent return statements or jumps to successor basic
blocks.

As each type uniquely identifies a point of definition, typical
SSA-related tasks can be represented as algorithms that construct,
analyze or modify the type structure of a program, without having to
make the program variables themselves syntactically distinct.  In
particular, as the SSA discipline is only performed on the type level,
out-of-SSA-translation is redundant.  The authors show the precise
correspondence of their type-based representation to programs in SSA
form, give a type inference algorithm that corresponds to Das and
Ramakrishna's algorithm for SSA construction~\cite{DasRamakrishna},
and present type-based analoga to SSA-based dead-code elimination and
common subexpression elimination.

\section{Dataflow interpretations}
\label{section:Part1:Semantics:Dataflow}
The second group of interpretations for SSA is formed by models that
emphasize dataflow aspects, i.e.~the flow of values along the
def-use-chains. Control flow is either disregarded entirely or
restricted to the boundaries between basic blocks.

\subsection{Glesner's intra-block dataflow model}
Glesner~\cite{DBLP:conf/asm/Glesner04} presents a model of SSA that
retains the control flow structure between basic blocks, and also the
phase distinction between the execution of $\phi$-instructions and the
execution of ordinary instructions in a basic block. The latter,
however, proceed data-driven in that their progress is only governed
by the availability of operands.

Formally, the model is phrased in terms of \emph{abstract state
machines}~\cite{DBLP:journals/tocl/Gurevich00}, an algebraic formalism
for transition systems that supports the partitioning of states into
\emph{static} components and
\emph{dynamic} components. Being invariant under execution, the syntax
of a program is encoded using the static features and is modeled as a
single first-order algebra over a signature of operations, basic
blocks, and predicate symbols which encode the basic-block-level
CFG. The dynamic aspects of program execution are encoded using the
dynamic features: states are modeled as different first-order algebras
over a shared signature. Transition rules model the evolution of
states by stipulating how -- given a fixed static algebra -- the
dynamic algebras of adjacent states are related.

The dynamic language associates with each (static) instruction a slot
for the result value. Additional dynamic components include the
representations to the current and adjacent basic blocks, and a tag
that distinguishes the $\phi$-phase from the phase for non-$\phi$-
instructions. The transition rules for instructions are predicated on
the existence of values in the result slots of the dataflow
predecessor instructions, such that instructions that have all
argument positions filled may fire in an arbitrary order, updating
their result slots. Conditional and unconditional jumps make their
result available in slots that are used to update the current-block
data structures.

In addition to defining the operational model of SSA, Glesner also
defines a similar model for a machine language with unstructured
control flow, and then proves the functional correctness of a
translation that allocates one register per instruction (holding the
result value), eliminates $\phi$-instructions by appropriate copy
operations, and linearizes the data flow graphs inside basic blocks by
topological sorting. 

\subsection{Pop et al.'s global dataflow model}
\label{section:Part1:Semantics:PopSemantics}
%\footnote{Sebastian-Pop-style denotational semantics of SSA form (i.e. as
%dataflow/Lucid}
The model introduced by Pop et al.~\cite{PopJS2007} dispenses with the
control flow structure entirely, by eliminating any tangible forms of
basic blocks or program order. Programs are represented as collections
of defining equations,
\begin{eqnarray*}
x_1 & = & e_1\\
& : &\\
x_n & = & e_n
\end{eqnarray*}
one for each variable $x_i$. Contrary to the functional
representation, the right-hand sides $e_i$ of these equations do not
refer to \emph{control flow successors} but to \emph{data flow
predecessors}, i.e.~to the variables that provide the operands
necessary for updating $x_i$. As a consequence, the order in which
equations are presented is irrelevant, and execution proceeds
completely data-driven.

In order to transform a sequence of assignments into this form we may
apply the approach for converting a basic block into SSA: we introduce
a new variable for each assignment, and substitute these variables in
the right-hand sides of the instructions according to the data
flow. The order of assignments may be permuted arbitrarily, so a
sequence like $x := 5;\ y:=x+z;\ x:=y*3$ may, for example, be
represented by the equations
\begin{eqnarray*}
x_2 & = & y_2 + 3\\
y_1 & = & x_1 + z_1\\
x_1 & = & 5
\end{eqnarray*}
(variable $z$ is live-in here).

In order to transform loops, the category of right-hand side
expressions $e$ is extended by two novel operations, $\loopnode
\ell e {e'}$ and $\closenode \ell e {e'}$.
Both operations resemble $\phi$-instructions, but their arity is
independent of any control flow structure: $e$ and $e'$ are
expressions and $\ell$ is an index from $\{1,\ldots,N\}$ where $N$ is
the number of while-statements in the original program.  An equation
\begin{eqnarray*}
x & = & \loopnode \ell {e} {e'}
\end{eqnarray*}
roughly corresponds to the occurrence of a $\phi$-node for $x$ at the
beginning of a loop in SSA, assigning $e$ to $x$ during the first
iteration of loop $\ell$ and assigning $e'$ to $x$ in later
iterations.
An equation 
\begin{eqnarray*}
x & = & \closenode \ell {e} {e'}
\end{eqnarray*}
corresponds roughly to a loop-closing $\phi$-node for the loop
$\ell$. Expression $e$ represents the boolean loop condition, and $e'$
represents the value that will be assigned to $x$ when the loop is
left, i.e. when $e$ evaluates for the first time to $\mathtt{false}$.

For example, the representation of
$\mathtt{i=7;\ j=0;\ while\ (j<10)\ \{j=j+i\}}$
(taken from~\cite{PopJS2007}) contains the five equations
$$
\begin{array}{rclcrcl}
i_1 & = & 7 & \qquad & j_2 & = & \loopnode 1 {j_1} {j_3}\\
j_1 & = & 0 & & j_4 & = & \closenode 1 {j_2 < 10} {j_2}\\
j_3 & = & j_2 + i_1
\end{array} 
$$ 
where $1$ is the (trivial) index for the single while-command
occurring in the program.  In effect, $j_4$ is assigned the value held
in $j_2$ in that iteration for which $j_2 < 10$ is falsified for the
first time, i.e.~$14$.

In order to formally define concepts such as \emph{for the first
time}, the representation is equipped with a semantics that employs
so-called
\emph{iteration space vectors}: for a program with $N$ loops, such
a vector consists of an $N$-tuple of values, with the value at
position $\ell$ representing the number of iterations of loop $\ell$.
Given such a vector $k$, the meaning of a right-hand-side expression
$e$ is given recursively as follows:

\begin{itemize} 

\item 
constant expressions and arithmetic operators have their standard
(iteration space vector independent) meaning.

\item a variable $x$ is interpreted by evaluating its defining
equation at the iteration space vector $k$.

\item an expression $\loopnode \ell {e_1} {e_2}$ evaluates to the value of
$e_1$ at $k$ if $k$ at index $\ell$ is zero. Otherwise, $\loopnode
\ell {e_1} {e_2}$ evaluates to the result of evaluating $e_2$ at $k'$,
where $k'$ is obtained by decrementing index $\ell$ of $k$ by one.

\item an expression $\closenode \ell {e_1} {e_2}$ evaluates to the value
of $e_2$ at the iteration space vector $k'$ that arises by updating
$k$ by the least value $x$ such that $e_1$ for $k'$ is
$\mathtt{false}$.

\end{itemize}

The semantics for the entire program is then given in denotational
style, by a mapping that associates each variable to the
interpretation of its right-hand side, i.e. to the function that given
an iteration space vector evaluates the expression on that vector.

Due to the decrementing of the iteration index in the interpretation
of $\loopnode \ell {e_1} {e_2}$ expressions, an evaluation of this
expression for a particular $k$ only requires further evaluations at
vectors that are smaller with respect to the component-wise ordering
on tuples, with least element $(0,\ldots,0)$. In contrast, the minima
mentioned in the interpretation of $\closenode
\ell {e_1} {e_2}$ do not necessarily exist. In each such case, the interpretation
of the equation in question, and thus the corresponding variable, is
set to $\bot$, in accordance with the standard treatment of
non-termination in denotational semantics~\cite{winskel_93_formal}.

In contrast to $\phi$-instructions in SSA, the semantics of the
equation-based representation thus does not require control-flow
information to be maintained, as the choice as to which argument of
$\loopnode \ell {e} {e'}$ is evaluated is encoded in the dependency on
the entry at the appropriate position in the iteration space vector.

The article~\cite{PopJS2007} and Pop's
dissertation~\cite{PopDissertation} contain formal details about the
representation, its interpretation, and its formal relationship to a
non-SSA language. In particular, these sources explain how a
conventional program of assignments and loops may be compositionally
converted into a set of equations, in a semantics-preserving
way. Source program expressions are uniquely labeled, so that globally
unique variable names can be generated by differentiating the original
program variables according to the (naturally distinct) labels for
assignments.  Contrary to the unstructured labels used
in~\cite{NielsonNielsonHanking:POPA}, the authors use a class of
labels ("Dewey-like numbers") with the following three properties.
\begin{description}
\item[extensibility:] this feature
is used for generating fresh target variables for $\phi$-operations
\item[hierarchical structure according to the subterm relation:]
   this admits a structure-directed translation into the
   equation-based representation
\item [compatibility with the control flow successor relationship:] 
  this feature -- in combination
with the iteration space vectors -- is employed to define a compositional
(denotational) semantics of the source language. 
\end{description}
In addition to proving a suitable theorem asserting that the
translation is semantics-preserving, the authors also give a reading
of this result in terms of classical models of computations by
interpreting it as the embedding of the RAM model into the model of
partial recursive functions. Similar to out-of-SSA translation, a
conversion is defined (and proven correct) that transforms systems of
equations into imperative programs. Finally, Pop's
dissertation~\cite{PopDissertation} describes how a number of program
analyses may be phrased in terms of the equational language, including
induction variable analysis and other loop optimizations.

%%%%%%%%%%%%

\section{Pointers to the literature}
\label{section:Part1:Semantics:Discussion}

The concept of continuations was introduced multiple times, the
earliest discoveries being attributed by
Reynolds~\cite{Reynolds:LSC1993} and Wadsworth~\cite{Wadsworth00} to
van Wijngaarden
\cite{vanWijngaarden1966} and Landin \cite{Landin1965}. Early uses
and studies of CPS include \cite{Reynolds:1972,Plotkin75}.  

The relative merits of the various functional representations remain
an active area of research, in particular with respect to their
integration with program analyses and optimizing transformations, and
conversions between these formats. Recent contributions include
\cite{DBLP:journals/jfp/DanvyMN07,DBLP:journals/lisp/Reppy02,DBLP:conf/icfp/Kennedy07}.
%Danvy et al.~\cite{DBLP:conf/dsl/DanvySZ09} show how to use
%continuations to translate a subset of Algol into JavaScript.

Closely related to continuations and direct-style functional
representations are \emph{monadic} languages such as Benton et al.'s
MIL~\cite{BentonKennedyRussel:ICFP1998} and Peyton-Jones et al.'s
language~\cite{PeytonJonesShieldsLT:POPL1998}. These partition
expressions into a category of \emph{values} and \emph{computations},
similar to the isolation of primitive terms in let-normal form (see
also~\cite{Reynolds1974,Plotkin75}). This allows one to treat
side-effects (memory access, IO, exceptions,\ldots) in a uniform way,
following Moggi~\cite{Moggi1991}.

Regarding formally worked-out instantiations of the correspondences
for program analyses, Chakravarty et al.~present a functional analysis
of sparse constant propagation~\cite{ChakravartyKZ:COCV03}. Beringer
et al.~\cite{DBLP:journals/entcs/BeringerMS03} consider data flow
equations for liveness and read-only variables, and formally translate
their solutions to properties of corresponding typing
derivations. Laud et al.~\cite{DBLP:journals/tcs/LaudUV06} present a
formal correspondence between dataflow analyses and type systems but
consider a simple imperative language rather than SSA or a functional
representation. The textbook~\cite{NielsonNielsonHanking:POPA}
presents a unifying perspective on program analysis techniques,
including data flow analysis, abstract interpretation, and type
systems.

Modern textbooks on programming language semantics and type systems
include~\cite{winskel_93_formal,GunterBook,PierceTAPL}.

%\bibliographystyle{alpha}
%\bibliography{chapter}
