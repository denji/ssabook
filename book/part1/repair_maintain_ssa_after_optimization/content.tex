% vim:spell:spelllang=en

\newcommand{\var}[1]{\ensuremath{\mathtt{#1}}}
% \newcommand{\phiop}{$\phi$-function}
% \newcommand{\phiops}{\phiop s}

\title{SSA Reconstruction}
\author{Sebastian Hack}

\chapter{SSA Reconstruction \Author{S. Hack}}
% \numberofpages{6}
\inputprogress
\graphicspath{{repair_maintain_ssa_after_optimization/}{part1/repair_maintain_ssa_after_optimization/}}


%Here the currssadir is \currentssadir.

\section{Introduction}

Some optimizations break the single-assignment property of the SSA form by inserting additional definitions for a single SSA value.
A common example is live-range splitting by inserting copy operations or inserting spill and reload code during register allocation.
Other optimizations, such as loop unrolling or jump threading might duplicate code, thus adding additional variable definitions, \emph{and} modify the control flow of the program.
Let us first go through two examples before we present algorithms to properly repair SSA.

\subsection{Live-Range Splitting}

In Figure~\ref{fig:nonssa}, our spilling pass decided to spill a part of the live range of the variable~$x_0$ in the right block.
Therefore, it inserted a store and a load instruction. 
This is indicated by assigning to the memory location $\textbf X$.
The load however is a second definition of $x_0$, hence SSA is violated and has to be reconstructed as shown in Figure~\ref{fig:recons}.
Furthermore, This figure shows that maintaining SSA also involves placing \phifuns.

\begin{figure}[htbp]
	\centering
	\subfloat[Original program] {
		\label{fig:harmless}
		\includegraphics[scale=1.0]{spill_orig.pdf}
		% \input spill_orig.tikz
	}
	\hfill
	\subfloat[Spilling~$x_0$, SSA violated] {
		\label{fig:nonssa}
		\includegraphics[scale=1.0]{spill_nonssa.pdf}
	}
	\hfill
	\subfloat[SSA reconstructed] {
		\label{fig:recons}
		\includegraphics[scale=1.0]{spill_recons.pdf}
	}
	\label{fig:ex1}
	\caption{Adding a second definition}
\end{figure}


Such program modifications are done by many optimizations. 
Not surprisingly, maintaining SSA is often one of the more complicated and error-prone parts in such optimizations;
owing to the insertion of additional \phifuns and the correct redirection of the variable's uses.

\subsection{Jump Threading}

Jump threading is a transformation performed by many popular compilers such as GCC and LLVM.
Jump threading is applied to enable more expressive optimizations. 
Consider following situation:
A block contains a conditional branch that depends on some variable \var x.
In the example shown in Figure~\ref{fig:threading}, the conditional branch tests, if $\var x> 0$.
Assume that the block containing that conditional branch has multiple predecessors and \var x can be proven constant for one of the predecessors.
In the example below, this is shown by the assignment $\var x_1\gets 0$. 
Jump threading now partially evaluates the conditional branch by directly making the corresponding successor of the branch block a successor of the \emph{predecessor} of the branch.
To this end, the code of the branch block is duplicated and attached to the predecessor. 
This also duplicates potential definitions of variables in the branch block.
Hence, SSA is destroyed for those variables and has to be reconstructed. 
In contrast to the examples above, the control flow has changed however which poses an additional challenge for an efficient SSA reconstruction algorithm. 
\begin{figure}[htbp]
	\begin{center}
		\subfloat[Original program] {
			\includegraphics{jump_thread_orig.pdf}
		}
		\hfill
		\subfloat[Jump threaded, SSA broken] {
			\includegraphics{jump_thread_broken.pdf}
		}
		\hfill
		\subfloat[SSA repaired] {
			\includegraphics{jump_thread_repair.pdf}
		}
	\end{center}
	\caption{Jump Threading}
	\label{fig:threading}
\end{figure}


\section{General Considerations}
In the following, we will discuss two algorithms.
The first is an adoption of the classical dominance-frontier based algorithm.
The second performs a search from the variables' uses to the definition and places \phifuns on demand at appropriate places. 
In contrast to the first, the second algorithm might not yield minimal SSA form (in Cytron's sense).
However, it does not need to update its internal data structures when the CFG is modified. 

We consider following scenario:
The program is represented as a control-flow graph (CFG) and is in SSA form.
For the sake of simplicity, we assume that each instruction in the program only writes to a single variable.
Due to the single-assignment property of the SSA form, we can then identify the program point of the instruction and the variable. 
An optimization/transformation now violates SSA by inserting additional definitions for an existing SSA variable, like in the examples above.
The original variable and the additional definitions can be seen as a single non-SSA variable that has multiple definitions and uses.

In the following, $v$ will be such a non-SSA variable.
$D$~is a set of SSA variables being the definitions of~$v$.
A use of a variable is a pair consisting of a program point (a variable) and an integer denoting the index of the operand at the using instruction.

Both algorithms which we are going to present share the same driver routine (Algorithm~\ref{alg:ssaconstr_driver}).
First, every basic block~\verb|b| is equipped with a list~\verb|b.defs| that contains all instructions in the block which define one of the variables in~$D$.
This list is sorted according to the schedule of the instructions in the block from back to front.
Hence, the latest definition is the first in the list.

Then, all uses of the variables in~$D$ are scanned.
For every program point~$\ell$ where a variable in~$D$ is used, search locally in $\ell$'s block for a definition of variable in~$D$.
By scanning the block's list from back to front, we find the latest such use if one exists. 
If the variable has no definition in the block, we have to find the definition that reaches this block from \emph{outside.}
Here we have to differentiate whether the user is a \phifun or not.
If it is a \phifun, the block of the use is the corresponding predecessor block.
We use two functions \verb|find_def_from_begin| and \verb|find_def_from_end| that find the reaching definition at the beginning and end of a block, respectively. 
As can seen from Algorithm~\ref{alg:ssaconstr_driver}, \verb|find_def_from_end| can be expressed in terms of \verb|find_def_from_begin|. 
\verb|find_def_from_end| additionally considers definitions in the block, while \verb|find_def_from_begin| does.
Our two approaches only differ in the implementation of the function \verb|find_def_from_begin|.
The differences are described in the next two sections.
\begin{algorithm}
	\caption{SSA Reconstruction Driver}
	\label{alg:ssaconstr_driver}

\begin{verbatim}
proc ssa_reconstruct(set of var D):
  for d in D:
    b = d.block
    insert d in b.defs according to schedule
    # latest definition is first in list

  for each use u of D:
    v = get_used_var(u)
    b = v.block
    d = None

    # search for a local definition in the block
    for e in b.defs:
      if v == e and is_later(u, e):
        d = e
        break
        
    # no local definition was found, search in the predecessors
    if d == None:
      # if the user is a phi we have to start the search 
      # at the end the corresponding predecessor block 
      if u.is_phi():
        d = find_def_from_end(u.block.pred(u.index), ...)
      else:
        d = find_def_from_begin(b, ...)

    rewrite use at u to d

proc find_def_from_end(block b, ...):
  if not b.defs.empty():
    return b.defs[0]
  return find_def_from_begin(b, ...)
\end{verbatim}
\end{algorithm}

\section{Reconstruction based on the Dominance Frontier}
This algorithm, proposed by Sastry and Ju~\cite{sastry98}, follows the same principles as the classical SSA construction algorithm by Cytron et al.~\cite{cytron:1991:ssa}.
We compute the iterated dominance frontier (IDF) of $D$ (cf.~Appel's book~\cite{appel:2002:modern} for an algorithm on how to compute the IDF).
This set is a sound over-approximation of the set where \phifuns must be placed (it might contain blocks where a \phifun would be dead).
Then, we search for each use the corresponding reaching definition.
This search starts at the block of~$u$.
If that block is in the IDF of~$D$ a \phifun needs to be placed at its entrance.
The operands of that \phifun are then (recursively) searched in the predecessors of the block.
If the block is not in the IDF, the search continues in the block's immediate dominator. 
This is because in SSA, every use of a variable must be dominated by its definition\footnote{The definition of an operand of a \phifun has to dominate the according predecessor block.}.
If the block is not in the IDF, the reaching definition is the same for all predecessors and hence for the immediate dominator of this block.
Note that by rewiring the uses of several variables, some variables defined by \phifuns may not be used anymore.
A dead code elimination pass after SSA reconstruction will remove these. 
Algorithm~\ref{alg:ssaconstr} shows this procedure in pseudo-code.

\begin{algorithm}
  \caption{SSA Reconstruction based on Dominance Frontiers}
  \label{alg:ssaconstr}
\begin{verbatim}
proc find_def_from_begin(block b, set of blocks F):
  if b in F:
    d = new_phi(b)
    i = 0
    for p in b.preds: 
      o = find_def_from_end(p, F)
      set i-th operand of d to o
      i = i + 1
  else:
    d = find_def_from_end(b.idom, F)
  b.def = d
  return d
\end{verbatim}
\end{algorithm}

\section{Search-based Reconstruction}

The second algorithm we present here is similar to the construction algorithm that Click describes in his thesis~\cite{click:thesis}.
Although his algorithm is designed to construct SSA from the abstract syntax tree, it also works well on control flow graphs.
Its major advantage over the algorithm presented in the last section is that it does neither require dominance information nor dominance frontiers.
Thus it is well suited to be used in transformations that change the control flow graph.
Its disadvantage is that potentially more blocks have to visited during the reconstruction.
The principle idea is to start a search from every use to find the corresponding definition inserting \phifuns on the fly while caching the found definitions at the basic blocks.
This is similar to the implementation of a data-flow analysis, that places the \phifuns.
As in the last section, we only consider the reconstruction for a single variable.
If multiple variables have to be reconstructed, the algorithm can be applied to each variable separately.

We perform a backward depth-first search in the CFG to collect the reaching definitions at each block. 
To mark a block as visited, the reaching definition of that block is inserted into the \verb|defs| list of that block.
If the CFG is a DAG, all predecessors of a block can be visited before the block itself is processed (post-order traversal).
Hence, all reaching definitions at a block~\verb|b| can be computed before we decide whether to place a \phifun in~\verb|b| or not.
If more than one definition reaches the block, we need to place a \phifun.

If the CFG has loops, there are blocks for which not all reaching definitions can be computed before we can decide whether a \phifun has to be placed.
Recursively computing the reaching definitions for a block~\verb|b| can end up at~\verb|b| itself.
To avoid infinite recursion, we create a \phifun without operands in the block before descending to the predecessors. 
Hence, if a variable has no definition in a loop, the \phifun placed in the header eventually reaches itself (and can later be eliminated). 
When we return to~\verb|b| we decide whether a \phifun has to be placed in~\verb|b| by looking at the reaching definition for every predecessor.
If the set of reaching definitions is a subset of $\{a,x\}$ where~$x$ is the \phifun inserted at~\verb|b|, then \phifun is not necessary and we can propagate~$a$ further downwards. 
Otherwise, we place a \phifun.

\begin{algorithm}
	\caption{Search-based SSA Reconstruction}
	\label{alg:ssaconstr_click}

\begin{verbatim}
proc find_def_from_begin(block b):
  phi = make_phi()
  if b.defs.empty():
    b.defs = [ phi ] 

  reaching_defs = []
  for p in b.preds:
    reaching_defs += find_def_from_end(p)
  if phi_necessary(reaching_defs):
    set_arguments(phi, reaching_defs)
    d = phi
  else:
    reaching_defs.remove(phi)
    d = reaching_defs[0]
  return d
\end{verbatim}
\end{algorithm}

\section{Conclusions}

Some optimizations, such as loop unrolling or live-range splitting destroy the single-assignment property of the SSA form.
In this chapter we presented two generic algorithms to reconstruct SSA.
The algorithms are independent of the transformation that violated SSA and can be used as a black box:
For every variable for which SSA was violated, a routine is called that restores SSA.
The presented algorithms differ in the prerequisites and their runtime behavior:
\begin{enumerate}
	\item 
		The first is based on the iterated dominance frontiers like the classical SSA construction algorithm by Cytron et al.~\cite{cytron:1991:ssa}.
		Hence, it is less suited for optimizations that also change the flow of control since that would require recomputing the iterated dominance frontiers.
		On the other hand, by using the iterated dominance frontiers, the algorithm can find the reaching definitions quickly by scanning the dominance tree upwards.
	\item 
		The second algorithm does not depend on additional analysis information such as iterated dominance frontiers or the dominance tree.
		Thus, it is well suited for transformations that change the CFG because no information needs to be recomputed.
		On the other hand, it might find the reaching definitions slower than the first one because they are searched by a depth-first search in the CFG.
\end{enumerate}
Both approaches construct \emph{pruned} SSA (TODO: cite other chapter), i.e.~no \phifun is dead. 
One can also show that the first approach produces minimal SSA in the sense of Cytron et al.~\cite{cytron:1991:ssa} whereas the second approach might create superfluous \phifuns in the case of irregular control flow.
