
\begin{figure}
{\small
{\bf Input:} Intermediate code for method being optimized, augmented
with the $\DS$ and $\DD$ relations defined in Section~\ref{svalnum}.

\vspace{14pt}

{\bf Output:} Transformed intermediate code after performing scalar
replacement.

\vspace{14pt}

{\bf Algorithm:}
\begin{enumerate}

\item \label{item:arrayssa}
{\bf Build extended Array SSA form for each heap array.}

Build Array SSA form, inserting control $\phi$,
$d\phi$ and $u\phi$ functions as outlined in Section~\ref{heaparray},
and renaming of all heap array definitions and uses.

As part of this step, we annotate each call instruction with dummy
defs and uses of each heap array for which a def or a use can reach the call
instruction.  
If interprocedural analysis is possible, the call
instruction's heap array defs and uses can
be derived from a simple flow-insensitive
summary of the called method.  

\item \label{item:dflow}
{\bf Perform index propagation.}
\begin{enumerate}
\item Walk through the extended Array SSA intermediate representation,
and for each $\phi$, $d\phi$, or $u\phi$ statement, create a 
dataflow equation with the appropriate operator as listed in 
Figures~\ref{fig:scalrep-dphi},~\ref{fig:scalrep-uphi}~or~\ref{fig:scalrep-join}.
\item Solve the system of dataflow equations by iterating to a fixed point.
\end{enumerate}
After index propagation, the lattice value of each 
heap array, $A_i$,
is 
$\L(A_i) = \{\; \V(\vec{k}) \; | \;
\mbox{location $A[\vec{k}]$ is ``available'' at def $A_i$ (and all uses of $A_i$)} \; \}$.

\item \label{item:analysis}
{\bf Scalar replacement analysis.}

\begin{enumerate}
\item Compute $\URS = \{\; \mbox{use $A_j[\vec{x}]$} \; | \; \exists\; \V(\vec{x}) \; \in \; \L(A_j)\;\}$ \ie\
use $A_j[\vec{x}]$ is placed in \URS\ if and only if location $A[\vec{x}]$ is available
at the def of $A_j$ and hence at the use of $A_j[\vec{x}]$.
(Note that $A_j$ uniquely identifies a use, since all uses are renamed in
extended Array SSA form.)

\item Compute $\DRS = \{\;  \mbox{def $A_i[\vec{k}]$} \; | \; \exists \; \mbox{use $A_j[\vec{x}] \in \URS$ with $\V(\vec{x})=\V(\vec{k})$} \; \}$ \ie\ def  $A_i[\vec{k}]$ is placed in \DRS\ 
if and only if a use $A_j[\vec{x}]$ was placed in \URS\ with  $\V(\vec{x})=\V(\vec{k})$.
\end{enumerate}

\item \label{item:transform}
{\bf Scalar replacement transformation.}

Apply scalar replacement actions selected in step~\ref{item:analysis}
above
to the {\it original} program and
obtain the transformed program.

\end{enumerate}
}
\caption{Overview of Redundant Load Elimination algorithm.}
\label{fig:overview}
\end{figure}

\REM{
\begin{figure}
{\small
{\bf Input:} Data flow equations for the extended Array SSA form of
the input method.

{\bf Outputs:}
Scalar replacement actions defined by the following sets:
\begin{enumerate}
\item \DRS\ = set of non-$\phi$ array defs selected for scalar replacement. 
 
For each def $A_i \in \DRS$,
the def
statement $A_i[\vec{k}] := \mbox{\sc rhs}$ in the original program should be replaced by
two statements, $\At_n := \mbox{\sc rhs} ;  A_i[\vec{k}] := \At_n$,
where $n=\V(\vec{k})$ is the value number for index expression $\vec{k}$.
%(The $\At_n := \mbox{\sc rhs}$ statement will usually be optimized away
%by copy propagation~\cite{AhSU86}.)

\item \URS\ = set of non-$\phi$ array uses (loads) selected for scalar replacement.  

For each use $A_j \in \URS$, the use
statement $\mbox{\sc lhs} := A_j[\vec{x}]$ in the original program should be replaced by the
statement, $\mbox{\sc lhs} := \At_n$,
where $n=\V(\vec{x})$ is the value number for index expression $\vec{x}$.
%The $\mbox{\sc lhs} := \At_n$ statement will also usually be optimized away
%by copy propagation.
\end{enumerate}
\REM{
In the above scalar replacement actions, $\At_n$ represents
a single
scalar temporary created for all references to array $A$ in the
original program for which the index expression has value number = $n$.
}

{\bf Algorithm:}
\begin{enumerate}
\item Initialization:
\begin{enumerate}
\item Initialize the lattice value for each dummy heap array definition
$\H_0$ inserted in the start block
to $\L(\H_0) = \bot$.  
Also, insert into {\it worklist} all  equations $E$
that contain $\L(\H_0)$ in their RHS.

\item Initialize the lattice value for all other heap arrays 
$\H_i$ to $\L(\H_i) = \top$.
\end{enumerate}

\item Index propagation (fixpoint iteration):
\begin{programa}
\Ta {\bf while} ( {\it worklist} is not empty ) \{ \\
\Tb $E :=$ remove any equation from {\it worklist}\\
\Tb Recompute $\L(v)$, the LHS of equation $E$, based on the values of $E$'s RHS terms\\
\Tb {\bf if} ( LHS of equation $E$ has changed ) \{ \\
\Tc {\bf for each} ( equation $E'$ that uses $\L(v)$ in its RHS ) \{ \\
\Td Insert equation $E'$ into {\it worklist}\\
\Tc \} \\
\Tb \} \\
\Ta \} \\
\end{programa}
After index propagation, the lattice value of each ($\phi$ or non-$\phi$) definition, $A_i$,
is 
$\L(A_i) = \{\; \V(\vec{k}) \; | \;
\mbox{location $A[\vec{k}]$ is ``available'' at def $A_i$ (and all uses of $A_i$)} \; \}$.

\item Compute $\URS = \{\; \mbox{use $A_j[\vec{x}]$} \; | \; \exists\; \V(\vec{x}) \; \in \; \L(A_j)\;\}$ \ie\
use $A_j[\vec{x}]$ is placed in \URS\ if and only if location $A[\vec{x}]$ is available
at the def of $A_j$ and hence at the use of $A_j[\vec{x}]$.
(Note that $A_j$ uniquely identifies a use, since all uses are renamed in
extended Array SSA form.)

\REM{
Note that since the program is in Array SSA form, there is a single
definition of $A_j$ and a single definition of $\vec{x}$, both of which must
dominate the use $A_j[\vec{x}]$ (which is a use of $A_j$ and a use of $\vec{x}$).
Also, $\vec{x}$ must have the same value number as the index of the assignment
that propagated $\V(\vec{x})$ into $\L(A_j)$.
}

\REM{
{\it Vivek: doesn't the above need to be justified wrt a modification to A and an modification to x between the def and the use. its true but it needs a reason.}
}

\item Compute $\DRS = \{\;  \mbox{def $A_i[\vec{k}]$} \; | \; \exists \; \mbox{use $A_j[\vec{x}] \in \URS$ with $\V(\vec{x})=\V(\vec{k})$} \; \}$ \ie\ def  $A_i[\vec{k}]$ is placed in \DRS\ 
if and only if a use $A_j[\vec{x}]$ was placed in \URS\ with  $\V(\vec{x})=\V(\vec{k})$.
\end{enumerate}
}
\caption{Algorithm for scalar replacement analysis for redundant load elimination.}
\label{alg:load}
\end{figure}
}


\begin{figure}%[htbp]
\begin{center}
\begin{tabular}{|l||c|c|c|}
\hline
$\L(A_2)$ & $\L(A_0) = \top$ & $\L(A_0) = \langle (\vec{i_1}), \ldots \rangle $ & $\L(A_0) = \bot$ \\
\hline \hline
$\L(A_1) = \top$ & $\top$ & $\top$ & $\top$ \\
\hline
$\L(A_1) = \langle(\vec{i'})\rangle$ & $\top$ & $\Update((\vec{i'}),\langle (\vec{i_1}), \ldots \rangle)$ & $\langle(\vec{i'})\rangle$ \\
\hline
$\L(A_1) = \bot$ & $\bot$ & $\bot$ & $\bot$ \\
\hline
\end{tabular}
\end{center}
\caption{Lattice computation for \protect{$\L(A_2)  =  \L_{d\phi}(\L(A_1), \L(A_0))$}
where $A_2 := d\phi(A_1, A_0)$ is
a definition $\phi$ operation}
\label{fig:scalrep-dphi}
\end{figure}

\begin{figure}%[htbp]
\begin{center}
\begin{tabular}{|l||c|c|c|}
\hline
$\L(A_2)$ & $\L(A_0) = \top$ & $\L(A_0) = \langle (\vec{i_1}), \ldots \rangle $ & $\L(A_0) = \bot$ \\
\hline \hline
$\L(A_1) = \top$ & $\top$ & $\top$ & $\top$ \\
\hline
$\L(A_1) = \langle(\vec{i'})\rangle$ & $\top$ & $ \L(A_1) \cup \L(A_0)$ & $ \L(A_1)$ \\
\hline
$\L(A_1) = \bot$ & $\bot$ & $\bot$ & $\bot$ \\
\hline
\end{tabular}
\end{center}
\caption{Lattice computation for \protect{$\L(A_2)  =  \L_{u\phi}(\L(A_1), \L(A_0))$}
where $A_2 := u\phi(A_1, A_0)$ is
a use $\phi$ operation}
\label{fig:scalrep-uphi}
\end{figure}


\begin{figure}%[htbp]
\begin{center}
\begin{tabular}{|l||c|c|c|}
\hline
$\L(A_2) = \L(A_1) \sqcap \L(A_0) $ & $\L(A_0) = \top$ & $\L(A_0) = \langle (\vec{i_1}), \ldots \rangle $ & $\L(A_0) = \bot$ \\
\hline \hline
$\L(A_1) = \top$ & $\top$ & $\L(A_0)$ & $\bot$ \\
\hline
$\L(A_1) = \langle(\vec{i'_1}), \ldots\rangle$ & $\L(A_1)$ & $\L(A_1) \cap \L(A_0)$ & $\bot$ \\
\hline
$\L(A_1) = \bot$ & $\bot$ & $\bot$ & $\bot$ \\
\hline
\end{tabular}
\end{center}
\caption{Lattice computation for 
$\L(A_2) = \L_{\phi}(\L(A_1), \L(A_0)) = \L(A_1) \sqcap \L(A_0) $,
where $A_2 := \phi(A_1, A_0)$ is
a control $\phi$ operation}
\label{fig:scalrep-join}
\end{figure}

\begin{figure}
\begin{tabular}[t]{|c|c|c|c|}
\hline
\begin{minipage}[t]{1.25in}
(a) Extended Partial Array SSA form: 
\begin{programa}
p := new Type1\\
q := new Type1\\
. . .\\
$\H^x_1[p]$ := ...\\
$\H^x_2$ := $d\phi(\H^x_1, \H^x_0)$\\
$\H^x_3[q]$ := ...\\
$\H^x_4$ := $d\phi(\H^x_3, \H^x_2)$\\
... := $\H^x_4[p]$ \\
$\H^x_5$ := $u\phi(\H^x_4, \H^x_3)$
\end{programa}
\end{minipage}
&
\begin{minipage}[t]{1.5in}
(b) After index propagation:
\begin{eqnarray*}
\L(\H^x_0) & = & \{ \; \} \\
\L(\H^x_1) & = & \{ \V(p) \} \\
\L(\H^x_2) & = & \{ \V(p) \} \\
\L(\H^x_3) & = & \{ \V(q) \} \\
\L(\H^x_4) & = & \{ \V(p), \V(q) \} \\
\L(\H^x_5) & = & \{ \V(p), \V(q) \} 
\end{eqnarray*}
\end{minipage}
&
\begin{minipage}[t]{1.5in}
(c) Scalar replacement actions selected:
\begin{eqnarray*}
\URS & = & \{ \H^x_4[p] \} \\
\DRS & = & \{ \H^x_1[p] \} 
\end{eqnarray*}
\end{minipage}
&
\begin{minipage}[t]{1.3in}
(d) After transforming original program:
\begin{programa}
p := new Type1\\
q := new Type1\\
. . .\\
$\At_{\V(p)}$ := ...\\
p.x := $\At_{\V(p)}$\\
q.x := ...\\
... := $\At_{\V(p)}$
\end{programa} 
\end{minipage}
\\
\hline
\end{tabular}
\caption{Trace of load elimination algorithm from figure
\protect{\ref{fig:overview}} for program in figure \protect{\ref{fig:ex2}}(a)}
\label{fig:ex2a:trace}
\end{figure}

Figure~\ref{fig:overview} outlines our algorithm for identifying
uses (loads) of 
heap array elements that are redundant with respect to prior 
defs and uses of the same heap array.
The algorithm's main analysis is {\it index propagation},
which identifies the set of indices that
are {\em available} at a specific def/use  $A_i$ of heap array $A$.

Index propagation is a dataflow problem, which computes a lattice value
$\L(\H)$ for each heap variable $\H$ in the Array SSA form.  This
lattice value $\L(\H)$ is a set of value number vectors $\{\vec{i_1},\ldots \}$, such that a load of $\H[\vec{i}]$ is available (previously stored in a register)
if $\V(\vec{i}) \in \L(\H)$.
Figures~\ref{fig:scalrep-dphi},~\ref{fig:scalrep-uphi}~and~\ref{fig:scalrep-join} give the lattice computations which define the index propagation solution.
The notation 
$\Update(\vec{i'},\langle \vec{i_1}, \ldots \rangle)$
used in the middle cell in 
figure~\ref{fig:scalrep-dphi} denotes a special update of 
the list $\L(A_0) = \langle \vec{i_1}, \ldots \rangle$
with respect to index $\vec{i'}$.
$\Update$ involves four steps:
\begin{enumerate}
\item Compute the list $T = \{\;\vec{i_j}\;|\;\vec{i_j}\in\L(A_0)
\mbox{~and~}\DD(\vec{i'},\vec{i_j})=\mbox{\it true}\;\}$.  List $T$
contains only those indices from $\L(A_0)$ that are
{\it definitely different} from $\vec{i'}$.

\item Insert $\vec{i'}$ into $T$ to obtain a new list, $I$.
\item If the 
size of list
$I$ exceeds the threshold size $Z$, then one of the indices in $I$ is
dropped from the output list so as to satisfy the size constraint.  
(Since the size of $\L(A_0)$ must have been $\leq Z$, it is sufficient
to drop only one index to satisfy the size constraint.)
\item Return $I$ as the value of 
$\Update(\vec{i'},\langle \vec{i_1}, \ldots \rangle)$.
\end{enumerate}

\REM{
The lattice computations for arrays are similar.  Since an $n$-dimensional
array is represented by an $n+1$-dimensional Heap variable, the 
index corresponding to an array load or store is an $n+1$-tuple.
For example, consider a Heap variable $A$ representing a one-dimensional
Java array.  Then $A$ is a two-dimensional array, and
an {\em index} into $A$ is a pair $< $p$, $v$>$, representing the
def or use of array element written as {\tt p[v]} (note that {\tt p}
is a pointer, and must be analyzed as such).
The index propagation solution
for $A$ will be a set of value number pairs $S$, such that 
we know $A[<p,x>]$ is available if $\exists <v,w> \in S s.t. DS(V(p),v) 
\wedge DS(V(x),w)$.
}

\REM{
{\it TODO: explain that Figure~\ref{fig:scalrep-dphi} is used for a
WRITE of a heap array, and a simple pass-through copy  operation is
used for a READ of a heap array.}
}

After index propagation, the algorithm selects an array use (load),
$A_j[\vec{x}]$, for scalar replacement if and only if index propagation
determines that an index with value number $\V(\vec{x})$ is available at the 
def of $A_j$.
If so, the use is included in  \URS\, the set of uses selected
for scalar replacement.
Finally, an array def, $A_i[\vec{k}]$, is selected for
scalar replacement if and only if some use $A_j[\vec{x}]$ was placed in \URS\ such
that  $\V(\vec{x})=\V(\vec{k})$.  
All such defs are included in \DRS\, the set of defs selected for scalar
replacement.



Figure~\ref{fig:ex2a:trace} illustrates a trace of this load
elimination algorithm for the example program in
figure~\ref{fig:ex2}(a).  Figure~\ref{fig:ex2a:trace}(a) shows the
partial Array SSA form computed for this example program.  The results
of index propagation are shown in figure~\ref{fig:ex2a:trace}(b).
These results depend on \dd\ analysis establishing that $\V(p) \not=
\V(q)$ by using allocation site information as described in
Section~\ref{svalnum}.
Figure~\ref{fig:ex2a:trace}(c) shows the scalar replacement actions
derived from the results of index propagation, and
Figure~\ref{fig:ex2a:trace}(d) shows the transformed code after
performing these scalar replacement actions.  The load of {\tt p.x}
has thus
been eliminated in the transformed code, and replaced by a use
of the scalar temporary, $\At_{\V(p)}$.

We now present a brief complexity analysis of the
redundant load elimination algorithm in Figure~\ref{fig:overview}.
Note that index propagation can be performed separately for each heap
array.  Let $k$ be the maximum number of defs and uses for a single
heap array.  Therefore, the number of $d\phi$ and $u\phi$ functions
created for a single heap array will be $O(k)$.  Based on past
empirical measurements for scalar SSA form~\cite{CFRWZ91a}, we can
expect that the number of control $\phi$ functions for a single heap
array will also be $O(k)$ in practice (since there are $O(k)$ names
created for a heap array).  Recall that the maximum size of
a lattice value list, as well as the maximum height of the lattice, is
a compiler-defined constant, $Z$.  Therefore, the worst case
execution-time complexity for index propagation of a {\it single heap
array} is $O(k \times Z^2)$.

To complete the complexity analysis, we define a size metric for
each method, $S = \max(\mbox{\# instrs in method}, k \times (\mbox{\# call instrs in method}))$.
The first term (\# instrs in method) usually dominates the $\max$ function in practice.
Therefore,
the worst-case complexity for index propagation
for all heap arrays is
$$
\sum_{\mbox{heap array $A$}} O(k_A \times Z^2) = O(S \times Z^2),
$$
since $\sum_A k_A$ must be $O(S)$.
Hence the execution time is a linear with a $Z^2$ factor.
As mentioned earlier, the value of $Z$ can be adjusted to trade off
precision and overhead.  For the greatest precision, we can set $Z = O(k)$, which yields a worst-case
$O(S \times k^2)$
algorithm.  
In practice, $k$ is usually small resulting in linear execution time.
This is the setting used to obtain the experimental results reported
in Section~\ref{results}.

We conclude this section with a brief discussion of the impact of
the Java Memory Model (JMM).
It has been observed that redundant load elimination
can be an illegal
transformation for multithreaded programs written for a
memory model, such as the JMM,
that includes the memory coherence
assumption~\cite{pugh99}. 
(This observation does not apply to single-threaded programs, such
as the benchmark programs used for the experimental results
in Section~\ref{results}.)
However, it is likely that the Java memory model will be revised
in the near future,
and that the new version will not require memory coherence~\cite{pugh99b}.
However, if necessary, our algorithms can be modified to 
obey memory coherence by simply treating each $u\phi$ function 
as a $d\phi$ function \ie\ by treating each array use as an array def.
Our implementation supports these semantics with a command-line option.
As in interesting side note, we observed that changing
the data-flow equations to support the strict memory model involved
changing fewer than ten lines of code.


\REM{
As a concluding note, we observe that the algorithm in figure~\ref{alg:load}
may sometimes select a \DRS\ that is larger than necessary.  For example,
if the program in figure~\ref{fig:ex2}(a) had a second def to {\tt p.x}
at the bottom, this def would also be included in \DRS\ even though no
loads might have been eliminated with respect to this second def.
However, selecting a larger \DRS\ than necessary is harmless
because the only potential overhead of including a def in \DRS\ is
the creation of an extra register-register move instruction
of the form, $\At_n := \mbox{\sc rhs}$, which is usually optimized away
by the clean-up phase in step~\ref{item:cleanup} of Figure~\ref{fig:overview}.





{\it Additional details on the algorithm for elimination of
redundant loads with respect to prior defs, including correctness
and complexity proofs, have been
suppressed due to space limitations.
}


A limitation of the algorithm in
Section~\ref{load} is that it does not eliminate a load that
is redundant with respect to a {\it prior load}.  For the example
program in figure~\ref{fig:ex2}(b), the algorithm in
section~\ref{load} will select an empty \URS\ and an empty
\DRS.  The key insight in enabling elimination of a load redundant
with respect to a prior load is to modify the dataflow processing to
treat a use as if it were also a definition for this purpose. This can
be accomplished by inserting a dummy def of an array element at each
use.  
This algorithm should be performed after the algorithm in Section~\ref{load},
otherwise the insertion of the extra defs in this section
might prevent some of the
scalar replacement transformations from Section~\ref{load} from being
enabled.


Therefore, the algorithm for elimination of
redundant loads with respect to prior defs and prior uses
begins
by inserting a dummy array def after each array use statement.
The algorithm from Section~\ref{load}
is then applied to the modified program as though we were only eliminating
redundant loads with respect to prior defs.
The main extension is to identify \DDRS, the set of dummy defs that were selected for
scalar replacement, separately from
\DRS\ (the set of non-dummy defs selected for scalar replacement),
because the scalar replacement actions are different for the two
cases.

{\it Additional details on the algorithm for elimination of
redundant loads with respect to prior defs and prior uses, including correctness
and complexity proofs, have been
suppressed  due to space limitations.
}

}

\REM{
Figure~\ref{alg:load2} outlines our algorithm for elimination of
redundant loads with respect to prior defs and prior uses. This algorithm
subsumes the algorithm in figure~\ref{fig:aload}.  The algorithm
begins in step~1
by inserting a dummy def after each array use statement.
Steps~2, 3, 4 are then performed as though we were only eliminating
redundant loads with respect to prior defs \ie\ as in
the approach from section~\ref{load}.
(Insertion of dummy defs ensures that redundancy with respect to prior loads
will also be accounted for.)
Step~5 then identifies \DDRS, the set of dummy defs that were selected for
scalar replacement.  We need to separate \DDRS\ from \DRS\ because the
scalar replacement actions are different for the two cases.
Finally, all dummy defs that are not included in \DDRS\ are deleted
from the program in step~6.

Figure~\ref{fig:ex2b:trace} illustrates a trace of this more
general load
elimination algorithm for the example program in
figure~\ref{fig:ex2}(b).  
Figure~\ref{fig:ex2b:trace}(a) shows the
original program augmented
with dummy defs.  For convenience, we only show a dummy def for the
first use of {\tt A[i]} though our algorithm will create dummy
defs for both uses.
Figure~\ref{fig:ex2b:trace}(b) shows the partial Array SSA form
for the augmented program in figure~\ref{fig:ex2b:trace}(a).
Note that, except for the initial use of $A_0[i]$, the partial
Array SSA in figure~\ref{fig:ex2b:trace}(b) is identical to the
partial Array SSA form that we see earlier in figure~\ref{fig:ex2a:trace}(a).
Hence, the results of index propagation are the same as the
results obtained in 
figure~\ref{fig:ex2a:trace}(b), and are not shown again in
figure~\ref{fig:ex2b:trace}.
Figure~\ref{fig:ex2b:trace}(c) shows the scalar replacement actions
derived from the results of index propagation; the key 
difference from figure~\ref{fig:ex2b:trace}(a) is that we now have
a \DDRS\ set as well.
Finally,
figure~\ref{fig:ex2b:trace}(d) shows the transformed code after
performing these scalar replacement actions.  The second
load of {\tt A[i]}
has thus
been eliminated in the transformed code.  

\begin{figure}
{\bf Input:} Original program.

\vspace{12pt}

{\bf Outputs:}
Scalar replacement actions defined by the following sets:
\begin{enumerate}
\item \DRS\ = set of non-$\phi$ array defs selected for scalar replacement,
just as in figure~\ref{alg:load}.

\item \URS\ = set of non-$\phi$ array uses (loads) selected for scalar replacement,
just as in figure~\ref{alg:load}.  

\item \DDRS\ = set of dummy non-$\phi$ array defs selected for scalar replacement.  The scalar replacement action for a dummy def is different from the
action for a def in \DRS.

For each dummy def $A_i \in \DDRS$,
the def
statement $A_i[k] := \ldots$ in the original program should be replaced by
the single statement, $\At_n := \ldots$,
where $n=\V(k)$ is the value number for index expression $k$.
The $\At_n := \ldots$ statement will usually be optimized away
by copy propagation~\cite{AhSU86}.
\end{enumerate}

In addition, the program is updated with insertion of dummy defs in
\DDRS\ so that they are available for scalar replacement (the dummy
defs were not present in the input program).
\vspace{12pt}

{\bf Algorithm:}
\begin{enumerate}
\item For each array use statement of the form, {\sc lhs}~{\tt :=~A[x]},
insert a dummy array def of the form, {\tt A[x]~:=}~{\sc lhs} immediately
after the use statement.  Let \DDS\ be the set of all such dummy array defs.

\item Compute value numbering and \dd\ analysis information as
well as partial Array SSA form (for the
modified program created after step~1).

\item Perform {\it index propagation} as described in step~2 of
figure~\ref{alg:load}.
After index propagation, the lattice value of each ($\phi$ or non-$\phi$) definition, $A_i$,
is 
$$\L(A_i) = \{\; \V(k) \; | \;
\mbox{location $A[k]$ is ``available'' at def $A_i$} \; \}$$
For the purpose of index propagation,
the dummy definitions created in step~1 are treated just like other non-$\phi$
definitions in the program.

\item Compute \URS\ and \DRS\ as described in steps~3 and 4 of
figure~\ref{alg:load}.

\item Compute $\DDRS := \DDS \cap \DRS$ \ie\ a def is placed in \DDRS\ if
and only if it is a dummy def that was selected for replacement.

\item Compute $\DRS := \DRS - \DDS$ \ie\ remove all dummy defs from 
\DRS.

\item Clean up by deleting from the program each def in $(\DDS - \DDRS)$
\ie\ by deleting each dummy def that has not been included in \DDRS.  
\end{enumerate}
\caption{Algorithm for scalar replacement of redundant loads with respect
to prior defs and prior uses}
\label{alg:load2}
\end{figure}

 
\begin{figure}
\begin{tabular}[t]{|c|c|c|c|}
\hline
\begin{minipage}[t]{1.0in}
(a) Original program augmented
with dummy defs:
\begin{programa}
i := k \\
j := k + 1 \\
. . .\\
{\sc lhs} := A[i]\\
$A[i] := \mbox{\sc lhs}$\\
A[j] := $\ldots$\\
$\ldots$ := A[i]\\
. . .
\end{programa}
\end{minipage}
&
\begin{minipage}[t]{1.0in}
(b) Partial Array SSA form: 
\begin{programa}
i := k \\
j := k + 1 \\
. . .\\
{\sc lhs} := A$_0$[i]\\
$A_1[i] := \mbox{\sc lhs}$\\
A$_2$ := $d\phi$($A_1$,A$_0$) \\
A$_3$[j] := $\ldots $ \\
A$_4$ := $d\phi$(A$_3$,A$_2$) \\
$\ldots$ := A$_4$[i]
\end{programa}
\end{minipage}
&
\begin{minipage}[t]{1.5in}
(c) Scalar replacement actions selected:
\begin{eqnarray*}
\URS & = & \{  A_4[i] \} \\
\DRS & = & \{  \}  \\
\DDRS & = & \{  A_1[i] \} 
\end{eqnarray*}
\end{minipage}
&
\begin{minipage}[t]{1.3in}
(d) After transforming original program:
\begin{programa}
i := k \\
j := k + 1 \\
. . .\\
{\sc lhs} := A[i]\\
A\_temp\_9 := {\sc lhs}\\
A[j] := $\ldots$\\
$\ldots$ := A\_temp\_9
\end{programa}
\end{minipage}
\\
\hline
\end{tabular}
\caption{Trace of load elimination algorithm from figure
\protect{\ref{alg:load2}} for program in figure \protect{\ref{fig:ex2}}(b).
Assumes $\DD$(i, j).}
\label{fig:ex2b:trace}
\end{figure}



}
