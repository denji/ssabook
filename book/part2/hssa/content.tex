\chapter{Hashed SSA form: HSSA \Author{M. Mantione \andAuthor F. Chow}}
\inputprogress
\label{chapter:hssa}
\index{aliasing}
\index{Hashed-SSA form, H-SSA}
\index{Hashed-SSA form, H-SSA}

\section{Introduction}

Hashed SSA (or in short HSSA), is an SSA extension that can effectively represent how aliasing relations affect a program in SSA form. It works equally well for aliasing among scalar variables and, more generally, for indirect load and store operations on arbitrary memory locations. This allows the application of all common SSA based optimizations to perform uniformly both on local variables and on external memory areas.

It should be noted, however, that HSSA is a technique useful for representing aliasing effects, but not for detecting aliasing. For this purpose, a separate alias analysis pass must be performed, and the effectiveness of HSSA will be influenced by the accuracy of this analysis.

The following sections explain how HSSA works. Initially, \emph{given} aliasing information, we will see how to represent them in SSA form for scalar variables. Then we will introduce a technique that reduces the overhead of the above representation, avoiding an explosion in the number of SSA versions for aliased variables. Subsequently we will represent indirect memory operations on external memory areas as operations on "virtual variables" in SSA form, which will be handled uniformly with scalar (local) variables.
Finally we will apply global value numbering (GVN) to all of the above, obtaining the so called Hashed SSA form~\footnote{The name \emph{Hashed} SSA comes from the use of hashing in value-numbering}.


\section{SSA and aliasing: $\mu$ and \chifuns}
\index{\mufun}\index{\chifun}
Aliasing occurs inside a compilation unit when a single one single storage location (that contains a value) can be potentially accessed through different program "variables".
This can happen in one of the four following ways:
\begin{itemize}
\item First, when two or more storage locations partially overlap. This, for instance, happens with the C "union" construct, where different parts of a program can access the same storage location under different names.
\item Second, when a local variable is referred by a pointer used in an indirect memory operation. In this case the variable can be accessed in two ways: {\em directly}, through the variable name, and {\em indirectly}, through the pointer that holds its address.
\item Third, when the address of a local variable is passed to a function, which in turn can then access the variable indirectly.
\item Finally, storage locations with global scope can obviously be accessed by different functions. In this case every function call can potentially access every global location, unless the compiler uses global optimizations techniques where every function is analyzed before the actual compilation takes place.
\end{itemize}

The real problem with aliasing is that these different accesses to the same program variable are difficult to predict. Only in the first case (explicitly overlapping locations) the compiler has full knowledge of when each access takes place. In all the other cases (indirect accesses through the address of the variable) the situation becomes more complex, because the access depends on the address that is effectively stored in the variable used in the indirect memory operation.
This is a problem because every optimization pass is concerned with the actual value that is stored in every variable, and when those values are used. If variables can be accessed in unpredictable program points, the only safe option for the compiler is to handle them as "volatile" and avoid performing optimizations on them, which is not desirable.

Intuitively, in the presence of aliasing the compiler could try to track the values of variable addresses inside other variables (and this is exactly what HSSA does), but the formalization of this process is not trivial.
The first thing that is needed is a way to model the {\em effects} of aliasing on a program in SSA form.
To do this, assuming that we have already performed alias analysis, we must formally define the effects of indirect definitions and uses of variables.
Particularly, each definition can be a {\em "MustDef"} operand in the direct case, or a {\em "MayDef"} operand in the indirect case. We will represent MayDef through the use of \chifuns.
Similarly, uses can be {\em "MustUse"} or {\em "MayUse"} operands (respectively in the direct and indirect case), and we will represent MayUse  through the use of \mufuns.
The semantic of $\mu$ and $\chi$ operators can be illustrated through the C like example of Figure~\ref{fig:hssa:muchi} where $*p$ represents an indirect access with address $p$.
Obviously the argument of the \muop is the potentially used variable.
Less obviously, the argument to the \chiop is the assigned variable itself. This expresses the fact that the \chiop only {\em potentially} modifies the variable, so the original value could "flow through" it. 

The use of $\mu$ and \chiops does not alter the complexity of transforming a program in SSA form. All that is necessary is a pre-pass that inserts them in the program. Ideally, a $\mu$ and a $\chi$ should be placed \emph{in parallel}\index{parallel instruction} to the instruction that led to its insertion. Parallel instructions are represented in Figure~\ref{fig:hssa:muchi} using the notation introduced in Section~\ref{sub:ssi:split}. Still, practical implementations may choose to insert $\mu$ and $\chi$s before or after the instructions that involve aliasing. Particularly, \mufuns could be inserted immediately {\em before} the involved statement or expression, and \chiops immediately {\em after} it. This distinction allows us to model call effects correctly: the called function appears to potentially use the values of variables before the call, and the potentially modified values appear after the call.

Thanks to the systematic insertion of \mufuns and \chifuns, an assignment of any scalar variable can be safely considered dead if it is not marked live by a standard SSA based dead-code elimination. In our running example of Figure~\ref{fig:hssa:muchi}(c), the potential side effect of any assignment to $i$, represented through the \mufun at the return, allows detecting that the assignment to $i_4$ is not dead. The assignment of value 2 to $i$ would have been considered as dead in the absence of the function call to $f()$, which potentially uses it (detected through its corresponding \mufun). 



\begin{figure}
\subfloat[Initial C code]{
  \begin{minipage}{0.25\textwidth}
   \begin{algorithm}[H]
         $i = 2$\; 
         \uIf{$j$}{
            $\dots$\;
         }
         \Else{
           $f()$\;
           $*p = 3$\;
         } 
         \BlankLine
         $i = 4$\; 
         return\;
   \end{algorithm}
   \end{minipage}
}
%
\subfloat[After $\mu$ and $\chi$ insertion]{
  \begin{minipage}{0.35\textwidth}
    \begin{algorithm}[H]
          $i = 2$\; 
          \uIf{$j$}{
            $\dots$\;
          } 
          \Else{
            $f()\ \parallel\ \mu(i)$\;
            $*p = 3\ \parallel\ i = \chi(i)$\;
          } 
          \BlankLine
          $i = 4$\; 
         return $\parallel\ \mu(i)$\;
    \end{algorithm}
\end{minipage}
}
%
\subfloat[After $\phi$ insertion and versioning]{
  \begin{minipage}{0.35\textwidth}
    \begin{algorithm}[H]
          $i_1 = 2$\; 
          \uIf{$j_1$}{
            $\dots$\;
          } 
          \Else{
            $f()\ \parallel\ \mu(i_1)$\;
            $*p = 3\ \parallel\ i_2 = \chi(i_1)$\;
          } 
          $i_3=\phi(i_1,i_2)$\;
          $i_4 = 4$\; 
          return $\parallel\ \mu(i_4)$\;
      \end{algorithm}
\end{minipage}
}
\caption{\label{fig:hssa:muchi}A program example where $*p$ might alias $i$, and function $f$ might indirectly use $i$ but not alter it}
\end{figure}


\section{Introducing ``zero versions'' to limit the explosion of the number of variables}
\index{zero version}
While it is true that $\mu$ and $\chi$ insertion does not alter the complexity of SSA construction, applying it to a production compiler as described in the previous section would make working with code in SSA form terribly inefficient.
This is because \chiops cause an explosion in the number of variable values, inducing the insertion of new \phifuns which in turn create new variable versions.
In practice the resulting IR, and especially the number of distinct variable versions, would be needlessly large. The biggest issue is that the SSA versions introduced by \chiops are useless for most optimizations that deal with variable values. $\chi$ definitions adds uncertainty to the analysis of variables values: the actual value of a variable after a $\chi$ definition could be its original value, or it could be the one indirectly assigned by the $\chi$.

Intuitively, the solution to this problem is to factor all variable versions\index{variable version} that are considered \emph{useless} together, so that no space is wasted to distinguish among them. We assign number 0 to this special variable version, and call it "{\em zero version}".

Our notion of useless versions relies on the concept of "{\em real occurrence of a variable}", which is an actual definition or use of a variable in the original program. Therefore, in SSA form, variable occurrences in $\mu$, $\chi$ and \phifuns are not "real occurrences". In our example of Figure~\ref{fig:hssa:muchi}, $i_2$ have no real occurrence while $i_1$ and $i_3$ have. The idea is that variable versions that have no real occurrence do not influence the program output. Once the program is converted back from SSA form these variables are removed from the code.
Since they do not directly appear in the code, and their value is usually unknown, distinguishing among them is almost pointless.
For those reasons, we consider zero versions, versions of variables that have no real occurrence, and whose value comes from at least one \chifun (optionally through \phifuns). An equivalent, recursive definition is the following:
\begin{itemize}
\item The result of a $\chi$ has zero version if it has no real occurrence.
\item If the operand of a $\phi$ has zero version, the $\phi$ result has zero version if it has no real occurrence.
\end{itemize}

Algorithm~\ref{alg:hssa:zero-versioning}\index{data flow} performs zero-version detection if only use-def chains\index{use-def chains}, and not def-use chains, are available. A "HasRealOcc" flag is associated to each variable version, setting it to true whenever a real occurrence is met in the code. This can be done while constructing the SSA form. A list "NonZeroPhiList", initially empty, is also associated to each original program variable.

\begin{algorithm}[htpb]
  \ForEach{variable $v$}{
    \ForEach{version $v_i$ of $v$}{
      \uIf{$\lnot v_i.HasRealOcc \wedge v_i.def.operator=\chi$}{
        $v_i.version=0$\;
      }
      \If{$\lnot HasRealOcc \wedge v_i.def.operator=\phi$}{
          let $V=v_i.def.operands$\;
          \uIf{$\forall v_j\in V,\ v_j.HasRealOcc$}{
            $v_i.HasRealOcc=true$\;
          }
          \uElseIf{$\exists v_j\in V, v_j.version=0$}{
            $v_i.version=0$\;
          }
          \Else{ $v.NonZeroPhiList.add(v_i)$ }
      }
    }
    $changes=true$\;
    \While{$changes$}{
      $changes=false$\;
      \ForEach{$v_i \in v.NonZeroPhiList$}{
        let $V=v_i.def.operands$\;
        \uIf{$\forall v_j\in V,\ v_j.HasRealOcc$}{
          $v_i.HasRealOcc=true$\;
          $v.NonZeroPhiList.remove(v_i)$\;
          $changes=true$\;
        }
        \ElseIf{$\exists v_j\in V, v_j.version=0$}{
          $v_i.version=0$\;
          $v.NonZeroPhiList.remove(v_i)$\;
          $changes=true$\;
        }
      }
    }
  }
  \caption{\label{alg:hssa:zero-versioning}Zero-version detection based on SSA use-def chains}
\end{algorithm}

The time spent in the first iteration grows linearly with the number of variable versions, which in turn is proportional to the number of definitions and therefore to the code size.
On the other hand, the while loop may, in the worst case, iterate as many times as the longest chain of contiguous $\phi$ assignments in the program. This bound can easily be reduced to the largest loop depth of the program by traversing the versions using a topological order of the forward control flow graph\index{forward control flow graph}.
All in all, zero version detection in the presence of $\mu$ and \chifuns does not change the complexity of SSA construction in a significant way, while the corresponding reduction in the number of variable versions is definitely desirable.

This loss of information has almost no consequences on the effectiveness of subsequent optimization passes.
control flow graph
Since variables with zero versions have uncertain values, not being able to distinguish them usually only slightly affects the quality of optimizations that operate on values.
On the other hand, when performing sparse dead-code elimination\index{dead code elimination} along use-def chains, zero versions for which use-def chains have been broken must be assumed live. However this has no practical effect. Zero versions have no real occurrence, so there is no real statement that would be eliminated by the dead-code elimination pass if we could detect that a zero version occurrence is dead.
There is only one case in dead-code elimination where the information loss is evident. A zero version can be used in a $\mu$ and defined in a $\chi$, which in turn can have a real occurrence as argument. If the $\mu$ is eliminated by some optimization, the real occurrence used in the $\chi$ becomes dead but will not be detected as it, since the $\chi$ is conservatively marked as non dead. This case is sufficiently rare in real world code that using zero versions is anyway convenient.


\section{SSA and indirect memory operations: virtual variables}

The technique described in the previous sections only apply to ''regular`` variables in a compilation unit, and not to arbitrary memory locations accessed indirectly. As an example, in Figure~\ref{fig:hssa:muchi}, $\mu$, $\chi$, and \phifuns have been introduced to keep track of $i$'s live-range, but $*p$ is not considered as an SSA variable. 
In other words, up to now, HSSA allows to apply SSA based optimizations to variables also when they are affected by aliasing, but memory access operations are still excluded from the SSA form.

This situation is far from ideal, because code written in current mainstream imperative languages (like C++, Java or C\#) typically contains many operations on data stored in global memory areas.
For instance, in C we can imagine to define bidimensional vector as a struct: "typedef struct \{double x; double y;\} point;", and then to have a piece of code that computes the modulus of a vector of address "p": "m = (p-\textgreater x * p-\textgreater x) + (p-\textgreater y * p-\textgreater y);".
Looking at the code it is obvious that "x" is accessed twice but both accesses give the same value so the second access could be optimized away (the same for "y").
The problem is that "x" and "y" are not ``regular'' variables: "p" is a variable while "p-\textgreater x" and "p-\textgreater y" are indirect memory operations.
Putting that code snipped in SSA form tells us that the value of "p" never changes, but it reveals nothing about the values stored in the locations "p-\textgreater x" and "p-\textgreater y".
It is worth noting that operations on array elements suffer from the same problem.

The purpose of HSSA is to handle indirect memory operations just like accesses to scalar variables, and be able to apply all SSA based optimizations uniformly on both of them.
To do this, in the general case, we assume that the code intermediate representation supports a {\em dereference} operator, which performs an indirect load (memory read) from the given address. This operator can be placed in expressions on both the left and right side of the assignment operator, and we will represent it with the usual "*" C language operator. We will then, for simplicity, represent indirect stores with the usual combination of dereference and assignment operators, like in C.
Some examples of this notation are:
\begin{itemize}
\item *p: access memory at address p.
\item *(p+1): access memory at address p+1 (like to access an object field at offset 1).
\item **p: double indirection.
\item *p = expression: indirect store.
\end{itemize}

As we noted above indirect memory operations cannot be handled by the SSA construction algorithm because they are {\em operations} while SSA construction works renaming {\em variables}.
What HSSA does is to represent the "target" locations of indirect memory operations with {\em virtual variables}. A virtual variable is an abstraction of a memory area and appears under HSSA thanks to the insertion of $\mu$ and \phifuns: indeed as any other variable, it may alias with an operand. The example of Figure~\ref{fig:hssa:virtual_vars} shows two possible forms after \mufun and \chifun insertion for the same code. In Figure~\ref{fig:hssa:virtual_vars}(b), two virtual variables $v^*$ and $w^*$ are respectively associated to the memory expressions $*p$ and $*q$; $v^*$ and $w^*$ alias with all indirect memory operands (of lines 3, 5, 6, and~8). In Figure~\ref{fig:hssa:virtual_vars}(c) $v$ is associated to the base pointer of $b$ and $w$ to $b+1$ respectively. As one can see, there exists many possible ways to chose virtual variables. The only discipline imposed by HSSA is that each indirect memory operand must be associated to a single virtual variable. So on one extreme, there would be one virtual variable for each indirect memory operation. \emph{Assignment factoring} corresponds to make each virtual variable represents more than one single indirect memory operand. On the other extreme, the most factored HSSA form would have only one single virtual variable on the overall. In the example of Figure~\ref{fig:hssa:virtual_vars}(c) we considered as given by alias analysis the non aliasing between $b$ and $b+1$, and choose two virtual variables to represent the two corresponding distinct memory locations. In the general case, virtual variables can obviously alias with one another as in Figure~\ref{fig:hssa:virtual_vars}(b).

\begin{figure}
\subfloat[Initial C code]{
  \begin{minipage}{0.17\textwidth}
    \LinesNumbered
   \begin{algorithm}[H]
     $p=b$\;
     $q=b$\;
     $*p=\dots$\;
     $p=p+1$\;
     $\dots=*p$\;
     $q=q+1$\;
     $*p=\dots$\;
     $\dots=*q$\;
   \end{algorithm}
  \end{minipage}
}
\subfloat[$v$ and $w$ alias with ops 3,5,7, and 8]{
  \begin{minipage}{0.43\textwidth}
    \begin{algorithm}[H]
     $p=b$\;
     $q=b$\;
     $*p=\dots \parallel\ v^*=\chi(v^*) \parallel\ w^*=\chi(w^*)$\;
     $p=p+1$\;
     $\dots=*p \parallel\ \mu(v^*) \parallel\ \mu(w^*)$\;
     $q=q+1$\;
     $*p=\dots \parallel\ v^*=\chi(v^*) \parallel\ w^*=\chi(w^*)$\;
     $\dots=*q \parallel\ \mu(v^*) \parallel\ \mu(w^*)$\;
   \end{algorithm}
  \end{minipage}
}
\subfloat[$x$ alias with op 3; $y$ with 5,7, and 8]{
  \begin{minipage}{0.37\textwidth}
    \begin{algorithm}[H]
     $p=b$\;
     $q=b$\;
     $*p=\dots \parallel\ x^*=\chi(x^*)$\;
     $p=p+1$\;
     $\dots=*p \parallel\ \mu(y^*)$\;
     $q=q+1$\;
     $*p=\dots \parallel\ y^*=\chi(y^*)$\;
     $\dots=*q \parallel\ \mu(y^*)$\;
   \end{algorithm}
  \end{minipage}
}
\caption{\label{fig:hssa:virtual_vars}Some virtual variables and their insertion depending on how they alias with operands.}
\end{figure}

To summarise, alias analysis, considered as a given, drives the choice of virtual variables\footnote{I'm not sure what this means.  Rephrase? (dnovillo)}.
The insertion of \mufun and \chifun for virtual variables is performed in the same pass than for scalar variables.
Also, when the program is converted into SSA form, virtual variables can be renamed just like scalar variables.
Use-def relationships of virtual variables now represent use-def relationships of the memory locations accessed indirectly.
Note that the zero versioning technique can be applied unaltered to virtual variables.
At this point, we can complete the construction of HSSA form by applying global value numbering.

\section{GVN and indirect memory operations: HSSA}

In the previous sections we sketched the foundations of a framework for dealing with aliasing and indirect memory operations in SSA form: we identified the effects of aliasing on local variables, introduced $\mu$ and \chiops to handle them, applied zero versioning to keep the number of SSA versions acceptable, and defined virtual variables as a way to apply SSA also to memory locations accessed indirectly. However, HSSA is complete only once {\em Global Value Numbering}\index{Global Value Numbering} is applied to scalar and virtual variables, handling all of them uniformly (GVN. See Chapter~\ref{chapter:pre_not_helped}).

GVN is normally used as a way to perform \emph{redundancy elimination} which means removing redundant expression computations from a program, typically storing the result of a computation in a temporary variable and reusing it later instead of performing the computation again.
As the name suggests, GVN works assigning a unique number to every expression in the program with the idea that expression identified by the same number are guaranteed to give the same result.
This value number is obtained using a hash function represented here as $\GVN{key}$.
To identify identical expressions, each expression tree is hashed bottom up: as an example, for $p_1+1$, $p_1$ and $1$ are replaced by their respective value numbers, then the expression is put into canonical form, and finally hashed. Which ends up to $\GVN{+(\GVN{b},\GVN{1})}$, as $\GVN{p_1}=\GVN{b}$ in our example.
If the program is in SSA form computing value numbers that satisfy the above property is straightforward: variables are defined only once, therefore two expressions that apply the same operator to the same SSA variables are guaranteed to give the same result and so can have the same value number.
While computing global value numbers for code that manipulates scalar variables is beneficial because it can be used to implement redundancy elimination, applying GVN to virtual variables has additional benefits.

\begin{figure}
\subfloat[Initial C code]{
  \begin{minipage}{0.17\textwidth}
    \LinesNumbered
   \begin{algorithm}[H]
     $p=b$\;
     $q=b$\;
     $*p=3$\;
     $p=p+1$\;
     $\dots=*p$\;
     $q=q+1$\;
     $*p=4$\;
     $\dots=*q$\;
   \end{algorithm}
  \end{minipage}
}\hfill
\subfloat[with one virtual variable]{
  \begin{minipage}{0.3\textwidth}
    \begin{algorithm}[H]
     $p_1=b$\;
     $q_1=b$\;
     $*p_1=3 \parallel\ v_1^*=\chi(v_0^*)$\;
     $p_2=p_1+1$\;
     $\dots=*p_2 \parallel\ \mu(v_1^*)$\;
     $q_2=q_1+1$\;
     $*p_2=4 \parallel\ v_2^*=\chi(v_1^*)$\;
     $\dots=*q_2 \parallel\ \mu(v_2^*)$\;
   \end{algorithm}
  \end{minipage}
}\hfill
\subfloat[HSSA statements]{
  \begin{minipage}{0.33\textwidth}
    \begin{algorithm}[H]
     $h_1=h_0$\;
     $h_2=h_0$\;
     $h_5=h_3 \parallel\ h_4=\chi(\GVN{v_0^*})$\;
     $h_8=h_7$\;
     $\dots=h_{10} \parallel\ \mu(h_4)$\;
     $h_{11}=h_7$\;
     $h_{14}=h_{12} \parallel\ h_{13}=\chi(h_4)$\;
     $\dots=h_{14} \parallel\ \mu(h_{13})$\;
   \end{algorithm}
  \end{minipage}
}
\\
\begin{center}
\subfloat[Hash Table]{
  \begin{tabular}{rc|c|l}
    &key                   & hash       & value\\ \hline
    &$b$                   & $h_0$   &  $b$\\
    &$p_1$                 & $h_1$   &  $h_0$\\
    &$q_1$                 & $h_2$   &  $h_0$\\
    &$3$                   & $h_3$   &  $const(3)$\\
    &$v^*_1$               & $h_4$   &  $v^*_1$\\
    $ivar(*p_1,v^*_1)\ \leadsto$ &$ivar(*h_0,v^*_1)$  & $h_5$   & $h_3$\\
    &$1$                   & $h_6$   & $const(1)$\\
    $p_1+1\ \leadsto$ &$ +(h_0,h_6)$  & $h_7$   & $+(h_0,h_6)$\\
    &$p_2$                 & $h_8$   & $h_7$\\
    $ivar(*p_2,v^*_1)\ \leadsto$& $ivar(*h_7,v^*_1)$      & $h_{10}$ &$ivar(*h_7,v^*_1)$\\
    &$q_2$                 & $h_{11}$ & $h_7$\\
    &$4$                   & $h_{12}$ & $const(4)$\\
    &$v^*_2$               & $h_{13}$   &  $v^*_2$\\
    $ivar(*p_2,v^*_2)\ \leadsto$& $ivar(*h_7,v^*_2)$      & $h_{14}$ &$h_{12}$\\
  \end{tabular}
}
\end{center}
\caption{\label{fig:hssa:versioning}Some code after variables versioning, its corresponding HSSA form along with its hash table entries. $q_1+1$ that simplifies into $ +(h_0,h_6)$ will be hashed to $h_7$, and $ivar(*q_2,v^*_2)$ that simplifies into $ivar(*h_7,v^*_2)$ will be hashed to $h_{14}$.
}
\end{figure}

First of all, it can be used to determine when two address expressions compute the same address: this is guaranteed if they have the same global value number. In our example, $p_2$ and $q_2$ will have the same value number $h_7$ while $p_1$ will not.
This allows indirect memory operands that have the same GVN for their address expressions and the same virtual variable version to become a single entity in the representation. This puts them in the same rank as scalar variables and allows the transparent application of SSA based optimizations on indirect memory operations.
For instance, in the vector modulus computation described above, every occurrence of the expression "p-\textgreater x" will always have the same GVN and therefore will be guaranteed to return the same value, allowing the compiler to emit code that stores it in a temporary register instead of performing the redundant memory reads (the same holds for "p-\textgreater y").
Similarly, consider the example of Figure~\ref{fig:hssa:versioning}. Loads of lines 5 and~8 can not be considered as redundant because the versions for $v$ ($v_1^*$ then $v_2^*$) are different. On the other hand the load of line~8 can be safely avoided using a rematerialization of the value computed in line~7 as both the version for $v$ ($v_2^*$) and the value number for the memory operands are identical. 
As a last example, if all the associated virtual variable versions for an indirect memory store (defined in parallel by \chifuns) are found to be dead, then it can be safely eliminated~\footnote{Note that any virtual variable that aliases with a memory region live-out of the compiled procedure is considered to alias with the return instruction of the procedure, and as a consequence will lead to a live \mufun}. In other words HSSA transparently extends DEADCE to work also on indirect memory stores.

Another advantage of GVN in this context is that it enables uniform treatment of indirect memory operations regardless of the levels of indirections (like in the "**p" expression which represents a double indirection).
This happens naturally because each node of the expression is identified by its value number, and the fact that it is used as an address in another expression does not raise any additional complication.

Having explained why HSSA uses GVN we are ready to explain how the HSSA intermediate representation is structured.
A program in HSSA keeps its CFG structure, with basic-blocks made of a sequence of statements (assignments and procedure calls), but the left and right hand-side of each statement are replaced by their corresponding entry in the hash table. 
Each constant, address and variable (both scalar and virtual) finds its entry in the hash table.
As already explained, expressions are also hashed using the operator and the global value number of its operands.
Each operand that corresponds to an indirect memory operation gets a special treatment on two aspects.
First of all, it is considered semantically both as an expression (that has to be computed) and as a memory location (just as other variables).
Second, its entry in the hash table is made up of both the address expression and its corresponding virtual variables version.
This allows two entries with identical value numbers to ``carry'' the same value.  This is illustrated in lines~3, 7, and~8 of Example~\ref{fig:hssa:versioning}.
Such entries are referred as \emph{ivar} node, for \emph{indirect variables} so as to recall its operational semantic and distinguish them from the original program scalar variables. 

It should be noted that ivar nodes, in terms of code generation, are operations and not variables: the compiler back end, when processing them, will emit indirect memory accesses to the address passed as their operand.
While each ivar node is \emph{related} to a virtual variable (the one corresponding to its address expression), and its GVN is determined taking into account the SSA version of its virtual variable; the virtual variable has no real counterpart in the program code.
In this sense virtual variables are just an aid to apply aliasing effects to indirect memory operations, be aware of liveness and perform SSA renaming (with any of the regular SSA renaming algorithms).
However, even thought virtual variables do not contribute to the emission of machine code, their use-def links can be examined by optimization passes to determine the program data dependency paths (just like use-def links of scalar variables).

Therefore in HSSA value numbers have the same role of SSA versions in "plain" SSA (while in HSSA variable versions can effectively be discarded: values are identified by GVNs and data dependencies by use-def links between IR nodes.
It is in this way that HSSA extends all SSA based optimizations to indirect memory operations, even if they were originally designed to be applied to "plain" program variables.


\section{Building HSSA}
We now present the HSSA construction algorithm.
It is straightforward, because it is a simple composition of $\mu$ and $\chi$ insertion, zero versioning and virtual variable introduction (described in previous sections), together with regular SSA renaming and GVN application.

\begin{algorithm}[H]
\LinesNumbered
\begin{enumerate}
\item Perform alias analysis and assign a virtual variables to each indirect memory operand
\item Insert \mufuns and \chifuns for scalar and virtual variables
\item Insert \phifuns (considering both regular and $\chi$ assignments) as for standard SSA construction
\item Perform SSA renaming on all scalar and virtual variables as for standard SSA construction 
\end{enumerate}
\caption{\label{algo:hssa:ssaconstruct}SSA form construction}
\end{algorithm}

At the end of Algorithm~\ref{algo:hssa:ssaconstruct} we have code in plain SSA form.
The use of $\mu$ and $\chi$ operands guarantees that SSA versions are correct also in case of aliasing.
Moreover, indirect memory operations are "annotated" with virtual variables, and also virtual variables have SSA version numbers.
However, note how virtual variables are sort of "artificial" in the code and will not contribute to the final code generation pass, because what really matters are the indirect memory operations themselves.

The next steps corresponds to Algorithm~\ref{algo:hssa:zeroversion} where steps~\ref{itm:hssa:deadce} and~\ref{itm:hssa:init} can be done using a single traversal of the program. 
At the end of this phase the code has exactly the same structure as before, but the number of unique SSA versions had diminished because of the application of zero versions.

\begin{algorithm}[H]
\begin{enumerate}
\setcounter{enumi}{4}
\item\label{itm:hssa:deadce} perform DEADCE (also on $\chi$ and $\phi$ stores)
\item\label{itm:hssa:init} initialize \emph{HasRealOcc} and \emph{NonZeroPhiList} as for Algorithm~\ref{alg:hssa:zero-versioning}, then run Algorithm~\ref{alg:hssa:zero-versioning} (Zero-version detection)
\end{enumerate}
\caption{\label{algo:hssa:zeroversion}Detecting zero versions}
\end{algorithm}

The application of global value numbering is straightforward. Some basics about GVN are recalled in Chapter~\ref{chapter:pre_not_helped}. 

\begin{algorithm}[H]
\begin{enumerate}
\setcounter{enumi}{6}
\item Perform a pre-order traversal of the dominator tree, applying GVN to the whole code (generate a unique hash table \emph{var} node for each scalar and virtual variable version that is still live)
  \begin{enumerate}
    \item expressions are processed bottom up, reusing existing hash table expression nodes and using var nodes of the appropriate SSA variable version (the current one in the dominator tree traversal)
%FAB: there is a point that needs to be discussed here. For me either there is a single node for two variables that carry the same value or the expression should be simplified to use a representative node for the value. Otherwise we miss some opportunities.
    \item two ivar nodes have the same value number if these conditions are both true:
      \begin{itemize}
        \item their address expressions have the same value number, and
        \item their virtual variables have the same versions, or are separated by definitions that do not alias the ivar (possible to verify because of the dominator tree traversal order)
      \end{itemize}
  \end{enumerate}
\end{enumerate}
\caption{\label{algo:hssa:gvnfinal}Applying GVN}
\end{algorithm}


As a result of this, each node in the code representation has a proper value number, and nodes with the same number are guaranteed to produce the same value (or hold it in the case of variables).
The crucial issue is that the code must be traversed following the dominator tree in pre-order. This is important because when generating value numbers we must be sure that all the involved definitions already have a value number. Since the SSA form is a freshly created one, it is strict (i.e., all definitions dominate their use). As a consequence, a dominator tree pre-order traversal satisfies this requirement.

Note that after this step virtual variables are not needed anymore, and can be discarded from the code representation: the information they convey about aliasing of indirect variables has already been used to generate correct value numbers for ivar nodes.

\begin{algorithm}[H]
\begin{enumerate}
\setcounter{enumi}{7}
  \item The left-hand side of each assignment (direct and indirect, real, $\phi$ and $\chi$) is updated to point to its var or ivar node (which will point back to the defining statement)
  \item Also all $\phi$, $\mu$ and $\chi$ operands are updated to point to the corresponding GVN table entry
\end{enumerate}
\caption{\label{algo:hssa:linkingdefs}Linking definitions}
\end{algorithm}

At the end of the last steps listed in Algorithm~\ref{algo:hssa:linkingdefs}, HSSA form is complete, and every value in the program code is represented by a reference to a node in the HSSA value table.

\section{Using HSSA}
As seen in the previous sections, HSSA is an internal code representation that applies SSA to indirect memory operations and builds a global value number table, valid also for values computed by indirect memory operations.


This representation, once built, is particularly memory efficient because expressions are shared among use points (they are represented as HSSA table nodes). In fact the original program code can be discarded, keeping only a list of statements pointing to the shared expressions nodes.

All optimizations conceived to be applied on scalar variables work "out of the box" on indirect locations: of course the implementation must be adapted to use the HSSA table, but their algorithm (and computational complexity) is the same even when dealing with values accessed indirectly.

Indirect memory operation (ivar) nodes are both variables and expressions, and benefit from the optimizations applied to both kinds of nodes. Particularly, it is relatively easy to implement register promotion\index{register promotion}.

Of course the effectiveness of optimizations applied to indirect memory operations depends on the quality of alias analysis: if the analysis is poor the compiler will be forced to "play safe", and in practice the values will have "zero version" most of the time, so few or no optimizations will be applied to them.
On the other hand, for all indirect memory operations where the alias analyzer determines that there are no interferences caused by aliasing all optimizations can happen naturally, like in the scalar case.




\section{Further readings}
%FAB: you can merge the last two sections and call it Further readings and discussions or something similar
Cite the original paper from Zhou et al. \cite{ZhouCC11}
Cite the original paper from Chow.
Cite work done in the GCC compiler (which was later scrapped due to compile time and memory consumption problems. but the experience is valuable): http://www.airs.com/dnovillo/Papers/mem-ssa.pdf
Memory SSA - A Unified Approach for Sparsely Representing Memory Operations, D. Novillo, 2007 GCC Developers' Summit, Ottawa, Canada, July 2007.
Talk about the possible differences (in terms of notations) that might exist between this chapter and the paper.
Cite Alpern, Wegman, Zadech paper for GVN.
Discuss the differences with Array SSA.
Cite the paper mentioned in Fred's paper about factoring.
Give some pointers to computation of alias analysis, but also representation of alias information (eg points-to).
Add a reference on the paper about register promotion and mention the authros call it indirect removal 
