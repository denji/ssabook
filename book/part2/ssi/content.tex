\chapter{Static Single Information Form \Author{F. Pereira \andAuthor F. Rastello}}
\inputprogress
\label{chap:ssi}

\graphicspath{{img/}{ssi/img/}{part2/ssi/img/}}

\section{Introduction}
\label{sec:ssi:pereira:intro}

The objective of a dataflow analysis is to discover facts that are true about a
program.
We call such facts {\em information}.
Using the notation introduced in Section~\ref{novillo:sec:prop-engine}, an
information is a point in the dataflow lattice.
For example, the information that concerns liveness analysis is the set of
variables alive at a certain program point.
Similarly to liveness analysis, many other classic dataflow approaches bind
information to pairs formed by a variable and a program point.
However, if the information is true for a variable $v$ at any program point where
$v$ is alive, then we can associate this information directly to $v$.
If a program's intermediate representation guarantees this correspondence between
information and variable for every variable, then we say that the program
representation provides the {\em Single Static Information} (SSI) property.

Different dataflow analysis might extract information from different program
facts.
Therefore, a program representation may afford the SSI property to some dataflow
analysis, but not to all of them.
For instance, the SSA form naturally provides the SSI property to the reaching
definition analysis.
Indeed, the SSA form provides the static single information property to any
dataflow analysis that obtains information at the definition sites of
variables.
These analyses and transformations include copy and constant propagation as illustrated in Chapter~\ref{XXX:chap:constant_propagation_is_easier}.
However, the SSA form does not provide the SSI property to a dataflow analysis
that derives information from the use site of variables.
In other words, because the same variable $v$ in a SSA form program might be used
at different program points, the information associated with $v$ might not be
unique along its entire live-range.

There exists extensions of the SSA form that provide the SSI property to more
dataflow analyses than the original SSA does.
Two classic examples, that we will detail further, are the {\em Extended-SSA} (e-SSA) form, and the {\em Static Single Information} (SSI) form.
The e-SSA form provides the SSI property to analyses that take information from
the definition site of variables, and also from conditional tests where these
variables are used.
The SSI form provides the static single information property to dataflow
analyses that extract information from the definition sites of variables, and from last use sites (as defined further).
These different intermediate representations rely on a common strategy to achieve the SSI property: {\em live range splitting}.
In this chapter we show how to use live range splitting to build program
representations that provide the static single information property to different
types of dataflow analysis.

\section{Single Information}
\label{sec:ssi:pereira:single}



\subsection{Sparse Analysis}
\label{sec:ssi:pereira:sparse}

Traditionally, data-flow analyses bind information to pairs formed by a variable and a program point.
Let's consider, as an example, the problem of estimating the interval of values that any integer variable may assume throughout the execution of a program.
An algorithm that solves this problem is called a {\em range analysis}.
A traditional implementation of this analysis would find, for each pair formed by a variable $v$ plus a program point $i$, the interval of possible values that $v$ might assume at $i$.
Figure~\ref{fig:rangeAnalysis} illustrates this analysis with an example.
In this case we call a program point any region between two consecutive
instructions and we let $[v]$ denote the abstract information that is associated
with variable $v$.
Because this approach keeps information at each program point, we call it {\em dense}, in contrast with the sparse analyses seen in Section~\ref{chapter:constant_propagation_is_easier:sec:prop-engine}.

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{ProgramPoint}
\caption{An example of a dense data-flow analysis that finds the range of
possible values associated with each variable at each program point.}
\label{fig:rangeAnalysis}
\end{figure}

The dense approach might keep more redundant information during the
data-flow analysis.
For instance, if we let $[v]^i$ denote the abstract state of variable
$v$ at program point $i$, then we have that, in our example, $[i]^1 = [i]^2$,
$[s]^3 = [s]^4$ and $[i]^4 = [i]^5$.
This redundancy happens because some transfer functions are identities.
For instance, in range analysis, an instruction that neither defines nor uses any variable is associated with an identity transfer function.
The transfer function that updates the abstract state of $i$ at program point $p_2$ is an identity.
Because the instruction immediately before $p_2$ does not add any new information to the abstract state of $i$, $[i]^2$ is updated with the information that flows directly from the predecessor point $p_1$.

The goal of sparse data-flow analysis is to shortcut the identity transfer functions, a task that we accomplish by grouping contiguous program points bound to identities into larger regions.
Going back to our example, a given variable $v$ may be mapped to the same interval along many consecutive program points.
Furthermore, if the information associated with a variable is invariant along its
entire live range, then we can bind this information to the variable itself.
In other words, we can replace all the constraint variables
$[v]^i$ by a single constraint variable $[v]$, for each variable $v$
and every $i\in \textrm{live}(v)$.
Not every data-flow problem can be easily solved sparsely; however, many of them can, as they fit into the family of PVL problems that we describe in the next section.

\subsection{Partitioned Variable Lattice (PVL) Data-Flow Problems}
\label{sec:ssi:pereira:pvpPvl}

There exists a class of data-flow problems that can be naturally solved via sparse data-flow analyses.
These problems are called {\em Partitioned Data-flow Analyses} (PDA).
These analyses, which include live variables, reaching definitions and forward/backward printing, can be decomposed into a set of sparse data-flow problems -- usually one per variable -- each independent on the other.
We define the PDA family of data-flow problems as the sum of two notions: Partitioned Variable Problem (PVP) and Partitioned Variable Lattice (PVL).
A typical example in the PVP category is the well-known liveness analysis.
Among the many examples of PVL - but not PVP - problems, we cite range analysis.
After defining these concepts formally, we discuss these two examples.

\begin{definition}[PVP/PVL]
\label{pro:pvp}
\textsc{Partitioned Data-Flow Problem}:\\Let ${\cal V}=\{v_1,\dots,v_n\}$ be the set of program variables.
We consider, without loss of generality, a forward data-flow analysis.
This data-flow analysis is an equation system that associates, with each program point $i$, an element of a lattice ${\cal L}$, given by the equation $a^i = \bigwedge_{s \in \mathit{pred}(i)} F^s(a^s)$, where $a^i$ denotes the abstract state associated  program point $i$, and $F^s$ is the transfer function associated with program point $s$.
The analysis can be written\footnote{As far as we are concerned with finding its maximum solution. See for example Section~1.3.2 of~\cite{Nielson05}.} as a constraint system that binds to each program point $i$ and each $s\in \mathit{pred}(i)$ the equation $a^i = a^i \wedge  F^s(a^s)$ or, equivalently, the inequation $a^i \sqsubseteq  F^s(a^s)$.
The corresponding Maximum Fixed Point (MFP) problem is said to be a \emph{Partitioned Variable Problem} iff:
\begin{description}
\item {\bf [PVL]:} $\cal L$ can be decomposed into the product of ${\cal L}_{v_1}\times \dots \times {\cal L}_{v_n}$ where each ${\cal L}_{v_i}$ is the lattice associated with program variable $v_i$.
\item {\bf [PVP]:} each transfer function $F^s$ can also be decomposed into a product $F_{v_1}^s\times F_{v_2}^s \times \dots \times F_{v_n}^s$ where $F_{v_j}^s$ is a function from ${\cal L}_{v_j}$ to ${\cal L}_{v_j}$.
\end{description}
\end{definition}

Liveness analysis is a partitioned variable problem. 
The liveness information can be computed for each \emph{individual} variable \emph{independently}. 
At each program point we are interested in computing the set of variables alive at that point.
The information related to each variable belongs to a lattice of Boolean values ${\cal B}$.
The abstract states associated with all the variables at each program point can be written as a cross product ${\cal L}={\cal B}^n$.
For a given variable $v$ its liveness information at program point $i$, e.g., $[v]^i$, can be expressed in term of its state at the successors $s$ of $i$,
$[v]^i = [v]^i \wedge F_v^s([v]^s)$ with $F_v^s$ from $\cal B$ to $\cal B$.
This information does not depend on any program variable other than $v$ itself.

Contrary to liveness analysis, most of the data-flow analyses do not provide the PVP property.
However, a large number of them \emph{do} fulfill the PVL property.
Going back to range analysis, if we denote by $\cal I$ the lattice of integer intervals, the overall lattice can be written as ${\cal L}={\cal I}^n$ with $n$ the number of variables (PVL property).
On the other hand, as opposed to liveness information, the range interval of some variable $v$ at program point $i$ has to be expressed in term of the intervals associated with \emph{some other} variables (not only $v$) at the predecessors $S$ of $i$.
In other words,  $[v]^i = [v]^i \wedge  F_v^s([v_1]^s,\dots,[v_n]^s)$ with $F_v^s$ from $\cal L$ to $\cal I$ (and not from $\cal I$ to $\cal I$).

There are data-flow analyses that do not meet the PVL property.
Noticeable examples include analyses that rely on relational lattices.
A typical example are the analyses that use the {\em octagon} domain.
This domain binds variables together in constraints such as $x + y \leq c$, where $c$ is an integer, and $x, y$ are program variables.


\subsection{The Single Information Property}
\label{sec:ssi:pereira:singProp}

If the information associated with a variable is invariant along its
entire live range, then we can bind this information to the variable itself.
In other words, we can replace all the constraint variables
$[v]^i$ by a single constraint variable $[v]$, for each variable $v$
and every $i\in \textrm{live}(v)$.
In the context of range analysis, at the program points $s\in \textrm{live}(v)$ that do not redefine or test a variable $v$, we have that the transfer function $[v]^i = [v]^i \wedge F_v^s([v_1]^s, \dots, [v_n]^s)$ simplifies into $[v]^i = [v]^i$.
On the other hand, there are two types of program points where the range analysis finds non-identity transfer functions: definitions and conditionals.
At the definition point of variable $v$, $F_v$ simplifies to a function that depends only on some $[u]$ where each $u$ is an argument of the instruction defining $v$.
Range analysis can also use conditional tests to acquire more information about a variable.
Hence, at the conditional tests that use a variable $v$, $F_v$ can be simplified to a function that uses $[v]$ and possibly other variables that appear in the test.
This gives the intuition on why a propagation engine along the def-use chains of a SSA-form program can be used to solve the constant propagation problem in an equivalent, yet ``sparser'', manner.
This also paves the way toward a formal definition of the Static Single Information property.

\begin{property}[SSI]
\label{pro:ssi}\textsc{Static Single Information:} Consider a forward (resp. backward) monotone PVL problem $E_{dense}$ stated as a set of constraints $[v]^i \sqsubseteq F_v^s([v_1]^s,\dots,[v_n]^s)$ for every variable $v$, each program point $i$, and each $s \in \mathit{pred}(i)$ (resp. $s\in \mathit{succ}(i)$).
A program representation fulfills the Static Single Information property iff:\begin{description}
\item {\bf [SPLIT]:} for each variable $v$, each $i\in \textrm{live}(v)$ with a unique predecessor (resp. successor) $s$, such that $F_v^s\neq \lambda x.\bot$ is non-trivial, i.e. is not the simple projection on ${\cal L}_v$, then $s$ should contain a definition (resp. last use) of $v$; 
Let $(Y_v^i)_{(v,i)\in variables\times prog\_points}$ be a maximum solution to $E_{dense}$. 
Each join (resp. split) node $i$ for which $F_v^s(Y_{v_1}^s,\dots,Y_{v_n}^s)$ has different values on its incoming edges $(s,i)$ (resp. outgoing edges $(i,s)$) should have a $\phi$-function at the entry of $i$ (resp. $\sigma$ function at the exit of $i$) for $v$
as defined in Section~\ref{sub:split}.
\item {\bf [INFO]:} each program point $i\not\in \textrm{live}(v)$ should be bound to an undefined  transfer function, e.g., $F_v^i=\lambda x.\bot$.
\item {\bf [LINK]:} each instruction {\em inst} for which $F_v^{inst}$ depends on some $[u]^s$ should contain an use (resp. def) of $u$ live-out (resp. live-in) at
{\em inst}.
\item {\bf [VERSION]:} for each variable $v$, $\textrm{live}(v)$ is a connected component of the CFG.
\end{description}
\end{property}

These properties allows us to attach the information to variables, instead of program points.
The {SPLIT} property forces the information related to a variable to be invariant along its entire live-range.
{INFO} forces this information to be irrelevant outside the live range of the variable.
The {LINK} property forces the def-use chains to reach the points where information is available for a transfer function to be evaluated.
The {VERSION} property provides an one-to-one mapping between variable names and live ranges.

We must split live ranges to provide the SSI properties.
If we split them between each pair of consecutive instructions, then we would automatically provide these properties, as the newly created variables
would be live at only one program point.
However, this strategy would lead to the creation of many trivial program regions, and we would lose sparsity.
In Section~\ref{sec:building} we provide a sparser way to split live ranges that fit Property~\ref{pro:ssi}.
Possibly, we may have to extend the live-range of a variable to cover every program point where the information is relevant.
We accomplish this last task by inserting into the program pseudo-uses and pseudo-definitions of this variable.

\subsection{Special instructions used to split live ranges}
\label{sub:split}

We group program points in three kinds:
interior nodes, branches and joins.
At each place we use a different notation to denote live range splitting.

{\em Interior nodes} are program points that have a unique predecessor and a unique successor.
At these points we perform live range splitting via copies.
If the program point already contains another instruction, then this copy \emph{must} be done \emph{in parallel} with the existing instruction.
The notation, \[inst \ \parallel\  v_1=v'_1 \ \parallel\  \dots \ \parallel\  v_m=v'_m\] denotes $m$ copies $v_i=v'_i$ performed in parallel with
instruction $inst$.
This means that all the uses of $inst$ plus all right-hand variables $v'_i$ are read simultaneously, then $inst$ is computed, then all definitions of $inst$ plus all left-hand variables $v_i$ are written simultaneously.


% need phi
We call {\em joins} the program points that have one successor and multiple predecessors.
For instance, two different definitions of the same variable $v$ might be associated with two different constants; hence, providing two different pieces of information about $v$.
To avoid that these definitions reach the same use of $v$ we merge them at the earliest program point where they meet.
We do it via our well-known $\phi$-functions.

% need sigma
In backward analyses the information that emerges from different uses of a variable may reach the same {\em branch point}, which is a program point with a unique predecessor and multiple successors.
To ensure Property~\ref{pro:ssi}, the use that reaches the definition of a
variable must be unique, in the same way that in a SSA-form program the definition that reaches a use is unique.
We ensure this property via special instructions called $\sigma$-functions.
The $\sigma$-functions are the dual of $\phi$-functions, performing a parallel assignment depending on the execution path taken.
The assignment \[(v_1^1:l^1, \ldots, v_1^q:l^q) = \sigma(v_1) \ \parallel\  \dots \ \parallel\  (v_m^1:l^1, \ldots, v_m^q:l^q) = \sigma(v_m)\] represents $m$ $\sigma$-functions that assign to each variable $v_i^j$ the value in $v_i$ if control flows into block $l^j$.
These assignments happen in parallel, i.e., the $m$ $\sigma$-functions encapsulate $m$ parallel copies.
Also, notice that variables live in different branch targets are given
different names by the $\sigma$-function that ends that basic block.
%TODO FAB: add reference to gated SSA, loop closed form etc.

\subsection{Propagating Information Forwardly and Backwardly}

Let us consider a unidirectional forward (resp. backward) PVL problem $E^{ssi}_{\mathit{dense}}$ stated as a set of equations $[v]^i \sqsubseteq  F_v^s([v_1]^s, \dots, [v_n]^s)$ (or equivalently $[v]^i = [v]^i \wedge    F_v^s([v_1]^s, \dots, [v_n]^s$) for every variable $v$, each program point $i$, and each $s \in \mathit{pred}(i)$ (resp. $s \in \mathit{succ}(i)$). 
To simplify the discussion, any $\phi$-function (resp. $\sigma$-function) is seen as a set of copies, one per predecessor (resp. successor), which leads to as many constraints.
In other words, a $\phi$-function such as $a=\phi(a_1:l^1,\dots,a_m:l^m)$, gives us $n$ constraints such as $[a]^i \sqsubseteq  F_v^{l^j}([v_1]^{l^j}, \dots, [v_n]^{l^j})$, which usually simplifies into $[a]^i \sqsubseteq [a_j]^{l^j}$. This last can be written equivalently into the classical meet $[a]^i \sqsubseteq \bigwedge_{l^j \in \mathit{pred}(i)} [a_j]^{l^j}$ used in Chapter~\ref{chapter:constant_propagation_is_easier}.
Similarly, a $\sigma$-function $(l^1:a_1,\dots,l^m:a_m)=\sigma(a)$ at program point $i$ yields $n$ constraints such as $[a_j]^{l_j} \sqsubseteq  F_v^{l^j}([v_1]^{l^j}, \dots, [v_n]^{l^j})$, which usually simplifies into $[a_j]^{l_j} \sqsubseteq [a]^i$.
Given a program that fulfills the SSI property for $E^{ssi}_{\mathit{dense}}$ and the set of transfer functions $F_v^s$, we show here how to build an equivalent sparse constrained system.  

\begin{definition}[SSI constrained system]
\label{def:ssi_eq}
Consider that a program in SSI form gives us a constraint system that associates with each variable $v$ the constraints $[v]^i \sqsubseteq  F_v^s([v_1]^s, \dots, [v_n]^s)$. We define a system of sparse equations $E^{ssi}_{sparse}$ as follows:

\begin{itemize}

\item For each instruction at a program point $i$ that defines (resp. uses) a variable $v$, we let $a \dots b$ be its set of used (resp. defined) variables. Because of the LINK property, $F^i_v$ depends only on some $[a]^i \dots [b]^i$.
Thus, there exists a function $G^i_v$ defined as the restriction of $F^i_v$ on ${\cal L}_a\times \dots \times{\cal L}_b$, i.e. $G^i_v([a], \dots, [b]) = F^i_v([v_1],\dots, [v_n])$.

\item The sparse constrained system associates with each variable $v$, and each definition (resp. use) point $i$ of $v$, the corresponding constraint $[v]  \sqsubseteq G_v^i([a], \ldots, [b])$ where $a,\dots, b$ are used (resp. defined) at $i$.
\end{itemize}

\end{definition}

The propagation engine discussed in Chapter~\ref{chapter:constant_propagation_is_easier} propagates information forwardly along the def-use chains naturally formed by the SSA-form program.
If a given program fulfills the SSI property for a backward analysis, we can use a very similar propagation algorithm to communicate information backwardly.
Figure~\ref{alg:ssi:propback} shows a worklist algorithm that propagates information backwardly.
A slightly modified version of this algorithm, seen in Figure~\ref{alg:ssi:propforward} propagates information forwardly.


\def\1{\qquad}
\def\2{\1\1}
\def\3{\2\1}
\def\4{\2\2}
\def\5{\3\2}
\def\6{\4\2}
\def\7{\5\2}
\def\8{\6\2}
\def\9{\7\2}
\def\If{{\sf  if }}
\def\Let{{\sf  let }}
\def\Then{{\sf  then }}
\def\Else{{\sf  else}}
\def\Foreach{{\sf foreach }}
\def\For{{\sf for }}
\def\While{{\sf while }}


\newcommand\val[1]{[#1]}
\begin{algorithm}[t!]
\begin{tabular}{rl}
$_1$ & \textsf{function back\_propagate}(transfer\_functions $\cal G$)\\
$_2$ & \1$\var{worklist} = \emptyset$\\
$_3$ & \1\Foreach $\var{v} \in \textrm{vars}$: $\val{v}=\top$\\
$_4$ & \1\Foreach $\var{i} \in \textrm{insts}$: $\var{worklist}\ +\hspace{-0.3em}= i$\\
$_5$ & \1\While $\var{worklist}\neq \emptyset$:\\
$_6$ & \1\1 \Let $i \in \var{worklist}$; $\var{worklist}\ -\hspace{-0.3em}= \var{i}$\\
$_7$ & \1\1 \Foreach $v \in \var{i.uses}()$:\\
$_8$ & \1  \2  $\val{v}_{new} = \val{v} \wedge G_v^i(\val{\var{i.defs}()})$\\
$_9$ &  \1 \2  \If $\val{v} \neq \val{v}_{new}$: \\
$_{10}$& \1   \3 $\var{stack}\ +\hspace{-0.3em}=\var{v.defs}()$\\
$_{11}$& \1   \3 $\val{v} = \val{v}_{new}$\\
\end{tabular}
\caption{\label{alg:ssi:propback} Backward propagation engine under SSI}
\end{algorithm}

\begin{algorithm}[t!]
\begin{tabular}{rl}
$_1$ & \textsf{function forward\_propagate}(transfer\_functions $\cal G$)\\
$_2$ & \1$\var{worklist} = \emptyset$\\
$_3$ & \1\Foreach $\var{v} \in \textrm{vars}$: $\val{v}=\top$\\
$_4$ & \1\Foreach $\var{i} \in \textrm{insts}$: $\var{worklist}\ +\hspace{-0.3em}= i$\\
$_5$ & \1\While $\var{worklist}\neq \emptyset$:\\
$_6$ & \1\1 \Let $i \in \var{worklist}$; $\var{worklist}\ -\hspace{-0.3em}= \var{i}$\\
$_7$ & \1\1 \Foreach $v \in \var{i.defs}()$:\\
$_8$ & \1  \2  $\val{v}_{new} = \val{v} \wedge G_v^i(\val{\var{i.uses}()})$\\
$_9$ &  \1 \2  \If $\val{v} \neq \val{v}_{new}$: \\
$_{10}$& \1   \3 $\var{stack}\ +\hspace{-0.3em}=\var{v.uses}()$\\
$_{11}$& \1   \3 $\val{v} = \val{v}_{new}$\\
\end{tabular}
\caption{\label{alg:ssi:propforward} Forward propagation engine under SSI}
\end{algorithm}

Of course, those algorithms can be made flow sensitive as for Algorithm~\ref{alg:constant_propagation_is_easier:ssa_propagation}. Still we should outline that what is solved here is a constrained system not an equation system as in Chapter~\ref{chapter:ssi}. This explains why in line~8 of algorithms~\ref{alg:ssi:propback} and~\ref{alg:ssi:propforward} $[v]$ appears on the right hand side of the assignment.
This is required as soon as the SSI constrained system has several inequations for the same left-hand-side which can occur as our definition of the SSI property does not ensure the Static Single Assignment or the Static Single Use properties.
Indeed such a guarantee is not necessary to every sparse analysis.
For instance, the sparse constraint system for dead-code elimination has several equations (one per variable use) for the same left-hand-side (one for each variable).
To solve dead-code elimination sparsely, we can convert the target program to standard SSA-form, and then replace $G_v^i$ in Algorithm~\ref{alg:ssi:propback} by ``\emph{i is a useful instruction or one of its definitions is marked as useful}''. 

\subsection{Examples of sparse data-flow analyses}
\label{sub:examples}

As we have mentioned before, many data-flow analyses can be classified as PVL problems.
In this section we present some meaningful examples.

\paragraph{Range analysis revisited}
We start this section revising this chapter's initial example of data-flow analysis, given in Figure~\ref{fig:rangeAnalysis}.
A range analysis acquires information from either the points where variables are defined, or from the points where variables are tested.
In Figure~\ref{fig:rangeAnalysis-sparse} we know that $i$ must be bound to the interval $[0, 0]$ immediately after program point $l_1$.
Similarly, we know that this variable is upper bounded by 100 at entry of program point $l_4$, due to the conditional test that happens before.
Therefore, in order to achieve the SSI property, we should split the live ranges of variables at their definition points, or at the conditionals where they are used.
Figure~\ref{fig:rangeAnalysis-sparse}(a) shows the original example after live range splitting.
In order to ensure the SSI property in this example, the live range of variable $i$ must be split at its definition, and at the conditional test.
The live range of $s$, on the other hand, must be split only at its definition point, as it is not used in the conditional.
Splitting at conditionals is done via $\sigma$-functions.
Figure~\ref{fig:rangeAnalysis-sparse}(b) shows the solution to the range analysis problem.
We can see that each variable is associated with a unique range interval.
The representation that we obtain by splitting live ranges at definitions and conditionals is called the Extended Static Single Assignment (e-SSA) form.
%% This representation can be used to solve many problems different than range analysis, such as taint analysis. 

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{RangeAnalysis-refs}
\caption{(a) The example from Figure~\ref{fig:rangeAnalysis} after live range splitting.
(b) A solution to this instance of the range analysis problem.}
\label{fig:rangeAnalysis-sparse}
% TODO FAB rappeler l'example de la fig 1.1. 
%TODO FAB: add the equations in (b): [i_1]=[i_0] \cups [i_3]; [i_2]=[i_1] \cap [-\infty,99]; [i_3]=[i_2]+1; [s_1]=[s_0]\cups[s_2]; [s_2]=[s_1]+[i_3] (widening required here)
\end{figure}



\paragraph{Class Inference} Some dynamically typed languages, such as Python, Java\-Scrip, Ruby or Lua, represent objects as tables containing methods and fields.
It is possible to improve the execution of programs written in these languages if we can replace these simple tables by actual classes with virtual tables.
A class inference engine tries to assign a class to a variable $v$ based on the ways that $v$ is used.
The Python program in Figure~\ref{fig:classInference}(a) illustrates this optimization.
Our objective is to infer the correct suite of methods for each object bound to variable $v$.
Figure~\ref{fig:classInference}(b) shows the control flow graph of the program, and Figure~\ref{fig:classInference}(c) shows the results of a dense implementation of this analysis.
Because type inference is a backward analysis that extracts information from use sites, we split live ranges at these program points, and rely on $\sigma$-functions to merge them back, as we see in Figure~\ref{fig:classInference}(e).
The use-def chains that we derive from the program representation lead naturally to a constraint system, which we show in Figure~\ref{fig:classInference}(f), where $[v_j]$ denotes the set of methods associated with variable $v_j$.
A fix-point to this constraint system is a solution to our data-flow problem.
This instance of class inference is a Partitioned Variable Problem~(PVP)~\footnote{Actually class inference is no more a PVP as soon as we want to propagate the information through copies.}, because the data-flow information associated with a variable $v$ can be computed independently from the other variables.

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{classInference}
\caption{Class inference analysis as an example of backward data-flow analysis that takes information from the uses of variables.}
\label{fig:classInference}
%TODO FAB: virer les new dans la figure. Virer les points de programme qui ne servent à rien en (d) (car erreur sur l7)
\end{figure}


\paragraph{Null pointer analysis} The objective of null pointer analysis is to determine which references may hold null values.
This analysis allows compilers to remove redundant null-exception tests and helps developers to find null pointer dereferences.
Figure~\ref{fig:nullAnalysis} illustrates this analysis.
Because information is both produced at definition but also at use sites, we split live ranges after each variable is used, as we show in Figure~\ref{fig:nullAnalysis}(b).
For instance, we know that $v_2$ cannot be null and hence the call $v_2.m()$ cannot result in a null pointer dereference exception, otherwise an exception would have been thrown during the invocation $v_1.m()$.
On the other hand, in Figure~\ref{fig:nullAnalysis}(c) we notice that the state of $v_4$ is the meet of the state of $v_3$, definitely not-null, and the state of $v_1$, possibly null, and we must conservatively assume that $v_4$ may be null.


\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{nullAnalysis}
\caption{Null pointer analysis as an example of forward data-flow analysis that takes information from the definitions and uses of variables.} \label{fig:nullAnalysis}
\end{figure}


\section{Building the Intermediate Program Representation}
\label{sec:building}
\def\Sdown{\downarrow}
\def\Sup{\uparrow}
\def\SS{{\cal P}}
\def\Out{\mathrm{Out}}
\def\In{\mathrm{In}}
\def\Defs{\mathrm{Defs}}
\def\Def{\mathrm{Def}}
\def\Uses{\mathrm{Uses}}

A {\em live range splitting strategy} \
$\SS_v = I_\uparrow \cup I_\downarrow$ over a variable $v$ consists of a set
of ``oriented'' program points.
We let $I_\downarrow$ denote a set of points $i$ with forward direction.
Similarly, we let $I_\uparrow$ denote a set of points $i$ with backward
direction.
The live-range of $v$ must be split at least at every point in $\SS_v$.
Going back to the examples from Section~\ref{sub:examples}, we have the live range splitting strategies enumerated below.
The list in Figure~\ref{fig:splittingSt} gives further examples of live range splitting strategies. Corresponding references are given in the last section of this chapter.

\begin{itemize}
\item Range analysis, is a forward analysis that takes information from points where variables are defined, and conditional tests that use these variables.
For instance, in Figure~\ref{fig:rangeAnalysis-sparse}, we have that $\SS_{i} = \{l_1, \Out(l_3), l_4\}_\downarrow$ where $\Out(l_i)$ denotes the exit of $l_i$, and that $\SS_{s}=\{l_2, l_5\}_\downarrow$.

\item Class inference is a backward analysis that takes information from the uses of variables; thus, for each variable, the live-range splitting strategy is characterized by the set $\mathit{Uses}_\uparrow$ where $\mathit{Uses}$ is the set of use points.
For instance, in Figure~\ref{fig:classInference}(e), we have that
$\SS_{v} = \{l_4, l_6,l_7\}_\uparrow$.


\item Null pointer analysis takes information from definitions and uses and propagates this information forwardly.
For instance, in Figure~\ref{fig:nullAnalysis}, we have that
$\SS_{v} = \{l_1, l_2, l_3, l_4\}_\downarrow$.
\end{itemize}

\begin{figure}[t!]
\begin{center}
\begin{small}
\renewcommand\arraystretch{1.4}
\begin{tabular}{| c | c |} \hline
{\bf Client} & {\bf Splitting strategy $\SS$} \\ \hline 
Alias analysis, reaching definitions & $\mathit{Defs}_\downarrow$ \\ 
cond. constant propagation &  \\ \hline  
Partial Redundancy Elimination & $\mathit{Defs}_\downarrow \bigcup \mathit{LastUses}_\uparrow$ \\ \hline 
ABCD, taint analysis,  & $\mathit{Defs}_\downarrow \bigcup \mathit{\Out(Conds)}_\downarrow$ \\ 
range analysis & \\ \hline 
Stephenson's bitwidth analysis & $\mathit{Defs}_\downarrow \bigcup \mathit{\Out(Conds)}_\downarrow \bigcup \mathit{Uses}_\uparrow$  \\ \hline 
Mahlke's bitwidth analysis & $\mathit{Defs}_\downarrow \bigcup \mathit{Uses}_\uparrow$  \\ \hline 
An's type inference, Class inference & $\mathit{Uses}_\uparrow$ \\ \hline 
Hochstadt's type inference & $\mathit{Uses}_\uparrow \bigcup \mathit{\Out(Conds)}_\uparrow$ \\ \hline 
Null-pointer analysis & $\mathit{Defs}_\downarrow \bigcup\mathit{Uses}_\downarrow$ \\ \hline
\end{tabular} \end{small} 
\caption{Live range splitting strategies for different data-flow analyses.
We let $\mathit{Defs}$ ($\mathit{Uses}$) denote the set of instructions that define (use) the variable. $\mathit{Conds}$ denotes the set of instructions that apply a conditional test on a variable; $\Out(\mathit{Conds})$ denotes the exits of the corresponding basic blocks; $\mathit{LastUses}$ denotes the set of instructions where a variable is used, and after which it is no longer live.}
\label{fig:splittingSt} \end{center} \end{figure}

\def\SSIfy{\textsf{SSIfy}}

The algorithm \textsf{\SSIfy} in Figure~\ref{fig:SSIfy} implements a
live range splitting strategy in three steps.
Firstly, it splits live ranges, inserting new definitions of variables into the
program code.
Secondly, it renames these newly created definitions; hence, ensuring that
the live ranges of two different re-definitions of the same variable do not
overlap.
Finally, it removes dead and non-initialized definitions from the program code.
We describe each of these phases in the rest of this section.

\begin{figure}[htbp]
\begin{tabular}{rl}
$_1$& \textsf{function \SSIfy}(var \var{v}, Splitting\_Strategy $\SS_v$)\\
$_2$& \1\textsf{split}($v$, $\SS_v$)\\
$_3$& \1\textsf{rename}($v$)\\
$_4$& \1\textsf{clean}($v$)\\
\end{tabular}
\caption{\label{fig:SSIfy} Split the live ranges of $v$ to convert it to SSI form}
\end{figure}

\paragraph{Splitting live ranges}

In order to implement $\SS_v$ we must split the live ranges of $v$ at each
program point listed by $\SS_v$.
However, these points are not the only ones where splitting might be
necessary.
As we have pointed out in Section~\ref{sub:split}, we might have, for the same original variable, many different sources of information reaching a common program point.
For instance, in Figure~\ref{fig:rangeAnalysis}, there exist two definitions of variable $i$, e.g., $l_1$ and $l_4$, that reach the use of $i$ at $l_3$.
The information that flows forward from $l_1$ and $l_4$ collides at $l_3$, the loop entry.
Hence the live-range of $i$ has to be split immediately before $l_3$ -- at $\In(l_3)$ --, leading,
in our example, to a new definition $i_1$.
In general, the set of program points where information collides can be easily
characterized by the notion of join sets and iterated dominance frontier ($\mathit{DF^+}$) seen in Chapter~\ref{chap:alternative_ssa_construction_algorithms}.
Similarly, split sets created by the backward propagation of information can
be over-approximated by the notion of {\em iterated post-dominance
frontier} ($\mathit{pDF^+}$), which is the dual of
$\mathit{DF^+}$.
That is, the post-dominance frontier is the dominance frontier in a CFG where
direction of edges have been reversed.

\begin{figure}[t!]
\begin{small}
\begin{tabular}{rl}
$_1$&{\sf function split}(var \var{v}, Splitting\_Strategy
$\SS_v = I_\downarrow \cup I_\uparrow$)\\
$_2$&\1 ``compute the set of split points"\\
$_3$&\1$S_\uparrow = \emptyset$\\
$_4$&\1\Foreach $i \in I_\uparrow$:\\
$_5$&\1\1 \If $i.is\_join$:\\
$_6$&\1  \2 \Foreach $e\in incoming\_edges(i)$:\\
$_7$&\1     \3  $S_\uparrow = S_\uparrow \bigcup \Out(pDF^+(e))$\\
$_8$&\1\1 \Else:\\
$_9$&\1  \2 $S_\uparrow = S_\uparrow \bigcup \Out(pDF^+(i))$\\
$_{10}$&\1$S_\downarrow = \emptyset$\\
$_{11}$&\1\Foreach $i \in S_\uparrow \bigcup \Defs(v) \bigcup I_\downarrow$:\\
$_{12}$&\1\1 \If $i.is\_branch$:\\
$_{13}$&\1  \2 \Foreach $e \in outgoing\_edges(i)$\\
$_{14}$&\1      \3 $S_\downarrow = S_\downarrow \bigcup \In(DF^+(e))$\\
$_{15}$&\1\1 \Else:\\
$_{16}$&\1  \2 $S_\downarrow = S_\downarrow \bigcup \In(DF^+(i))$\\
$_{17}$&\1$S = \SS_v \bigcup S_\uparrow \bigcup S_\downarrow$\\
$_{18}$&\1 ``Split live range of $v$ by inserting $\phi$, $\sigma$, and copies"\\
$_{19}$&\1\Foreach  $i \in S$:\\
$_{20}$&\1\1 \If $i$ does not already contain any definition of $v$:\\
$_{21}$&\1   \2  \If $i.is\_join$: insert ``$v=\phi(v,...,v)$" at $i$\\
$_{22}$&\1   \2  \Else \If $i.is\_branch$: insert ``$(v,...,v)= \sigma(v)$" at  $i$\\
$_{23}$&\1   \2 else: insert a copy ``$v=v$" at $i$\\
\end{tabular}
\caption{\label{fig:Spliting} Live range splitting. We use $\In(l)$ to denote a program point immediately before $l$, and $\Out(l)$ to denote a program point immediately after $l$.} 
\end{small}
\end{figure}

Figure~\ref{fig:Spliting} shows the algorithm that we use to create new
definitions of variables.
This algorithm has three main phases.
First, in lines 3-9 we create new definitions to split the live ranges
of variables due to backward collisions of information.
These new definitions are created at the iterated post-dominance
frontier of points that originate information.
If a program point is a join node, then each of its predecessors
will contain the live range of a different definition of $v$, as we ensure
in lines 6-7 of our algorithm.
Notice that these new definitions are not placed parallel to an instruction,
but in the region immediately after it, which we denote by $\Out(\dots)$.
In lines 10-16 we perform the inverse operation: we create new definitions of
variables due to the forward collision of information.
Our starting points, in this case, include also the original definitions of
$v$, as we see in line 11, because we want to stay in SSA form in order to
have access to a fast liveness check as described in Chapter~\ref{chapter:ssa_tells_nothing_of_liveness}.
Finally, in lines 17-23 we actually insert the new definitions of $v$.
These new definitions might be created by $\sigma$ functions (due to $\SS_v$ or
to the splitting in lines 3-9); by $\phi$-functions (due to $\SS_v$ or to the
splitting in lines 10-16); or by parallel copies.

\begin{figure}[t!]
\begin{small}
\begin{tabular}{rl}
$_{1}$&{\sf function rename(var $v$)}\\
$_{2}$&\1 ``Compute use-def \& def-use chains"\\
$_{3}$&\1 ``We consider here that ${\sf stack.peek}()=\bot$ if
{\sf stack.isempty()},\\
$_{4}$&\1~~~and that $def(\bot)=entry$"\\
$_{5}$&\1$stack = \emptyset$\\
$_{6}$&\1\Foreach CFG node $n$ in dominance order:\\
$_{7}$&\1\1 \If exists $v =\phi(v: l^1, \ldots, v: l^q)$ in $\In(n)$:\\
$_{8}$&\1  \2 ${\sf stack.set\_def}(v =\phi(v: l^1, \ldots, v: l^q))$\\
$_{9}$&\1\1 \Foreach instruction $u$ in $n$ that uses $v$:\\
$_{10}$&\1  \2 ${\sf stack.set\_use}(u)$\\
$_{11}$&\1\1 \If exists instruction $d$ in $n$ that defines $v$:\\
$_{12}$&\1  \2 ${\sf stack.set\_def}(d)$\\
$_{13}$&\1\1 \Foreach instruction $(\ldots) =\sigma(v)$ in $\Out(n)$:\\
$_{14}$&\1  \2 ${\sf stack.set\_use}((\ldots) =\sigma(v))$\\
$_{15}$&\1\1 \If exists $(v: l^1, \ldots, v: l^q) =\sigma(v)$ in $\Out(n)$:\\
$_{16}$&\1  \2 \Foreach $v: l^i = v$ in $(v: l^1, \ldots, v: l^q) =\sigma(v)$:\\
$_{17}$&\1     \3 ${\sf stack.set\_def}(v: l^i = v)$\\
$_{18}$&\1\1 \Foreach $m$ in $successors(n)$:\\
$_{19}$&\1  \2 \If exits $v =\phi(\dots, v:l^n, \ldots)$ in $\In(m)$:\\
$_{20}$&\1     \3 ${\sf stack.set\_use}(v = v: l^n)$\\  
\end{tabular}

\begin{tabular}{rl}
$_{21}$&{\sf function stack.set\_use(instruction \var{inst})}:\\
$_{22}$&\1\While $def({\sf stack.peek()})$ does not dominate inst: {\sf stack.pop()}\\
$_{23}$&\1$v_i = {\sf stack.peek()}$\\
$_{24}$&\1replace the uses of $v$ by $v_i$ in inst\\
$_{25}$&\1\If $v_i\neq \bot$: set $\Uses(v_i)=\Uses(v_i) \bigcup$ inst
\end{tabular}

\begin{tabular}{rl}
$_{26}$&{\sf function stack.set\_def}(instruction inst):\\
$_{27}$&\1let $v_i$ be a fresh version of $v$\\
$_{28}$&\1replace the defs of $v$ by $v_i$ in inst\\
$_{29}$&\1set $\Def(v_i)=$ inst\\
$_{30}$&\1${\sf stack.push}(v_i)$
\end{tabular}
\end{small}
\caption{\label{fig:Rename} Versioning} 
\end{figure}

\paragraph{Variable Renaming}
The algorithm in Figure~\ref{fig:Rename} builds def-use and use-def chains
for a program after live range splitting.
This algorithm is similar to the classic algorithm used to rename variables
during the SSA construction that we saw in Chapter~\ref{chap:classical_construction}.
To rename a variable $v$ we traverse the program's dominance tree, from top to
bottom, stacking each new definition of $v$ that we find.
The definition currently on the top of the stack is used to replace all the
uses of $v$ that we find during the traversal.
If the stack is empty, this means that the variable is not defined at this point.
The renaming process replaces the uses of undefined variables by $\bot$ (line~3). 
We have two methods, {\sf stack.set\_use} and {\sf stack.set\_def} that build
the chains of relations between variables.
Notice that sometimes we must rename a single use inside a $\phi$-function,
as in lines~19-20 of the algorithm.
For simplicity we consider this single use as a simple
assignment when calling {\sf stack.set\_use}, as one can see in line 20.
Similarly, if we must rename a single definition inside a $\sigma$-function, then we treat it as a simple assignment, like we do in lines 15-16 of the algorithm.

\begin{figure}[t!]
\begin{small}
\begin{tabular}{rl}
$_{1}$ & \textsf{clean}(var $v$)\\
$_{2}$ & \1 \Let web = $\{ v_i | v_i \textrm{ is a version of } v \}$\\
$_{3}$ & \1 \Let defined = $\emptyset$\\
$_{4}$ & \1 \Let active = \{ $\var{inst} \ | \var{inst}$ actual instruction and $\textrm{web}\cap \var{inst}.\textrm{defs}  \neq \emptyset \}$\\
$_{5}$ & \1 \While $\exists \var{inst} \in \textrm{active}$ s.t. $\textrm{web}\cap\var{inst}.\textrm{defs} \backslash  \textrm{defined}\neq\emptyset$:\\
$_{6}$ & \1  \1 \Foreach $v_i \in \textrm{web}\cap\var{inst}.\textrm{defs} \backslash \textrm{defined}$: \\
$_{7}$ & \1     \2 active = active $\cup$ $\Uses(v_i)$ \\
$_{8}$ & \1     \2 defined = defined $\cup$ $\{ v_i \}$ \\
$_{9}$ & \1 \Let used = $\emptyset$\\
$_{10}$ & \1 \Let active = \{ $\var{inst} \ | \var{inst}$ actual instruction and $\textrm{web}\cap\var{inst}.\textrm{uses} \neq \emptyset \}$\\
$_{11}$ & \1 \While $\exists \var{inst} \in \textrm{active}$ s.t. $\var{inst}.\textrm{uses} \backslash  \textrm{used}\neq\emptyset$:\\
$_{12}$ & \1  \1 \Foreach $v_i \in \textrm{web}\cap\var{inst}.\textrm{uses} \backslash \textrm{used}$: \\
$_{13}$ & \1     \2 active = active $\cup$ $\Def(v_i)$ \\
$_{14}$ & \1     \2 used = used $\cup$ $\{ v_i \}$ \\
$_{15}$ & \1 \Let live = defined $\cap$ used\\
$_{16}$ & \1 \Foreach non actual $\var{inst} \in \Def(web)$:\\
$_{17}$ & \1  \1 \Foreach $v_i$ operand of \var{inst} s.t. $v_i \notin \textrm{live}$:\\
$_{18}$ & \1             \4 replace $v_i$ by $\bot$\\
$_{19}$ & \1  \1 \If $\var{inst}.\textrm{defs}=\{\bot\}$ or $\var{inst}.\textrm{uses}=\{\bot\}$\\
$_{20}$ & \1  \2 remove \var{inst}\\
\end{tabular}
\end{small}
\caption{\label{fig:clean} Dead and undefined code elimination. Original instructions not inserted by \textsf{split} are called \emph{actual} instruction. We let {\em inst}.defs denote the set of variable(s) defined by {\em inst}, and {\em inst}.uses denote the set of variables used by {\em inst}.}
\end{figure}

\paragraph{Dead and Undefined Code Elimination}

Just as Algorithm~\ref{alg:classical_construction_algorithm:pruning}, the algorithm in Figure~\ref{fig:clean} eliminates $\phi$-functions and parallel copies that define variables not actually used in the code. By symmetry, it also eliminates $\sigma$-functions  and parallel copies  that use variables not actually defined in the code.
We mean by ``actual'' instructions, those instructions that already existed in the program before we transformed it with {\sf split}.
In line~3 we let ``web'' be the set of versions of $v$, so as to restrict the cleaning process to variable~$v$, as we see in lines~4-6 and lines~10-12.
The set ``active'' is initialized to actual instructions in line~4.
Then, during the loop in lines~5-8 we add to active $\phi$-functions, $\sigma$-functions, and copies that can reach actual definitions through use-def chains.
The corresponding version of $v$ is then marked as \emph{defined} (line~8).
The next loop, in lines 11-14 performs a similar process, this time to add to the active set instructions that can reach actual uses through def-use chains.
The corresponding version of $v$ is then marked as \emph{used} (line~14).
Each non live variable (see line~15), i.e. either undefined or dead (non used) is replaced by $\bot$ in all $\phi$, $\sigma$, or copy functions where it appears in.
This is done by lines~15-18.
Finally every useless $\phi$, $\sigma$, or copy functions are removed by lines~19-20. 

\subsection{Implementing parallel copies, $\phi$ and $\sigma$-functions}
\label{sub:special}
%TODO: we need to explain how to get rid of copies in parallel with another instruction here.
Traditional instruction sets do not provide $\phi$-functions nor $\sigma$-functions.
Thus, before producing an executable program, the compiler must implement these instructions.
Normally, $\phi$-functions and parallel copies are replaced by ordinary copy instructions, as explained in Section~\ref{sec:classical_construction_algorithm} and Chapter~\ref{chapter:alternative_ssa_destruction_algorithm}.
A possible solution is to get rid of copies and $\sigma$-functions by simply copy-propagating them; thus, leaving for the SSA elimination module the task of replacing $\sigma$-functions with special instructions.
As an example, Figure~\ref{fig:sigImpl}(a) shows the result of copy folding applied on Figure~\ref{fig:classInference}(d). 
The $\sigma$-functions can be implemented as single arity $\phi$-functions.
As an example, Figure~\ref{fig:sigImpl}(b) shows how we would represent the $\sigma$-functions of Figure~\ref{fig:classInference}(d). 
If $l$ is a branch point with $n$ successors that would contain a $\sigma$-function $(v_1:l^1, \ldots, v_n:l^n) =\sigma(v)$, then, for each successor $l^j$ of $l$, we insert at the beginning of $l^j$ an instruction $v_j = \phi(v:l)$.
Notice that it is possible that $l^j$ already contains a $\phi$-function for $v$.
This case happens when the control flow edge $l \rightarrow l^j$ is {\em critical}.
A critical edge links a basic block with several successors to a basic block with several predecessors.
If $l^j$ already contains a $\phi$-function $v' = \phi(\ldots, v_j, \ldots)$, then we rename $v_j$ to $v$.


\begin{figure}[t!]
\centering
\includegraphics[width=\textwidth]{sigImpl}
\caption{(a) getting rid of copies and $\sigma$-functions; (b) implementing $\sigma$-functions via single arity $\phi$-functions.}
\label{fig:sigImpl}
\end{figure}



\section{Further Reading}

The original description of the intermediate program representation known as Static Single Information form was given by Ananian in his Master's thesis~\cite{Ananian99}.
The notation for $\sigma$-functions that we use in this chapter was borrowed from Ananian's work.
The SSI program representation was subsequently revisited by Jeremy Singer in his PhD thesis~\cite{Singer06}.
Singer proposed new algorithms to convert programs to SSI form, and also showed how this program representation could be used to handle truly bidirectional data-flow analyses.
We did not discuss bidirectional data-flow problems, but the interested reader can find examples of such analyses in Khedker {\em et al.}'s work~\cite{Khedker99}.
Working on top of Ananian's and Singer's work, Boissinot {\em et al.}~\cite{Benoit09} have proposed a new algorithm to convert a program to SSI form.
Boissinot {\em et al.} have also separated the SSI program representation in two flavors, which they call {\em weak} and {\em strong}.

There exist other intermediate program representations that, like the SSI form, make it possible to solve some data-flow problems sparsely.
Well-known among these representations is the {\em Extended Static Single Assignment} form, introduced by Bodik {\em et al.} to provide a fast algorithm to eliminate array bound checks in the context of a JIT compiler~\cite{Bodik00}.
Another important representation, which supports data-flow analyses that acquire information at use sites, is the \emph{Static Single Use} form (SSU).
As uses and definitions are not fully symmetric (the live-range can ``traverse'' a use while it cannot traverse a definition) there exists different variants of SSU (eg.~\cite{Plevyak96,George03-IXP,Lo98_registerPromotion}).
For instance, the ``strict'' SSU form enforces that each definition reaches a
single use, whereas SSI and other variations of SSU allow two consecutive uses
of a variable on the same path.
All these program representations are very effective, having seen use in a number of implementations of flow analyses; however, they only fit specific data-flow problems.

The notion of {\em Partitioned Variable Problem} (PVP) was introduced by Zadeck, in his PhD dissertation~\cite{Zadeck84}.
Zadeck proposed fast ways to build data-structures that allow one to solve these problems efficiently.
He also discussed a number of data-flow analyses that are partitioned variable problems.
When re-stating Zadeck's definition we mentioned that there are examples of data-flow problems that cannot be classified as Partitioned Variable Lattice.
Examples include abstract interpretation problems on relational domains, such as Polyhedrons~\cite{Cousot78}, Octagons~\cite{Mine06} and Pentagons~\cite{Logozzo08}.

In terms of data-structures, the first, and best known method proposed to support sparse data-flow analyses is Choi {\em et al.}'s {\em Sparse Evaluation Graph} (SEG)~\cite{Choi91}.
The nodes of this graph represent program regions where information produced by the data-flow analysis might change.
Choi {\em et al.}'s ideas have been further expanded, for example, by Johnson {\em et al.}'s {\em Quick Propagation Graphs}~\cite{Johnson93}, or Ramalingan's {\em Compact Evaluation Graphs}~\cite{Ramalingan02}.
Nowadays we have efficient algorithms that build such data-structures~\cite{Pingali95,Pingali97,Johnson94}.
These data-structures work best when applied on partitioned variable problems.

As opposed to those approaches, the solution promoted by this chapter consists in an intermediate representation (IR) based evaluation graph, and has advantages and disadvantages when compared to the data-structure approach.
The intermediate representation based approach has two disadvantages, which we have already discussed in the context of the standard SSA form.
First it has to be maintained and at some point destructed.
Second, because it increases the number of variables, it might add some overhead to analyses and transformations that do not require it.
On the other hand, IR based solutions to sparse data-flow analyses have many advantages over data-structure based approaches.
For instance, an IR allows concrete or abstract interpretation.
Solving any coupled data flow analysis problem along with a SEG was mentioned in~\cite{Choi91} as an opened problem.
However, as illustrated by the conditional constant propagation problem described in Chapter~\ref{constant_propagation_is_easier}, coupled data-flow analysis can be solved naturally in IR based evaluation graphs.
Last, SSI is compatible with SSA extensions such as gated-SSA described in Chapter~\ref{chapter:vsdg} which allows demand driven interpretation.

The data-flow analyses discussed in this chapter are well-known in the literature.
Class inference was used by Chambers {\em et al.} in order to compile Self programs more efficiently~\cite{Chambers89}.
Nanda and Sinha have used a variant of null-pointer analysis to find which method dereferences may throw exceptions, and which may not~\cite{Nanda09}.
Ananian~\cite{Ananian99}, and later Singer~\cite{Singer06}, have showed how to use the SSI representation to do partial redundancy elimination sparsely.
In addition to being used to eliminate redundant array bound checks~\cite{Bodik00}, the e-SSA form has been used to solve Taint Analysis~\cite{Rimsa11}, and range analysis~\cite{Su05,Gawlitza09}.
Stephenson {\em et al.}~\cite{Stephenson00} described a bit-width analysis that is both forward, and backwards, taking information from definitions, uses and conditional tests.
For another example of bidirectional bitwidth analysis, see Mahlke {\em et al.}'s algorithm~\cite{Mahlke01}.
The type inference analysis that we mentioned in Figure~\ref{fig:splittingSt} was taken from Hochstadt {\em et al.}'s work~\cite{Hochstadt08}.

