\chapter{Static Single Information Form \Author{F. Pereira \andAuthor F.  
Rastello}}
\label{chapter:ssi}
\inputpath{part2}{ssi}
\inputprogress

{
\def\progpoint{program point\xspace}
\def\progpoints{program points\xspace}
\def\splitpoint{control-flow point\xspace}
\def\splitpoints{control-flow points\xspace}
\def\psplit{\emph{Split}\xspace}
\def\pinfo{\emph{Info}\xspace}
\def\plink{\emph{Link}\xspace}
\def\pversion{\emph{Version}\xspace}

\section{Introduction}
\label{sec:ssi:pereira:intro}

The objective of a data-flow analysis is to discover facts that are true about a
program.
We call such facts {\em information}.
Using the notation introduced in 
Chapter~\ref{chapter:constant_propagation_is_easier}, an
information is a element in the data-flow lattice.
For example, the information that concerns liveness analysis is the set of
variables alive at a certain \progpoint.
Similarly to liveness analysis, many other classical data-flow approaches bind
information to pairs formed by a variable and a \progpoint.
However, if an invariant occurs for a variable $v$ at any \progpoint where
$v$ is alive, then we can associate this invariant directly to $v$.
If the intermediate representation of a program guarantees this correspondence 
between
information and variable for every variable, then we say that the program
representation provides the {\em Static Single Information} (SSI)\index{static 
single information}\index{SSI|see{static single information}} property.

In Chapter~\ref{chapter:constant_propagation_is_easier} we have shown how the 
SSA form allows us to solve sparse forward data-flow problems such as constant 
propagation.
In the particular case of constant propagation, the SSA form lets us assign to 
each variable the invariant---or information---of being constant or not.
The SSA intermediate representation gives us this invariant because it splits 
the live-ranges of variables in such a way that each variable name is defined 
only once.
Now we will show that live-range splitting can also provide the SSI property not only to forward, but also to backward data-flow analyses.

Different data-flow analyses might extract information from different program
facts.
Therefore, a program representation may afford the SSI property to some data-flow
analyses, but not to all of them.
For instance, the SSA form naturally provides the SSI property to the reaching
definition analysis.
Indeed, the SSA form provides the static single information property to any
data-flow analysis that obtains information at the definition sites of
variables.
These analyses and transformations include copy and constant propagation as illustrated in Chapter~\ref{chapter:constant_propagation_is_easier}.
However, for a data-flow analysis that derives information from the use sites 
of variables, such as the class inference analysis we will describe in 
Section~\ref{sub:ssi:examples}, the information associated with a variable 
might not be unique along its entire live-range even under SSA: in that case 
the SSA form does not provide the SSI property.

There exists extensions of the SSA form that provide the SSI property to more
data-flow analyses than the original SSA does.
Two classic examples---detailed later---are the {\em Extended-SSA} (e-SSA) 
form, and the {\em Static Single Information} (SSI) form.
The e-SSA form provides the SSI property to analyses that take information from
the definition site of variables, and also from conditional tests where these
variables are used.
The SSI form provides the static single information property to data-flow
analyses that extract information from the definition sites of variables and 
from the last use sites (which we define later).
These different intermediate representations rely on a common strategy to achieve the SSI property: {\em live-range splitting}.
In this chapter we show how to use live-range splitting to build program
representations that provide the static single information property to different
types of data-flow analyses.

\section{Static Single Information}
\label{sec:ssi:pereira:single}

The goal of this section is to define the notion of \emph{Static Single 
Information}\index{Static Single Information}, and to explain how it supports 
the sparse data-flow analyses discussed in 
Chapter~\ref{chapter:constant_propagation_is_easier}.
With this purpose, we revisit the concept of sparse analysis in Section~\ref{sec:ssi:pereira:sparse}.
There exists a special class of data-flow problems, which we call 
\emph{Partitioned Lattice per Variable} (PLV), that fits in the sparse 
data-flow framework of this chapter very well.
We will look more carefully into these problems in Section~\ref{sec:ssi:pereira:pvpPvl}.
The intermediate program representations discussed in this chapter provide the 
static single information property---formalized in 
Section~\ref{sec:ssi:pereira:singProp}---to any PLV problem.
We give in Section~\ref{sec:ssi:pereira:engine} algorithms to solve sparsely 
any data-flow problem that contains the SSI property.
This sparse framework is very broad: many well-known data-flow problems are 
partitioned lattice, as we will see in the examples of 
Section~\ref{sub:ssi:examples}.


\subsection{Sparse Analysis}
\label{sec:ssi:pereira:sparse}

Traditionally, non relational data-flow analyses bind information to pairs formed by a variable and a \progpoint.
Consider for example the problem of \emph{range analysis},\index{range 
analysis}\index{analysis!range} i.e., estimating the interval of values that an 
integer variable may assume throughout the execution of a program.
A traditional implementation of this analysis would find, for each pair $(v,p)$ 
of a variable $v$ and a \progpoint $p$, the interval of possible values that 
$v$ might assume at $p$ (see the example Figure~\ref{fig:rangeAnalysis}).
In this case, we call a \progpoint any region between two consecutive 
instructions and denote by $[v]$ the abstract information associated with 
variable $v$.
Because this approach keeps information at each \progpoint, we call it {\em 
dense},\index{dense analysis} in contrast with the sparse analyses seen in 
Chapter~\ref{chapter:constant_propagation_is_easier},
Section~\ref{chapter:constant_propagation_is_easier:sec:prop-engine}.

\begin{figure}[t!]
  \defineheight{\tikzfigure{range}}

\begin{minipage}[b]{0.2\textwidth}%
  \centerheight{%
  \begin{algorithm}[H]
    $i \gets 0$\;
    $s \gets 0$\;
    \While{$i<100$}{
      $i \gets i+1$\;
      $s \gets s+i$\;
    }
    \Return{$s$}
  \end{algorithm}
  }
\end{minipage}
%
\hfill
%
  \useheightbox
%
\hfill
%
\centerheight{$%
  \begin{array}[b]{cc@{\quad}c}
    \textit{prog. point} & [i] & [s]\\[4pt] \hline\\[-4pt]
  0 & \top & \top\\
  1 & [0,0] & \top\\
  2 & [0,0] & [0,0]\\
  3 & [0,100] & [0, +\infty[\\
  4 & [100,100] & [0,+\infty[\\
  5 & [0,99] & [0,+\infty[\\
  6 & [0,100] & [0,+\infty[\\
  7 & [0,100] & [0,+\infty[\\
  \end{array}
$}
% \vspace{1cm}
\caption{An example of a dense data-flow analysis that finds the range of
possible values associated with each variable at each \progpoint.}
\label{fig:rangeAnalysis}
\end{figure}

The dense approach might keep a lot of redundant information during the
data-flow analysis.
For instance, if we denote $[v]^p$ the abstract state of variable
$v$ at \progpoint $p$, we have for instance in our example $[i]^1 = [i]^2$,
$[s]^5 = [s]^6$ and $[i]^6 = [i]^7$ (see Figure~\ref{fig:rangeAnalysis}).
This redundancy happens because some transfer functions are identities:
In range analysis, an instruction that neither defines nor uses any variable is 
associated with an identity transfer function.
Similarly, the transfer function that updates the abstract state of $i$ at 
\progpoint $2$ is an identity, because the instruction immediately before $2$ 
does not add any new information to the abstract state of $i$, $[i]^2$ is 
updated with the information that flows directly from the predecessor point 
$1$.

The goal of \emph{sparse} data-flow analysis\index{sparse analysis} is to 
shortcut the identity transfer functions, a task that we accomplish by grouping 
contiguous \progpoints bound to identities into larger regions.
Solving a data-flow analysis sparsely has many advantages over doing it densely: because we do not need to keep bitvectors associated with each \progpoint, the sparse solution tends to be more economical in terms of space and time.
Going back to our example, a given variable $v$ may be mapped to the same interval along many consecutive \progpoints.
Furthermore, if the information associated with a variable is invariant along its
entire live-range, then we can bind this information to the variable itself.
In other words, we can replace all the constraint variables
$[v]^p$ by a single constraint variable $[v]$, for each variable $v$
and every $p\in \textrm{live}(v)$.

Although not every data-flow problem can be easily solved sparsely, many of 
them can as they fit into the family of PLV problems described in the next 
section.

\subsection{Partitioned Lattice per Variable (PLV) Problems}
\label{sec:ssi:pereira:pvpPvl}

The class of non-relational data-flow analysis problems we are interested in are the ones that bind information to pairs of program variables and \progpoints.
We design this class of problems as \emph{Partitioned Lattice per 
Variable}\index{partitioned lattice per variable}\index{PLV problems} problems 
and formally describe them as follows.

\begin{definition}[PLV]
Let ${\cal V}=\{v_1,\dots,v_n\}$ be the set of program variables.
Let us consider, without loss of generality, a forward data-flow analysis that 
searches for a maximum.
This data-flow analysis can be written as an equation system that associates to 
each \progpoint $p$, n element of a lattice ${\cal L}$, given by the following 
equation:
$$x^p = \bigwedge_{s \in \textrm{preds}(p)} F^{s\to p}(x^s),$$
where $x^p$ denotes the abstract state associated  with \progpoint $p$, and
% for $s$ a control-flow predecessor of $p$,
$F^{s\to p}$ is the transfer function from predecessor $s$ to $p$.
The analysis can alternatively be written as a constraint system that binds to 
each \progpoint $p$ and each $s\in \textrm{preds}(p)$ the equation $x^p = x^p 
\wedge  F^{s\to p}(x^s)$ or, equivalently, the inequation $$x^p \sqsubseteq  
F^{s\to p}(x^s).$$
The corresponding Maximum Fixed Point (MFP) problem is said to be 
a \emph{Partitioned Lattice per Variable Problem} iff $\cal L$ can be 
decomposed into the product of ${\cal L}_{v_1}\times \dots \times {\cal 
L}_{v_n}$ where each ${\cal L}_{v_i}$ is the lattice associated with program 
variable $v_i$. In other words $x^s$ can be writen as $([v_1]^s,\dots,[v_n]^s)$ 
where $[v]^s$ denotes the abstract state associated to variable $v$ and 
\progpoint $s$. $F^{s\to p}$ can thus be decomposed into the product of 
$F^{s\to p}_{v_1}\times \dots\times F^{s\to p}_{v_n}$ and the constraint system 
decomposed into the inequalities $[v_i]^p\sqsubseteq  F^{s\to 
p}_{v_i}([v_1]^s,\dots,[v_n]^s)$.
\end{definition}

Going back to range analysis, if we denote by $\cal I$ the lattice of integer intervals, then the overall lattice can be written as ${\cal L}={\cal I}^n$, where $n$ is the number of variables.
%
Note that the class of PLV problems includes a smaller class of problems called 
{\em Partitioned Variable Problems} (PVP).\index{partitioned variable 
problems}\index{PVP}
These analyses, which include live variables, reaching definitions and 
forward/backward printing, can be decomposed into a set of sparse data-flow 
problems---usually one per variable---each independent of the others.

Note that not all data-flow analyses are PLV, for instance problems dealing 
with relational information, such as ``$i<j$?'', which needs to hold 
information on \emph{pairs} of variables.


\subsection{The Static Single Information Property}
\label{sec:ssi:pereira:singProp}

If the information associated with a variable is invariant along its
entire live-range, then we can bind this information to the variable itself.
In other words, we can replace all the constraint variables
$[v]^p$ by a single constraint variable $[v]$, for each variable $v$
and every $p\in \textrm{live}(v)$.  Consider the problem of range analysis 
again. There are two types of \splitpoints associated with non-identity 
transfer functions: definitions and conditionals.
(1) At the definition point of variable $v$, $F_v$ simplifies to a function that depends only on some $[u]$ where each $u$ is an argument of the instruction defining $v$;
(2) At the conditional tests that use a variable $v$, $F_v$ can be simplified to a function that uses $[v]$ and possibly other variables that appear in the test.
The other programs points are associated with an identity transfer function and 
can thus be ignored:  $[v]^p =[v]^p \wedge F_v^{s\to p}([v_1]^s, \dots, 
[v_n]^s)$ simplifies to  $[v]^p = [v]^p \wedge [v]^p$ i.e., $[v]^p = [v]^p$.  
This gives the intuition on why a propagation engine along the def-use chains 
of a SSA-form program can be used to solve the constant propagation problem in 
an equivalent, yet ``sparser,'' manner.


A program representation that fulfills the Static Single Information (SSI) 
property allows us to attach the information to variables, instead of 
\progpoints, and needs to fulfills the following four properties:
\psplit forces the information related to a variable to be invariant along its 
entire live-range;
\pinfo forces this information to be irrelevant outside the live-range of the 
variable;
\plink forces the def-use chains to reach the points where information is 
available for a transfer function to be evaluated;
Finally, \pversion provides a one-to-one mapping between variable names and 
live-ranges.

We now give a formal definition of the SSI, and the four properties.



\begin{property}[SSI]
\label{pro:ssi}\textsc{Static Single Information:}
\index{Static Single Information}\index{SSI}
Consider a forward (resp.  backward) monotone PLV problem $E_\textit{dense}$ 
stated as a set of constraints
$$[v]^p \sqsubseteq F_v^{s\to p}([v_1]^s,\dots,[v_n]^s)$$
for every variable $v$, each \progpoint $p$, and each $s \in \textrm{preds}(p)$ 
(resp.  $s\in \textrm{succs}(p)$).
A program representation fulfills the Static Single Information property if and 
only if it fulfills the following four properties:

\begin{description}
\item[\bf\psplit] let $s$ be the unique predecessor (resp. successor) of 
  a \progpoint where a variable $v$ is live and such that $F_v^{s\to p}\neq 
  \lambda x.\bot$ is non-trivial, i.e., is not the simple projection on ${\cal 
  L}_v$, then $s$ should contain a definition (resp. last use) of $v$;
  For ${(v,p)\in \textit{variables}\times \textit{progPoints}}$, let $(Y_v^p)$
  be a maximum solution to $E_\textit{dense}$.  Each node $p$ that has several 
  predecessors (resp. successors), and for which $F_v^{s\to 
  p}(Y_{v_1}^s,\dots,Y_{v_n}^s)$ has different values on its incoming edges 
  $(s\to p)$ (resp. outgoing edges $(p\to s)$), should have a \phifun at the 
  entry of $p$ (resp. \sigmafun at the exit of $p$) for $v$
  as defined next section.\\[-1em]

\item[\bf \pinfo] each program point $p$ such that $v\not\in 
    \textrm{live-out}(p)$ (resp. $v\not\in \textrm{live-in}(p))$)  should be 
    bound to an undefined  transfer function, i.e., $F_v^p=\lambda 
    x.\bot$.\\[-1em]

\item[\bf \plink] each instruction \textit{inst} for which 
  $F_v^{\textit{inst}}$ depends on some $[u]^s$ should contain a use (resp.  
  definition) of $u$ live-in (resp. live-out) at \textit{inst}.\\[-1em]

\item[\bf \pversion] for each variable $v$, $\textrm{live}(v)$ is a connected 
  component of the CFG.
\end{description}
\end{property}

We must split live-ranges using special instructions to provide the SSI 
properties.
A naive way would be to split them between each pair of consecutive 
instructions, then we would automatically provide these properties, as the 
newly created variables would be live at only one \progpoint.
However, this strategy would lead to the creation of many trivial program regions, and we would lose sparsity.
We provide a sparser way to split live-ranges that fit Property~\ref{pro:ssi} 
in Section~\ref{sec:building}.
We may also have to extend the live-range of a variable to cover every 
\progpoint where the information is relevant;
We accomplish this last task by inserting pseudo-uses and pseudo-definitions of 
this variable.

\subsection{Special instructions used to split live-ranges}
\label{sub:ssi:split}

We perform live-range splitting via special instructions: the \sigmafuns and parallel copies that, together with \phifuns, create new definitions of variables.
These notations are important elements of the propagation engine described in the section that follows.
In short, a \sigmafun\index{\sigmafun} (for a branch point) is the dual of 
a \phifun (for a join point), and a parallel copy\index{parallel copy} is 
a copy that \emph{must} be done in parallel with another instruction.
Each of these special instructions, \phifun, \sigmafuns, and parallel copies, 
split live-ranges at different kinds of program points: interior nodes, 
branches and joins.

\emph{Interior nodes}\index{interior node} are program points that have 
a unique predecessor and a unique successor.
At these points we perform live-range splitting via copies.
If the program point already contains another instruction, then this copy 
\emph{must} be done \emph{in parallel} with the existing instruction.
The notation, 
\[\textit{inst} \ \parallel\  v'_1=v_1 \ \parallel\  \dots \ \parallel\  
v'_m=v_m\]
denotes $m$ copies $v'_i=v_i$ performed in parallel with
instruction \textit{inst}.
This means that all the uses of \textit{inst} plus all right-hand variables 
$v_i$ are read simultaneously, then \textit{inst} is computed, then all 
definitions of \textit{inst} plus all left-hand variables $v'_i$ are written 
simultaneously.
For a usage example of parallel copies, we will see later in this chapter 
a example of null pointer analysis Figure~\ref{fig:ssi:nullAnalysis}.


% need phi
We call {\em joins} the program points that have one successor and multiple predecessors.
For instance, two different definitions of the same variable $v$ might be associated with two different constants; hence, providing two different pieces of information about $v$.
To avoid that these definitions reach the same use of $v$ we merge them at the earliest program point where they meet.
We do it via our well-known \phifuns.

% need sigma
In backward analyses the information that emerges from different uses of a variable may reach the same {\em branch point}, which is a program point with a unique predecessor and multiple successors.
To ensure Property~\ref{pro:ssi}, the use that reaches the definition of a
variable must be unique, in the same way that in an SSA-form program the 
definition that reaches a use is unique.
We ensure this property via special instructions called \sigmafuns.
The \sigmafuns are the dual of \phifuns, performing a parallel assignment depending on the execution path taken.
The assignment \[(l^1:v_1^1, \ldots, l^q:v_1^q) = \sigma(v_1) \ \parallel\  \dots \ \parallel\  (l^1:v_m^1, \ldots, l^q:v_m^q) = \sigma(v_m)\] represents $m$ \sigmafuns that assign to each variable $v_i^j$ the value in $v_i$ if control flows into block $l^j$.
As with \phifuns, these assignments happen in parallel, i.e., the $m$ \sigmafuns encapsulate $m$ parallel copies.
Also, notice that variables live in different branch targets are given
different names by the \sigmafun.
%TODO FAB: add reference to gated SSA, loop closed form etc.

\subsection{Propagating Information Forward and Backward}
\label{sec:ssi:pereira:engine}

Let us consider a unidirectional forward (resp. backward) PLV problem 
$E^{\textit{ssi}}_{\textit{dense}}$ stated as a set of equations $[v]^p 
\sqsubseteq  F_v^{s\to p}([v_1]^s, \dots, [v_n]^s)$ (or equivalently $[v]^p 
= [v]^p \wedge    F_v^{s\to p}([v_1]^s, \dots, [v_n]^s$) for every variable 
$v$, each \progpoint $p$, and each $s \in \textrm{preds}(p)$ (resp. $s \in 
\textrm{succs}(p)$).  To simplify the discussion, any \phifun (resp. \sigmafun) 
is seen as a set of copies, one per predecessor (resp. successor), which leads 
to as many constraints.
In other words, a \phifun such as $p:a=\phi(a_1:l^1,\dots,a_m:l^m)$, gives us 
$n$ constraints such as
$$[a]^p \sqsubseteq  F_a^{l^j\to p}([a_1]^{l^j}, \dots, [a_n]^{l^j})$$
which usually simplifies into $[a]^p \sqsubseteq [a_j]^{l^j}$.
%
This last can be written equivalently into the classical meet
$$[a]^p \sqsubseteq \bigwedge_{l^j \in \textrm{preds}(p)} [a_j]^{l^j}$$
used in Chapter~\ref{chapter:constant_propagation_is_easier}.
Similarly, a \sigmafun $(l^1:a_1,\dots,l^m:a_m)=\sigma(p:a)$ after \progpoint 
$p$ yields $n$ constraints such as
$$[a_j]^{l_j} \sqsubseteq  F_v^{p\to l^j}([a_1]^p, \dots, [a_n]^p)$$
which usually simplifies into $[a_j]^{l_j} \sqsubseteq [a]^p$.
Given a program that fulfills the SSI property for $E^{\textit{ssi}}_{\textit{dense}}$ and the set of transfer functions $F_v^s$, we show here how to build an equivalent sparse constrained system.  

%% TODO: (Florent) c'est vraiment utile tout ce formalisme ?
\begin{definition}[SSI constrained system]
\label{def:ssi_eq}
Consider that a program in SSI form gives us a constraint system that 
associates with each variable $v$ the constraints $[v]^p \sqsubseteq  F_v^{s\to 
p}([v_1]^s, \dots, [v_n]^s)$. We define a system of sparse equations 
$E^{\textit{ssi}}_{\textit{sparse}}$ as follows:

\begin{itemize}

\item For each instruction $i$ that defines (resp. uses) a variable $v$, let $a 
  \dots z$ be the set of used (resp. defined) variables. Because of the \plink 
  property, $F^{s\to p}_v$ (that we will denote $F^i_v$ from now) depends only 
  on some $[a]^s \dots [z]^s$.
Thus, there exists a function $G^i_v$ defined as the restriction of $F^i_v$ on 
${\cal L}_a\times \dots \times{\cal L}_z$, i.e., informally, ``$G^i_v([a], 
\dots, [z]) = F^i_v([v_1],\dots, [v_n])$.''
% FAB: I do not like much this in,formal notation
\item The sparse constrained system associates the constraint $[v]  \sqsubseteq 
  G_v^i([a], \ldots, [z])$ to each variable $v$, for each definition (resp.  
  use) point $i$ of $v$, where $a,\dots, z$ are used (resp.  defined) at $i$.
\end{itemize}

\end{definition}

The propagation engine discussed in 
Chapter~\ref{chapter:constant_propagation_is_easier} sends information forwards 
along the def-use chains naturally formed by the SSA-form program.
If a given program fulfills the SSI property for a backward 
analysis,\index{backward analysis} we can use a very similar propagation 
algorithm to communicate information backwards, such as the worklist 
Algorithm~\ref{alg:ssi:propback}.
A slightly modified version, presented in Algorithm~\ref{alg:ssi:propforward},
propagates information forwards.
If necessary, these algorithms can be made control-flow sensitive, like 
Algorithm~\ref{alg:constant_propagation_is_easier:propagation} of 
Chapter~\ref{chapter:constant_propagation_is_easier}.

\def\defall{
  \def\1{\qquad}
  \def\2{\1\1}
  \def\3{\2\1}
  \def\4{\2\2}
  \def\5{\3\2}
  \def\6{\4\2}
  \def\7{\5\2}
  \def\8{\6\2}
  \def\9{\7\2}
  \def\If{{\sf  if }}
  \def\Let{{\sf  let }}
  \def\Then{{\sf  then }}
  \def\Else{{\sf  else}}
  \def\Foreach{{\sf foreach }}
  \def\For{{\sf for }}
  \def\While{{\sf while }}
}


\newcommand\val[1]{[#1]}
\begin{algorithm}[t!]
%  \textsf{function back\_propagate}(transfer\_functions $\cal G$)\\
$\var{worklist} \gets \emptyset$\;
 \lForEach{$v \in \textrm{vars}$}{$\val{v} \gets \top$}
 \lForEach{$i \in \textrm{insts}$}{push(\var{worklist}, $i$)}
 \While{$\var{worklist}\neq \emptyset$}{
   $i \gets \textrm{pop}(\var{worklist}$)\;
   \ForEach{$v \in \var{i.uses}$}{
     $\val{v}_\var{new} \gets \val{v} \land G_v^i(\val{\var{i.defs}})$\; \nllabel{line:diff-forward-backward}
     \If{$\val{v} \neq \val{v}_\var{new}$}{
       $\var{worklist} \gets \var{worklist}\, \cup \var{v.defs}$\;
       $\val{v} \gets \val{v}_\var{new}$\;
     }
   }
 }
\caption{Backward propagation engine under SSI}
\label{alg:ssi:propback}
\end{algorithm}

\begin{algorithm}[t!]
$\var{worklist} \gets \emptyset$\;
 \lForEach{$v \in \textrm{vars}$}{$\val{v} \gets \top$}
 \lForEach{$i \in \textrm{insts}$}{push(\var{worklist}, $i$)}
 \While{$\var{worklist}\neq \emptyset$}{
   $i \gets \textrm{pop}(\var{worklist}$)\;
   \ForEach{$v \in \var{i.defs}$}{
     $\val{v}_\var{new} \gets G_v^i(\val{\var{i.uses}})$\;
     \If{$\val{v} \neq \val{v}_\var{new}$}{
       $\var{worklist} \gets \var{worklist}\, \cup \var{v.uses}$\;
       $\val{v} \gets \val{v}_\var{new}$\;
     }
   }
 }
\caption{Forward propagation engine under SSI}
\label{alg:ssi:propforward}
\end{algorithm}

Still we should outline a quite important subtlety that appears in 
line~\ref{line:diff-forward-backward} of algorithms~\ref{alg:ssi:propback} 
and~\ref{alg:ssi:propforward}: $[v]$ appears on the right hand side of the 
assignment for Algorithm~\ref{alg:ssi:propback} while it does not for 
Algorithm~\ref{alg:ssi:propforward}. This comes from the asymmetry of our SSI 
form that ensures (for practical purpose only as we will explain soon) the 
Static Single Assignment property but not the Static Single Use (SSU) property.
If we have several uses of the same variable, then the sparse backward 
constraint system will have several inequations---one per variable use---with 
the same left-hand-side.
Technically this is the reason why we manipulate a constraint system (system 
with inequations) and not an equation system as in 
Chapter~\ref{chapter:constant_propagation_is_easier}. Both systems can be 
solved\footnote{In the ideal world with monotone framework and lattice of 
finite height.} using a scheme known as \emph{chaotic iteration}\index{chaotic 
iteration} such as the worklist algorithm we provide here. The slight and 
important difference for a constraint system as opposed to an equation system, 
is that one needs to meet $G_v^i(\dots)$ with the old value of $[v]$ to ensure 
the monotonicity of the consecutive values taken by $[v]$.
It would be still possible to enforce the SSU property, in addition to the SSA property, of our intermediate representation at the expenses of adding more \phifuns and \sigmafuns.
However, this guarantee is not necessary to every sparse analysis.
The dead-code elimination problem illustrates well this point:
for a program under SSA form, replacing $G_v^i$ in 
Algorithm~\ref{alg:ssi:propback} by the property ``$i$ is a useful instruction 
or one of the variables it defines is marked as useful'' leads to the standard 
SSA-based dead-code elimination algorithm.
The sparse constraint system does have several equations (one per variable use) for the same left-hand-side (one for each variable).
It is not necessary to enforce the SSU property in this instance of dead-code 
elimination, and doing so would lead to a less efficient solution in terms of 
compilation time and memory consumption.
In other words, a code under SSA form fulfills the SSI property for dead-code elimination.

\subsection{Examples of sparse data-flow analyses}
\label{sub:ssi:examples}

As we have mentioned before, many data-flow analyses can be classified as PLV problems.
In this section we present some meaningful examples.

\paragraph{Range analysis revisited}
We start this section revising the initial example of data-flow analysis of 
this chapter, given in Figure~\ref{fig:rangeAnalysis}.
\index{range analysis}\index{analysis!range}
A range analysis acquires information from either the points where variables are defined, or from the points where variables are tested.
In the original figure we know that $i$ must be bound to the interval $[0, 0]$ 
immediately after instruction $l_1$.
Similarly, we know that this variable is upper bounded by 100 when arriving at 
$l_4$, due to the conditional test that happens before.
Therefore, in order to achieve the SSI property, we should split the live-ranges of variables at their definition points, or at the conditionals where they are used.
Figure~\ref{fig:rangeAnalysis-sparse} shows on the left the original example 
after live-range splitting.
In order to ensure the SSI property in this example, the live-range of variable $i$ must be split at its definition, and at the conditional test.
The live-range of $s$, on the other hand, must be split only at its definition point, as it is not used in the conditional.
Splitting at conditionals is done via \sigmafuns.
The representation that we obtain by splitting live-ranges at definitions and conditionals is called the Extended Static Single Assignment (e-SSA) form.
Figure~\ref{fig:rangeAnalysis-sparse} also shows on the right the result of the 
range analysis on this intermediate representation.
This solution assigns to each variable a unique range interval.

\begin{figure}[t!]
\hfill
  \defuseheight{\tikzfigure{range-ssi}}
\hfill
\centerheight{
    $\begin{array}[b]{ll}
      [i_1] = [i_1] \cup [0,0] &= [0,0]\\

      [s_1] = [s_1] \cup [0,0] &= [0,0]\\

      [i_2] = [i_2] \cup [i_1] \cup [i_4] &= [0,100]\\

      [s_2] = [s_2] \cup [s_1] \cup [s_3] &= [0,+\infty[\\

      [i_3] = [i_3] \cup \left([i_2] \cap \left]-\infty,99\right]\right) &= [0,99]\\

      [i_4] = [i_4] \cup \left([i_3] + 1\right) &= [1,100]\\

      [s_3] = [s_3] \cup \left([s_2] + [i_4]\right) &= [1,+\infty[\\~\\\\

      \end{array}
    $
    }
    \hfill\null
\caption{Live-range splitting on Figure~\ref{fig:rangeAnalysis} and a solution 
to this instance of the range analysis problem.}
\label{fig:rangeAnalysis-sparse}
% TODO FAB rappeler l'example de la fig 1.1. 
%TODO FAB: add the equations in (b): [i_1]=[i_0] \cups [i_3]; [i_2]=[i_1] \cap [-\infty,99]; [i_3]=[i_2]+1; [s_1]=[s_0]\cups[s_2]; [s_2]=[s_1]+[i_3] (widening required here)
\end{figure}



\paragraph{Class Inference}
Some dynamically typed languages, such as Python, Java\-Scrip, Ruby or Lua, 
represent objects as tables containing methods and fields.
\index{class inference}
It is possible to improve the execution of programs written in these languages if we can replace these simple tables by actual classes with virtual tables.
A class inference engine tries to assign a class to a variable $v$ based on the ways that $v$ is defined and used.
Figure~\ref{fig:classInference} illustrates this optimization on a Python 
program \subref{sub:classInference-cfg}.
Our objective is to infer the correct suite of methods for each object bound to variable $v$.
Figure~\ref{sub:classInference-full} shows the results of a dense 
implementation of this analysis.
Because type inference is a backward analysis that extracts information from 
use sites, we split live-ranges using parallel copies at these program points, 
and rely on \sigmafuns to merge them back, as shown on 
Figure~\ref{sub:classInference-ssi}.
The use-def chains that we derive from the program representation lead 
naturally to a constraint system, shown on 
Figure~\ref{sub:classInference-sparse}, where $[v_j]$ denotes the set of 
methods associated with variable $v_j$.
A fix-point to this constraint system is a solution to our data-flow problem.
This instance of class inference is a Partitioned Variable Problem 
(PVP),\footnote{Actually class inference is no more a PVP as soon as we want to 
propagate the information through copies.} because the data-flow information 
associated with a variable $v$ can be computed independently from the other 
variables.

\begin{figure}[t!]
  \begin{tabular}[c]{cc}
  \subfloat[]{%
    \label{sub:classInference-cfg}%
    \tikzsubfigure[1]{class-inference}%
  }
  &
  \subfloat[]{%
    \label{sub:classInference-ssi}%
    \tikzsubfigure[2]{class-inference}%
  } \\[-1em]

  \subfloat[]{%
    \label{sub:classInference-full}%
\begin{minipage}[b]{0.35\textwidth}
  \centering
  \begin{equation*}
    \begin{array}[b]{cc}
      \textit{prog. point} & [v]\\ \hline
      1 & \{m_1, m_3\}\\
      2 & \{m_1, m_3\}\\
      3 & \{m_1, m_3\}\\
      4 & \{m_3\}\\
      5 & \top\\
      6 & \{m_2, m_3\}\\
      7 & \{m_3\}
    \end{array}
  \end{equation*}
\end{minipage}\hfill
  }
  &
  % \hfill
  \subfloat[]{%
    \label{sub:classInference-sparse}%
\begin{minipage}[b]{0.5\textwidth}
$$
\begin{array}[b]{lll}
[v_6] &=& [v_6] \wedge \{ m_3 \} = \{ m_3 \}\\

[v_5] &=& [v_5] \wedge [v_6] = \{m_3\}\\

[v_4] &=& [v_4] \wedge [v_6] = \{m_3\}\\

[v_2] &=& [v_2] \wedge \left(\{m_1\} \wedge [v_4]\right) = \{m_1,m_3\}\\

[v_3] &=& [v_3] \wedge \left(\{m_2\} \wedge [v_4]\right) = \{m_2,m_3\}\\

[v_1] &=& [v_1] \wedge [v_2] = \{m_1,m_3\}\\

[v_1] &=& [v_1] \wedge [v_7] = \{m_1,m_3\}
\end{array}
$$%
\end{minipage}%
}
\end{tabular}
% \hfill
\caption{Class inference analysis as an example of backward data-flow analysis that takes information from the uses of variables.}
\label{fig:classInference}
%TODO FAB: virer les new dans la figure. Virer les points de programme qui ne servent à rien en (d) (car erreur sur l7)
\end{figure}


\paragraph{Null pointer analysis} The objective of null pointer 
analysis\index{null pointer analysis}\index{analysis!null pointer} is to 
determine which references may hold null values.
This analysis allows compilers to remove redundant null-exception tests and 
helps developers find null pointer dereferences.
Figure~\ref{fig:ssi:nullAnalysis} illustrates this analysis.
Because information is produced not only at definition but also at use sites, 
we split live-ranges after each variable is used as show on Figure~\ref{sub:ssi:nullAnalysis-ssi}.
For instance, we know that $v_2$ cannot be null, otherwise an exception would 
have been thrown during the invocation $v_1.m()$;
Hence the call $v_2.m()$ cannot result in a null pointer dereference exception.
On the other hand, we notice in Figure~\ref{sub:ssi:nullAnalysis-info} that the 
state of $v_4$ is the meet of the state of $v_3$, definitely not-null, and the 
state of $v_1$, possibly null, and we must conservatively assume that $v_4$ may 
be null.


\begin{figure}[t!]
  \defineheight{\tikzsubfigure[1]{null-pointer}}
  \subfloat[]{
    \label{sub:ssi:nullAnalysis-cfg}
    \useheightbox
  }
  \hfill
  \subfloat[]{
    \label{sub:ssi:nullAnalysis-ssi}
    \tikzsubfigure[2]{null-pointer}
  }
  \hfill
  \subfloat[]{
    \label{sub:ssi:nullAnalysis-info}
    \centerheight{$%
      \begin{array}{l}
        [v_1] = [v_1] \wedge 0 = 0\\[3pt]

        [v_2] = [v_2] \wedge \nonnull = \nonnull\\[3pt]

        [v_3] = [v_3] \wedge \nonnull = \nonnull\\[3pt]

        [v_4] = [v_4] \wedge \left([v_3] \wedge [v_1]\right) = 0
      \end{array}$%
    }
  }
% \end{minipage}
\caption{Null pointer analysis as an example of forward data-flow analysis that takes information from the definitions and uses of variables ($0$ represents the fact that the pointer is possibly null, $\nonnull$ if it cannot be).}
\label{fig:ssi:nullAnalysis}
%TODO: add [v4]=possibly null
\end{figure}

\section{Construction and Destruction of the Intermediate Program Representation}
\label{sec:building}
\def\Sdown{\downarrow}
\def\Sup{\uparrow}
\def\SS{{\cal P}}
\def\Out{\textrm{Out}}
\def\In{\textrm{In}}
\def\Defs{\textrm{Defs}}
\def\Def{\textrm{Def}}
\def\Uses{\textrm{Uses}}

In the previous section we have seen how the static single information
property gives the compiler the opportunity to solve a data-flow problem sparsely.
However, we have not yet seen how to convert a program to a format that provides the SSI property.
This is a task that we address in this section, via the three-steps algorithm from Section~\ref{sub:ssi:ssify}.

\subsection{Splitting strategy}
A {\em live-range splitting strategy}\index{splitting strategy} \
$\SS_v = I_\uparrow \cup I_\downarrow$ over a variable $v$ consists of a set
of ``oriented'' program points.
We let $I_\downarrow$ denote a set of points $i$ with forward direction.
Similarly, we let $I_\uparrow$ denote a set of points $i$ with backward
direction.
The live-range of $v$ must be split at least at every point in $\SS_v$.
Going back to the examples from Section~\ref{sub:ssi:examples}, we have the live-range splitting strategies enumerated below.
The list in Figure~\ref{fig:splittingSt} gives further examples of live-range splitting strategies. Corresponding references are given in the last section of this chapter.

\begin{itemize}
  \item Range analysis\index{range analysis}\index{analysis!range} is a forward 
    analysis that takes information from points where variables are defined and 
    conditional tests that use these variables.
For instance, in Figure~\ref{fig:rangeAnalysis}, we have $\SS_{i} = \{l_1, 
\Out(l_3), l_4\}_\downarrow$ where $\Out(l_i)$ is the exit of $l_i$ (i.e., the 
\progpoint immediately after $l_i$), and $\SS_{s}=\{l_2, l_5\}_\downarrow$.

\item Class inference\index{class inference} is a backward analysis that takes 
  information from the uses of variables; thus, for each variable, the 
  live-range splitting strategy is characterized by the set 
  $\textit{Uses}_\uparrow$ where $\textit{Uses}$ is the set of use points.
For instance, in Figure~\ref{fig:classInference}, we have $\SS_{v} = \{l_4, 
l_6,l_7\}_\uparrow$.


\item Null pointer analysis\index{null pointer analysis}\index{analysis!null 
  pointer} takes information from definitions and uses and propagates this 
  information forwardly.
For instance, in Figure~\ref{fig:ssi:nullAnalysis}, we have that
$\SS_{v} = \{l_1, l_2, l_3, l_4\}_\downarrow$.
\end{itemize}

\begin{figure}[t!]
\begin{center}
\begin{small}
\renewcommand\arraystretch{1.4}
\begin{tabular}{ c | c }
{\bf Client} & {\bf Splitting strategy $\SS$} \\  \hline

Alias analysis, reaching defs., cond. constant propagation  
& $\textit{Defs}_\downarrow$ \\

Partial Redundancy Elimination & $\textit{Defs}_\downarrow \bigcup \textit{LastUses}_\uparrow$ \\

ABCD, taint analysis, range analysis & $\textit{Defs}_\downarrow \bigcup 
\Out\textit{(Conds)}_\downarrow$ \\

Stephenson's bitwidth analysis & $\textit{Defs}_\downarrow \bigcup 
\Out\textit{(Conds)}_\downarrow \bigcup \textit{Uses}_\uparrow$  \\

Mahlke's bitwidth analysis & $\textit{Defs}_\downarrow \bigcup 
\textit{Uses}_\uparrow$  \\

An's type inference, Class inference & $\textit{Uses}_\uparrow$ \\

Hochstadt's type inference & $\textit{Uses}_\uparrow \bigcup 
\Out\textit{(Conds)}_\uparrow$ \\

Null-pointer analysis & $\textit{Defs}_\downarrow 
\bigcup\textit{Uses}_\downarrow$ \\

\end{tabular}
\end{small}

\caption{Live-range splitting strategies for different data-flow analyses.
$\textit{Defs}$ (resp. $\textit{Uses}$) denotes the set of instructions that define (resp. use) the variable; $\textit{Conds}$ denotes the set of instructions that apply a conditional test on a variable; $\Out(\textit{Conds})$ denotes the exits of the corresponding basic blocks; $\textit{LastUses}$ denotes the set of instructions where a variable is used, and after which it is no longer live.}
\label{fig:splittingSt}
\end{center}
\end{figure}

\def\SSIfy{\texttt{SSIfy}\xspace}

The algorithm \SSIfy in Figure~\ref{fig:SSIfy} implements a
live-range splitting strategy in three steps.
Firstly, it splits live-ranges, inserting new definitions of variables into the
program code.
Secondly, it renames these newly created definitions; hence, ensuring that
the live-ranges of two different re-definitions of the same variable do not
overlap.
Finally, it removes dead and non-initialized definitions from the program code.
We describe each of these phases in the rest of this section.

\begin{figure}[htbp]
  {
  \begin{algorithm}[H]
    \SetKwFunction{SSIfy}{SSIfy}
    \SetKwFunction{ssiSplit}{split}
    \SetKwFunction{ssiRename}{rename}
    \SetKwFunction{ssiClean}{clean}

  % \begin{function}[H]
    \Fn{\SSIfy{var \var{v}, Splitting\_Strategy $\SS_v$}}{
       \ssiSplit{$v$, $\SS_v$}\;
       \ssiRename{$v$}\;
       \ssiClean{$v$}\;
     }
    \end{algorithm}
    % \end{function}
  }
\caption{Split the live-ranges of $v$ to convert it to SSI form.}
\label{fig:SSIfy}
\end{figure}

\subsection{Splitting live-ranges}
\label{sub:ssi:ssify}

In order to implement $\SS_v$ we must split the live-ranges of $v$ at each
program point listed by $\SS_v$.
However, these points are not the only ones where splitting might be
necessary.
As we have pointed out in Section~\ref{sub:ssi:split}, we might have, for the same original variable, many different sources of information reaching a common program point.
For instance, in Figure~\ref{fig:rangeAnalysis}, there exist two definitions of variable $i$, e.g., $l_1$ and $l_4$, that reach the use of $i$ at $l_3$.
The information that flows forward from $l_1$ and $l_4$ collides at $l_3$, the loop entry.
Hence the live-range of $i$ has to be split immediately before $l_3$---at $\In(l_3)$---leading, in our example, to a new definition $i_1$.
In general, the set of \progpoints where information collides can be easily
characterized by the notion of join sets and iterated dominance frontier ($\textit{DF}^+$) seen in Chapter~\ref{chapter:alternative_ssa_construction_algorithms}.
Similarly, split sets created by the backward propagation of information can be over-approximated by the notion of {\em iterated post-dominance frontier} ($\textit{pDF}^+$)\index{iterated post-dominance frontier}, which is the dual of $\textit{DF}^+$.
That is, the post-dominance frontier is the dominance frontier in a CFG where direction of edges have been reversed.
Note that, just as the notion of dominance requires the existence of a unique entry node that can reach every CFG node, the notion of post dominance requires the existence of a unique exit node reachable by any CFG node.
For control-flow graphs that contain several exit nodes or loops with no exit, we can ensure the single-exit property by creating a dummy common exit node and inserting some never-taken exit edges into the program.

\begin{figure}[htbp]
  \def\Sup{S_{\kern.4pt\uparrow}}
  \def\Sdown{S_\downarrow}
  \def\DF{\textit{DF}}
  \def\pDF{\textit{pDF}}
  \begin{algorithm}[H]
    \Fn{\ssiSplit{var \var{v},  Splitting\_Strategy $\SS_v = I_\downarrow \cup I_\uparrow$}}{
    \tcc{compute the set of split nodes}
      $\Sup \gets \emptyset$\;\nllabel{ssi:split:start-new-defs}
      \ForEach{$i \in I_\uparrow$}{
        \If{$i.\textrm{is\_join}$}{
          \ForEach{$e\in \textit{incoming\_edges}\,(i)$}{\nllabel{ssi:split:start-join}
               $\Sup \gets \Sup \cup \Out(\pDF^+(e))$\;\nllabel{ssi:split:end-join}
             }
           }
         \lElse{ $\Sup \gets \Sup \cup \Out(\pDF^+(i))$}
      }\nllabel{ssi:split:end-new-defs}
      $\Sdown \gets \emptyset$\;\nllabel{ssi:split:start-defs-forwd}
      \ForEach{$i \in \Sup \cup \Defs(v) \cup I_\downarrow$}{\nllabel{ssi:split:defs-forws-use-v}
         \If{$i.\textrm{is\_branch}$}{
           \ForEach{$e \in \textit{outgoing\_edges}\,(i)$}{
               $\Sdown \gets \Sdown \cup \In(\DF^+(e))$\;
             }
          }
         \lElse{
           $\Sdown \gets \Sdown \cup \In(\DF^+(i))$
         }
      }\nllabel{ssi:split:end-defs-forwd}
      $S \gets \SS_v \cup \Sup \cup \Sdown$\;\nllabel{ssi:split:start-insert}
       \tcc{Split live-range of $v$ by inserting $\phi$, $\sigma$, and copies}
       \ForEach{$i \in S$}{
         \If{$i$ does not already contain any definition of $v$}{
           \lIf{$i.\textrm{is\_join}$}{insert ``$v\gets\phi(v,...,v)$'' at $i$}
           \Else{
             \lIf{$i.\textrm{is\_branch}$}{insert ``$(v,...,v)\gets \sigma(v)$" 
             at $i$}
             \lElse{insert a copy ``$v\gets v$" at $i$}
           }
         }
       }
     }\nllabel{ssi:split:end-insert}
  \end{algorithm}
  \caption{Live-range splitting. $\In(l)$ denotes a program point immediately 
  before $l$, and $\Out(l)$ a program point immediately after $l$.}
  \label{ssi:Spliting}
\end{figure}

Figure~\ref{ssi:Spliting} shows the algorithm that we use to create new
definitions of variables.
This algorithm has three main phases.
First, in lines \ref{ssi:split:start-new-defs}-\ref{ssi:split:end-new-defs} we create new definitions to split the live-ranges of variables due to backward collisions of information.
These new definitions are created at the iterated post-dominance frontier of points that originate information.
If a program point is a join node, then each of its predecessors will contain the live-range of a different definition of $v$, as we ensure in lines \ref{ssi:split:start-join}-\ref{ssi:split:end-join} of our algorithm.
Notice that these new definitions are not placed parallel to an instruction, but in the region immediately after it, which we denote by ``$\Out(\dots)$.''
In lines \ref{ssi:split:start-defs-forwd}-\ref{ssi:split:end-defs-forwd} we perform the inverse operation: we create new definitions of variables due to the forward collision of information.
Our starting points $S_\downarrow$, in this case, include also the original definitions of $v$, as we see in line \ref{ssi:split:defs-forws-use-v}, because we want to stay in SSA form in order to have access to a fast liveness check as described in Chapter~\ref{chapter:ssa_tells_nothing_of_liveness}.
Finally, in lines \ref{ssi:split:start-insert}-\ref{ssi:split:end-insert} we actually insert the new definitions of $v$.
These new definitions might be created by $\sigma$ functions (due to $\SS_v$ or to the splitting in lines \ref{ssi:split:start-new-defs}-\ref{ssi:split:end-new-defs}); by \phifuns (due to $\SS_v$ or to the splitting in lines \ref{ssi:split:start-defs-forwd}-\ref{ssi:split:end-defs-forwd}); or by parallel copies.

\begin{figure}[htbp]
  \SetKwFunction{ssiStackSetUse}{stack.set\_use}
  \SetKwFunction{ssiStackSetDef}{stack.set\_def}
  \begin{algorithm}[H]
    \Fn{\ssiRename{var \var{v}}}{
      \tcc{Compute use-def \& def-use chains.}
      $\textrm{stack} \gets \emptyset$\;
      \ForEach{CFG node $n$ in dominance order}{
        \If{$\exists v \gets\phi(v: l^1, \ldots, v: l^q)$ in $\In(n)$}{
          $\ssiStackSetDef(v \gets\phi(v: l^1, \ldots, v: l^q))$\;
        }
        \ForEach{instruction $u$ in $n$ that uses $v$}{
          $\ssiStackSetUse(u)$\;
        }
        \If{$\exists$ instruction $d$ in $n$ that defines $v$}{
          $\ssiStackSetDef(d)$\;
        }
        \ForEach{instruction $(\ldots) \gets\sigma(v)$ in $\Out(n)$}{
          $\ssiStackSetUse((\ldots) \gets\sigma(v))$\;
        }
        \If{$\exists\, (v: l^1, \ldots, v: l^q) \gets\sigma(v)$ in $\Out(n)$}{\nllabel{ssi:rename:start-single-def}
          \ForEach{$v: l^i \gets v$ in $(v: l^1, \ldots, v: l^q) 
          \gets\sigma(v)$}{
            $\ssiStackSetDef(v: l^i \gets v)$\;
          }
        }\nllabel{ssi:rename:end-single-def}
        \ForEach{$m$ in $\textit{successors}\,(n)$}{
          \If{$\exists v \gets\phi(\dots, v:l^n, \ldots)$ in $\In(m)$}{\nllabel{ssi:rename:start-single-use}
            $\ssiStackSetUse(v \gets v: l^n)$\;\nllabel{ssi:rename:end-single-use}
          }
        }
      }
    }
  \end{algorithm}

  \begin{algorithm}[H]
    \Fn{\ssiStackSetUse{instruction \var{inst}}}{
      \tcc{We consider here that $\texttt{stack.peek()}=\bot$ if
      \texttt{stack.isempty()}, and that $\Def\,(\bot)=\textrm{entry}$}
      \While{$\Def\,(\texttt{stack.peek()})$ does not dominate inst}{\texttt{stack.pop()}}
      $v_i \gets \texttt{stack.peek()}$\;
      replace the uses of $v$ by $v_i$ in inst\;
      \lIf{$v_i\neq \bot$}{set $\Uses(v_i)=\Uses(v_i) \cup$ inst}
    }
  \end{algorithm}

  \begin{algorithm}[H]
    \Fn{\ssiStackSetDef{instruction inst}}{
      let $v_i$ be a fresh version of $v$\;
      replace the defs of $v$ by $v_i$ in inst\;
      set $\Def(v_i)=$ inst\;
      $\texttt{stack.push}(v_i)$
    }
  \end{algorithm}
    \caption{Versioning}
    \label{fig:Rename}
\end{figure}

\subsection{Variable Renaming}


The \index{variable renaming}\ssiRename algorithm in Figure~\ref{fig:Rename} builds def-use and use-def chains
for a program after live-range splitting.
This algorithm is similar to the classic algorithm used to rename variables
during the SSA construction that we saw in Chapter~\ref{chapter:classical_construction_algorithm}.
To rename a variable $v$ we traverse the program's dominance tree, from top to
bottom, stacking each new definition of $v$ that we find.
The definition currently on the top of the stack is used to replace all the
uses of $v$ that we find during the traversal.
If the stack is empty, this means that the variable is not defined at this point.
The renaming process replaces the uses of undefined variables by~$\bot$ (see comment of function \ssiStackSetUse).
We have two methods, \ssiStackSetUse and \ssiStackSetDef, that build the chains of relations between variables.
Notice that sometimes we must rename a single use inside a \phifun, as in lines~\ref{ssi:rename:start-single-use}-\ref{ssi:rename:end-single-use} of the algorithm.
For simplicity we consider this single use as a simple assignment when calling \ssiStackSetUse, as one can see line~\ref{ssi:rename:end-single-use}.
Similarly, if we must rename a single definition inside a \sigmafun, then we treat it as a simple assignment, like we do in lines \ref{ssi:rename:start-single-def}-\ref{ssi:rename:end-single-def} of the algorithm.

\begin{figure}[t!]
  \SetKwFunction{ssiClean}{clean}
  \begin{algorithm}[H]
    \Fn{\ssiClean{var $v$}}{
      \Let web = $\{ v_i | v_i \textrm{ is a version of } v \}$\;\nllabel{ssi:clean:web}
       defined $\gets \emptyset$\;
       active $\gets \{ \var{inst} \ | \var{inst}$ actual instruction and $\textrm{web}\cap \var{inst}.\textrm{defs}  \neq \emptyset \}$\;\nllabel{ssi:clean:active}
       \While{$\exists \var{inst} \in \textrm{active}$ | $\textrm{web}\cap\var{inst}.\textrm{defs} \backslash  \textrm{defined}\neq\emptyset$}{\nllabel{ssi:clean:start-loop-def}
         \ForEach{$v_i \in \textrm{web}\cap\var{inst}.\textrm{defs} \backslash \textrm{defined}$}{
           active $\gets$ active $\cup$ $\Uses(v_i)$ \;
           defined $\gets$ defined $\cup$ $\{ v_i \}$ \;\nllabel{ssi:clean:v-defined}
         }
       }\nllabel{ssi:clean:end-loop-def}
       used $\gets \emptyset$\;
       active $\gets \{ \var{inst} \ | \var{inst}$ actual instruction and $\textrm{web}\cap\var{inst}.\textrm{uses} \neq \emptyset \}$\;
       \While{$\exists \var{inst} \in \textrm{active}$ | $\var{inst}.\textrm{uses} \backslash  \textrm{used}\neq\emptyset$}{\nllabel{ssi:clean:start-loop-use}
         \ForEach{$v_i \in \textrm{web}\cap\var{inst}.\textrm{uses} \backslash \textrm{used}$}{
           active $\gets$ active $\cup$ $\Def\,(v_i)$ \;
           used $\gets$ used $\cup$ $\{ v_i \}$ \;\nllabel{ssi:clean:mark-used}
         }
       }\nllabel{ssi:clean:end-loop-use}
       \Let live = defined $\cap$ used\;\nllabel{ssi:clean:live}\nllabel{ssi:clean:start-repl}
       \ForEach{non actual $\var{inst} \in \Def\,(\textrm{web})$}{
         \ForEach{$v_i$ operand of \var{inst} | $v_i \notin \textrm{live}$}{
           replace $v_i$ by $\bot$\;
         }\nllabel{ssi:clean:end-repl}
         \If{$\var{inst}.\textrm{defs}=\{\bot\}$ or $\var{inst}.\textrm{uses}=\{\bot\}$}{\nllabel{ssi:clean:start-rem}
           remove \var{inst}\;
         }
       }
     }\nllabel{ssi:clean:end-rem}
   \end{algorithm}
   \caption{Dead and undefined code elimination. Original instructions not inserted by \protect\ssiSplit are called \emph{actual} instructions.
   {\em inst}.defs denotes the (set) of variable(s) defined by {\em inst}, and {\em inst}.uses denotes the set of variables used by {\em inst}.}
   \label{fig:clean}
\end{figure}

\subsection{Dead and Undefined Code Elimination}

Just as Algorithm~\ref{alg:classical_construction_algorithm:pruning}, the algorithm in Figure~\ref{fig:clean} eliminates \phifuns and parallel copies that define variables not actually used in the code. By symmetry, it also eliminates \sigmafuns  and parallel copies  that use variables not actually defined in the code.
We mean by ``actual'' instructions those that already existed in the program before we transformed it with \ssiSplit.
Line~\ref{ssi:clean:web}, ``web'' is fixed to the set of versions of $v$, so as to restrict the cleaning process to variable~$v$, as we see in the first two loops.
The ``active'' set is initialized to actual instructions line~\ref{ssi:clean:active}.
Then, during the first loop lines~\ref{ssi:clean:start-loop-def}-\ref{ssi:clean:end-loop-def}, we augment it with \phifuns, \sigmafuns, and copies that can reach actual definitions through use-def chains.
The corresponding version of $v$ is hence marked as \emph{defined} (line~\ref{ssi:clean:v-defined}).
The next loop, lines~\ref{ssi:clean:start-loop-use}-\ref{ssi:clean:end-loop-use} performs a similar process, this time to add to the active set instructions that can reach actual uses through def-use chains.
The corresponding version of $v$ is then marked as \emph{used} (line~\ref{ssi:clean:mark-used}).
Each non live variable, i.e., either undefined or dead (non used) hence not in the ``live'' set (line~\ref{ssi:clean:live}) is replaced by $\bot$ in all $\phi$, $\sigma$, or copy functions where it appears by the loop lines~\ref{ssi:clean:start-repl}-\ref{ssi:clean:end-repl}.
Finally every useless $\phi$, $\sigma$, or copy functions are removed by lines~\ref{ssi:clean:start-rem}-\ref{ssi:clean:end-rem}.
\subsection{Implementation Details}
\label{sub:special}

%TODO: we need to explain how to get rid of copies in parallel with another instruction here.
\paragraph{Implementing \sigmafuns: }
The most straightforward way to implement \sigmafuns,\index{\sigmafun} in a compiler that already supports the SSA form, is to represent them by \phifuns.
In this case, the \sigmafuns can be implemented as single arity \phifuns.
As an example, Figure~\ref{sub:sigImpl-single-arity} shows how we would represent the \sigmafuns of Figure~\ref{sub:classInference-sparse}.
If $l$ is a branch point with $n$ successors that would contain a \sigmafun $(l^1:v_1, \ldots, l^n:v_n) \gets \sigma(v)$, then, for each successor $l^j$ of $l$, we insert at the beginning of $l^j$ an instruction $v_j \gets \phi(l^j:v)$.
Note it is possible that $l^j$ already contains a \phifun for $v$.
This case happens when the control-flow edge $l \rightarrow l^j$ is {\em critical}\index{critical edge}:
a critical edge links a basic block with several successors to a basic block with several predecessors.
If $l^j$ already contains a \phifun $v' \gets \phi(\ldots, v_j, \ldots)$, then we rename $v_j$ to $v$.

\begin{figure}[t!]
\hspace{-1.2cm}
  \subfloat[]{
    \label{sub:sigImpl-single-arity}%
    %DOES_NOT_COMPILE_INO_VERLEAF\tikzfigure{class-inference-ssi-out-1}\hspace{-0.6cm}
  }
  \subfloat[]{
    \label{sub:sigImpl-remove-copies}%
    \tikzfigure{class-inference-ssi-out-2}
  }
  \caption{\protect\subref{sub:sigImpl-single-arity} implementing \sigmafuns via single arity \phifuns; \protect\subref{sub:sigImpl-remove-copies} getting rid of copies and \sigmafuns.}
\label{fig:sigImpl}
\end{figure}

\paragraph{SSI Destruction: }
Traditional instruction sets do not provide \phifuns nor \sigmafuns.\index{SSI destruction}
Thus, before producing an executable program, the compiler must implement these instructions.
We have already seen in Chapter~\ref{chapter:classical_construction_algorithm} how to replace \phifuns with actual assembly instructions; however, now we must also replace \sigmafuns and parallel copies.
A simple way to eliminate all the \sigmafuns and parallel copies is via copy-propagation.
In this case, we copy-propagate the variables that these special instructions define.
As an example, Figure~\ref{sub:sigImpl-remove-copies} shows the result of copy folding applied on Figure~\ref{sub:sigImpl-single-arity}.

\section{Further Reading}

The monotone data-flow framework is an old ally of compiler writers.
Since the work of pionners like Prosser~\cite{Prosser59}, Allen~\cite{Allen70,Allen76}, Kildall~\cite{Kildall77} and Hecht~\cite{Hecht77}, data-flow analyses such as reaching definitions, available expressions and liveness analysis have made their way into the implementation of virtually every important compiler.
Many compiler textbooks describes the theoretic basis of the notions of lattice, monotone data-flow framework and fixed points.
For a comprehensive overview of these concepts, including algorithms and formal proofs, we refer the interested reader to Nielson {\em et al.}'s book~\cite{Nielson05} on static program analysis.

The original description of the intermediate program representation known as Static Single Information form was given by Ananian in his Master's thesis~\cite{Ananian99}.
The notation for \sigmafuns that we use in this chapter was borrowed from Ananian's work.
The SSI program representation was subsequently revisited by Jeremy Singer in his PhD thesis~\cite{Singer06}.
Singer proposed new algorithms to convert programs to SSI form, and also showed how this program representation could be used to handle truly bidirectional data-flow analyses.
We did not discuss bidirectional data-flow problems, but the interested reader can find examples of such analyses in Khedker {\em et al.}'s work~\cite{Khedker99}.
Working on top of Ananian's and Singer's work, Boissinot {\em et al.}~\cite{BoissinotBDR12} have proposed a new algorithm to convert a program to SSI form.
Boissinot {\em et al.} have also separated the SSI program representation in two flavors, which they call {\em weak} and {\em strong}.
Tavares {\em et al.}~\cite{Tavares11b} have extended the literature on SSI representations, defining building algorithms and giving formal proofs that these algorithms are correct.
The presentation that we use in this chapter is mostly based on Tavares {\em et
al.}'s work.

There exist other intermediate program representations that, like the SSI form, make it possible to solve some data-flow problems sparsely.
Well-known among these representations is the {\em Extended Static Single Assignment} form, introduced by Bodik {\em et al.} to provide a fast algorithm to eliminate array bound checks in the context of a JIT compiler~\cite{Bodik00}.
Another important representation, which supports data-flow analyses that acquire information at use sites, is the \emph{Static Single Use} form (SSU).
As uses and definitions are not fully symmetric (the live-range can ``traverse'' a use while it cannot traverse a definition), there exists different variants of SSU \cite{Plevyak96,George03-IXP,Lo98_registerPromotion}.
For instance, the ``strict'' SSU form enforces that each definition reaches a
single use, whereas SSI and other variations of SSU allow two consecutive uses
of a variable on the same path.
All these program representations are very effective, having seen use in a number of implementations of flow analyses; however, they only fit specific data-flow problems.

The notion of {\em Partitioned Variable Problem} (PVP) was introduced by Zadeck, in his PhD dissertation~\cite{Zadeck84}.
Zadeck proposed fast ways to build data-structures that allow one to solve these problems efficiently.
He also discussed a number of data-flow analyses that are partitioned variable problems.
There are data-flow analyses that do not meet the Partitioned Lattice per Variable property.
Noticeable examples include abstract interpretation problems on relational domains, such as Polyhedrons~\cite{Cousot78}, Octagons~\cite{Mine06} and Pentagons~\cite{Logozzo08}.
%This domain binds variables together in constraints such as $x + y \leq c$, where $c$ is an integer, and $x, y$ are program variables.

In terms of data-structures, the first, and best known method proposed to support sparse data-flow analyses is Choi {\em et al.}'s {\em Sparse Evaluation Graph} (SEG)~\cite{Choi91}.
The nodes of this graph represent program regions where information produced by the data-flow analysis might change.
Choi {\em et al.}'s ideas have been further expanded, for example, by Johnson {\em et al.}'s {\em Quick Propagation Graphs}~\cite{Johnson93}, or Ramalingam's {\em Compact Evaluation Graphs}~\cite{Ramalingam02}.
Nowadays we have efficient algorithms that build such data-structures~\cite{Pingali95,Pingali97,Johnson94}.
These data-structures work best when applied on partitioned variable problems.

As opposed to those approaches, the solution promoted by this chapter consists in an intermediate representation (IR) based evaluation graph, and has advantages and disadvantages when compared to the data-structure approach.
The intermediate representation based approach has two disadvantages, which we have already discussed in the context of the standard SSA form.
First it has to be maintained and at some point destructed.
Second, because it increases the number of variables, it might add some overhead to analyses and transformations that do not require it.
On the other hand, IR based solutions to sparse data-flow analyses have many advantages over data-structure based approaches.
For instance, an IR allows concrete or abstract interpretation.
Solving any coupled data-flow analysis problem along with a SEG was mentioned by Choi {\em et al.}~\cite{Choi91} as an open problem.
However, as illustrated by the conditional constant propagation problem described in Chapter~\ref{chapter:constant_propagation_is_easier}, coupled data-flow analysis can be solved naturally in IR based evaluation graphs.
Last, SSI is compatible with SSA extensions such as gated-SSA described in Chapter~\ref{chapter:vsdg} which allows demand driven interpretation.

The data-flow analyses discussed in this chapter are well-known in the literature.
Class inference was used by Chambers {\em et al.} in order to compile Self programs more efficiently~\cite{Chambers89}.
Nanda and Sinha have used a variant of null-pointer analysis to find which method dereferences may throw exceptions, and which may not~\cite{Nanda09}.
Ananian~\cite{Ananian99}, and later Singer~\cite{Singer06}, have showed how to use the SSI representation to do partial redundancy elimination sparsely.
In addition to being used to eliminate redundant array bound checks~\cite{Bodik00}, the e-SSA form has been used to solve Taint Analysis~\cite{Rimsa11}, and range analysis~\cite{Su05,Gawlitza09}.
Stephenson {\em et al.}~\cite{Stephenson00} described a bit-width analysis that is both forward, and backwards, taking information from definitions, uses and conditional tests.
For another example of bidirectional bitwidth analysis, see Mahlke {\em et al.}'s algorithm~\cite{Mahlke01}.
The type inference analysis that we mentioned in Figure~\ref{fig:splittingSt} was taken from Hochstadt {\em et al.}'s work~\cite{Hochstadt08}.

}
