\chapter{Static Single Information Form \Author{F. Pereira \andAuthor F. Rastello}}
\inputprogress
\label{chap:ssi}

\graphicspath{{img/}{ssi/img/}{part2/ssi/img/}}

\section{Introduction}
\label{sec:ssi:pereira:intro}

The objective of a dataflow analysis is to discover facts that are true about a
program.
We call such facts {\em informations}.
Using the notation introduced in Section~\ref{novillo:sec:prop-engine}, an
information is a point in the dataflow lattice.
For example, liveness analysis is interested in finding out the set of
variables alive at a certain program point.
Similarly to liveness analysis, many other classic dataflow approaches bind
information to pairs formed by a variable and a program point.
However, if the information is true for a variable $v$ at any program point where
$v$ is alive, then we can associate this information directly to $v$.
If a program's intermediate representation guarantees this correspondence between
information and variable for every variable, then we say that the program
representation provides the {\em Single Static Information} (SSI) property.

Different dataflow analysis might extract information from different program
facts.
Therefore, a program representation may afford the SSI property to some dataflow
analysis, but not to all of them.
For instance, the SSA form naturally provides the SSI property to the reaching
definition analysis.
Indeed, the SSA form provides the static single information property to any
dataflow analysis that obtains information at the definition sites of
variables.
These analyses and transformations include classic constant propagation, as
seen in Chapter~\ref{XXX:chap:constant_propagation_is_easier}, and partial redundancy elimination, as seen in Chapter~\ref{XXX:chap:pre_not_helped}.
However, the SSA form does not provide the SSI property to a dataflow analysis
that derives information from the use site of variables.
In other words, because the same variable $v$ in a SSA form program might be used
at different program points, the informations associated with $v$ might not be
unique.

There exist extensions of the SSA form that provide the SSI property to more
dataflow analyses than the original SSA does.
Two classic examples are the {\em Extended-SSA} (e-SSA) form, and the {\em Static Single Information} (SSI) form.
The e-SSA form provides the SSI property to analyses that take information from
the definition site of variables, and also from conditional tests where these
variables are used.
Ananian's SSI form gives the static single information property to dataflow
analyses that extract information from particular use sites of variables, such
as {\em Busy Expression Analysis}.
These different intermediate representations rely on a common strategy to achieve the SSI property: {\em live range splitting}.
In this chapter we show how to use live range splitting to build program
representations that provide the static single information property to different
types of dataflow analysis.

\section{Single Information}
\label{sec:ssi:pereira:single}



\subsection{Sparse Analysis}
\label{sec:ssi:pereira:sparse}

Traditionally, data-flow analyses bind information to pairs formed by a variable and a program point.
Let's consider, as an example, the problem of estimating the interval of values that any integer variable may assume throughout the execution of a program.
An algorithm that solves this problem is called a {\em range analysis}.
A traditional implementation of this analysis would find, for each pair formed by a variable $v$ plus a program point $i$, the interval of possible values that $v$ might assume at $i$.
Figure~\ref{fig:ProgramPoint} illustrates this analysis with an example.
In this case we call a program point any region between two consecutive
instructions and we let $[v]$ denote the abstract information that is associated
with variable $v$.
Because this approach keeps information at each program point, we call it {\em dense}, in contrast with the sparse analyses seen in Section~\ref{novillo:sec:prop-engine}.

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{ProgramPoint}
\caption{An example of a dense data-flow analysis that finds the range of
possible values associated with each variable at each program point.}
\label{fig:aliasAnalysis}
\end{figure}

The dense approach might keep more redundant information during the
data-flow analysis.
For instance, if we let $[v]^i$ denote the abstract state of variable
$v$ at program point $i$, then we have that, in our example, $[i]^1 = [i]^2$,
$[i]^3 = [i]^4$ and $[s]^4 = [s]^5$.
This redundancy happens because some transfer functions are identities.
For instance, in range analysis, an instruction that neither defines nor uses any variable is associated with an identity transfer function.
The transfer function that updates the abstract state of $i$ at program point $p_2$ is an identity.
Because the instruction immediately before $p_2$ does not add any new information to the abstract state of $i$, $[i]^2$ is updated with the information that flows directly from the predecessor point $p_1$.

The goal of sparse data-flow analysis is to shortcut the identity transfer functions, a task that we accomplish by grouping contiguous program points bound to identities into larger regions.
Going back to our example, a given variable $v$ may be mapped to the same interval along many consecutive program points.
Furthermore, if the information associated with a variable is invariant along its
entire live range, then we can bind this information to the variable itself.
In other words, we can replace all the constraint variables
$[v]^i$ by a single constraint variable $[v]$, for each variable $v$
and every $i\in \textrm{live}(v)$.
Not every data-flow problem can be easily solved sparsely; however, many of them can, and in the next section we will see a family of such problems.

\subsection{PVP and PVL Data-Flow Problems}
\label{sec:ssi:pereira:pvpPvl}

There exists a class of data-flow problems that can be naturally solved via sparse data-flow analyses.
These problems are called {\em Partitioned Data-flow Analyses} (PDA).
These analyses, which include live variables, reaching definitions and forward/backward printing, can be decomposed into a set of sparse data-flow problems -- usually one per variable -- each independent on the other.
We define the PDA family of data-flow problems via the sum of two notions: Partitioned Variable Problem (PVP) and Partitioned Variable Lattice (PVL).

\begin{definition}[PVP/PVL]
\label{pro:pvp}
\textsc{Partitioned Data-Flow Problem}:\\Let ${\cal V}=\{v_1,\dots,v_n\}$ be the set of program variables.
We consider, without loss of generality, a forward data-flow analysis.
This data-flow analysis is an equation system that associates, with each program point $i$, an element of a lattice ${\cal L}$, given by the equation $[x]^i = \bigwedge_{s \in \mathit{pred}(i)} F^s([x]^s)$, where $[x]^i$ denotes the abstract state associated with variable $x$ at program point $i$, and $F^s$ is the transfer function associated with program point $s$.
The analysis can be written\footnote{As far as we are concerned with finding its maximum solution. See for example Section~1.3.2 of~\cite{Nielson05}.} as a constraint system that binds to each program point $i$ and each $s\in \mathit{pred}(i)$ the equation $[x]^i = [x]^i \wedge  F^s([x]^s)$ or, equivalently, the inequation $[x]^i \sqsubseteq  F^s([x]^s)$.
The corresponding Maximum Fixed Point (MFP) problem is said to be a \emph{Partitioned Variable Problem} iff:
\begin{description}
\item {\bf [PVL]:} $\cal L$ can be decomposed into the product of ${\cal L}_{v_1}\times \dots \times {\cal L}_{v_n}$ where each ${\cal L}_{v_i}$ is the lattice associated with program variable $v_i$.
\item {\bf [PVP]:} each transfer function $F^s$ can also be decomposed into a product $F_{v_1}^s\times F_{v_2}^s \times \dots \times F_{v_n}^s$ where $F_{v_j}^s$ is a function from ${\cal L}_{v_j}$ to ${\cal L}_{v_j}$.
\end{description}
\end{definition}

Liveness analysis is a partitioned variable problem:
the liveness information (lattice of Boolean values ${\cal B}$) can be computed for each \emph{individual} (PVL property) variable \emph{independently} (PVP property).
the overall lattice can be written as a cross product ${\cal L}={\cal B}^n$.
The liveness information for variable $v$ at program point $i$, e.g., $[v]^i$, can be expressed in term of its state at the successors $s$ of $i$:
$[v]^i = [v]^i \wedge F_v^s([v]^s)$ with $F_v^s$ from $\cal B$ to $\cal B$.

Contrary to liveness analysis, most of the data-flow analyses do not provide the PVP property.
However, a large number of them \emph{do} fulfill the PVL property.
Going back to range analysis, if we denote by $\cal I$ the lattice of integer intervals, the overall lattice can be written as ${\cal L}={\cal I}^n$ with $n$ the number of variables (PVL property).
On the other hand, as opposed to liveness information, the range interval of some variable $v$ at program point $i$ has to be expressed in term of the intervals associated with \emph{some other} variables (not only $v$) at the predecessors $S$ of $i$.
In other words,  $[v]^i = [v]^i \wedge  F_v^s([v_1]^s,\dots,[v_n]^s)$ with $F_v^s$ from $\cal L$ to $\cal I$ (and not from $\cal I$ to $\cal I$).

There are data-flow analyses that do not meet the PVL property.
Noticeable examples include analyses that rely on relational lattices.
A typical example are the analyses that use the {\em octagon} domain.
This domain binds variables together in constraints such as $x + y \leq c$, where $c$ is an integer, and $x, y$ are program variables.
Analyses on the octagon domain are not PVL because the overall lattice is the product of a quadratic number of lattices.
In our PVL problems, the underlying lattice is the produce of a linear number of sub-lattices, one per program variable.


\subsection{The Single Information Property}
\label{sec:ssi:pereira:singProp}

If the information associated with a variable is invariant along its
entire live range, then we can bind this information to the variable itself.
In other words, we can replace all the constraint variables
$[v]^i$ by a single constraint variable $[v]$, for each variable $v$
and every $i\in \textrm{live}(v)$.
In the context of range analysis, at the program points $s\in \textrm{live}(v)$ that do not redefine or test a variable $v$, we have that the transfer function $[v]^i = [v]^i \wedge F_v^s([v_1]^s, \dots, [v_n]^s)$ simplifies into $[v]^i = [v]^i$.
On the other hand, there are two types of program points where the range analysis finds non-identity transfer functions: definitions and conditionals.
At the definition point of variable $v$, $F_v$ simplifies to a function that depends only on some $[u]$ where each $u$ is an argument of the instruction defining $v$.
Range analysis can also use conditional tests to acquire more information about a variable.
Hence, at the conditional tests that use a variable $v$, $F^v$ can be simplified to a function that uses $v$ and possibly other variables that appear in the test.
This gives the intuition on why a propagation engine along the def-use chains of a SSA-form program can be used to solve the constant propagation problem in an equivalent, yet ``sparser'', manner.
This also paves the way toward a formal definition of the Static Single Information property.

\begin{property}[SSI]
\label{pro:ssi}\textsc{Static Single Information:} Consider a forward (resp. backward) monotone PVL problem $E_{dense}$ stated as a set of constraints $[v]^i \sqsubseteq F_v^s([v_1]^s,\dots,[v_n]^s)$ for every variable $v$, each program point $i$, and each $s \in \mathit{pred}(i)$ (resp. $s\in \mathit{succ}(i)$).
A program representation fulfills the Static Single Information property iff:\begin{description}
\item {\bf [SPLIT]:} for each variable $v$, each $s\in \textrm{live}(v)$ such that $F_v^s\neq \lambda x.\bot$ is not an identity transfer function, should contain a definition (resp. last use) of $v$;
Let $(Y_v^i)_{(v,i)\in variables\times prog\_points}$ be a maximum solution to $
E_{dense}$.
Each join (resp. split) node for which $F_v^s(Y_{v_1}^s,\dots,Y_{v_n}^s)$ has different values on its incoming edges should have a $\phi$-function (resp. $\sigma$ function) for $v$ as defined in Section~\ref{sub:split}.
\item {\bf [INFO]:} each program point $i\not\in \textrm{live}(v)$ should be bound to an undefined transfer function $F_v^s=\lambda x.\bot$.
\item {\bf [LINK]:} each instruction {\em inst} for which $F_v^{inst}$ depends on some $[u]^s$ should contain an use (resp. def) of $u$ live-out (resp. live-in) at
{\em inst}.
\item {\bf [VERSION]:} for each variable $v$, $\textrm{live}(v)$ is a connected component of the CFG.
\end{description}
\end{property}

These properties allows us to attach the information to variables, instead of program points.
The {SPLIT} property forces the information related to a variable to be invariant along its entire live-range.
{INFO} forces this information to be irrelevant outside the live range of the variable.
The {LINK} property forces the def-use chains to reach the points where information is available for a transfer function to be evaluated.
The {VERSION} property provides an one-to-one mapping between variable names and live ranges.

We must split live ranges to provide the SSI properties.
If we split them between each pair of consecutive instructions, then we would automatically provide these properties, as the newly created variables
would be live at only one program point.
However, this strategy would lead to the creation of many trivial program regions, and we would lose sparsity.
In Section~\ref{sec:building} we provide a sparser way to split live ranges that fit Property~\ref{pro:ssi}.
Possibly, we may have to extend the live-range of a variable to cover every program point where the information is relevant.
We accomplish this last task by inserting into the program pseudo-uses and pseudo-definitions of this variable.

\subsection{Special instructions used to split live ranges}
\label{sub:split}

We group program points in three kinds:
interior nodes, branches and joins.
At each place we use a different notation to denote live range splitting.

{\em Interior nodes} are program points that have a unique predecessor and a unique successor.
At these points we perform live range splitting via copies.
If the program point already contains another instruction, then this copy \emph{must} be done \emph{in parallel} with the existing instruction.
The notation, \[inst \ \parallel\  v_1=v'_1 \ \parallel\  \dots \ \parallel\  v_m=v'_m\] denotes $m$ copies $v_i=v'_i$ performed in parallel with
instruction $inst$.
This means that all the uses of $inst$ plus all right-hand variables $v'_i$ are read simultaneously, then $inst$ is computed, then all definitions of $inst$ plus all left-hand variables $v_i$ are written simultaneously.


% need phi
We call {\em joins} the program points that have one successor and multiple predecessors.
For instance, two different definitions of the same variable $v$ might be associated with two different constants; hence, providing two different pieces of information about $v$.
To avoid that these definitions reach the same use of $v$ we merge them at the earliest program point where they meet.
We do it via our well-known $\phi$-functions.

% need sigma
In backward analyses the information that emerges from different uses of a variable may reach the same {\em branch point}, which is a program point with a unique predecessor and multiple successors.
To ensure Property~\ref{pro:ssi}, the use that reaches the definition of a
variable must be unique, in the same way that in a SSA-form program the definition that reaches a use is unique.
We ensure this property via special instructions called $\sigma$-functions.
The $\sigma$-functions are the dual of $\phi$-functions, performing a parallel assignment depending on the execution path taken.
The assignment \[(v_1^1:l^1, \ldots, v_1^q:l^q) = \sigma(v_1) \ \parallel\  \dots \ \parallel\  (v_m^1:l^1, \ldots, v_m^q:l^q) = \sigma(v_m)\] represents $m$ $\sigma$-functions that assign to each variable $v_i^j$ the value in $v_i$ if control flows into block $l^j$.
These assignments happen in parallel, i.e., the $m$ $\sigma$-functions encapsulate $m$ parallel copies.
Also, notice that variables live in different branch targets are given
different names by the $\sigma$-function that ends that basic block.

\subsection{Propagating Information Forwardly and Backwardly}

Let us consider a unidirectional forward (resp. backward) PVL problem $E^{ssi}_{\mathit{dense}}$ stated as a set of equations $[v]^i \sqsubseteq  F_v^s([v_1]^s, \dots, [v_n]^s)$ (or equivalently $[v]^i = [v]^i \wedge    F_v^s([v_1]^s, \dots, [v_n]^s$) for every variable $v$, each program point $i$, and each $s \in \mathit{pred}(i)$ (resp. $s \in \mathit{succ}(i)$). 
Any $\phi$-function $a=\phi(a_1:l^1,\dots,a_m:l^m)$ (resp. $\sigma$-function $(a_1:l^1,\dots,a_m:l^m)=\sigma(a)$) at program point $i$ leads to as many constraints as the set of predecessors (resp. successors) $S_j$ of $i$.
In other words, a $\phi$-function such as $a=\phi(a_1:l^1,\dots,a_m:l^m)$, gives us $n$ constraints such as $[a]^i \sqsubseteq [a_j]^{l_j}$, which we can simplify into the classical meet $[a]^i \sqsubseteq \bigwedge_{l_j \in \mathit{pred}(i)} [a_j]^{l_j}$.
Similarly, a $\sigma$-function $(S_1:a_1,\dots,S_m:a_m)=\sigma(a)$ at program point $i$ yields $n$ constraints such as $[a_j]^{l_j} \sqsubseteq [a]^i$.
Given a program that fulfills the SSI property for $E^{ssi}_{\mathit{dense}}$ and the set of transfer functions $F_v^s$, we show here how to build an equivalent sparse constrained system.  

\begin{definition}[SSI constrained system]
\label{def:ssi_eq}
Consider that a program in SSI form gives us a constraint system that associates with each variable $v$ the constraints $[v]^i = [v]^i \wedge F_v^s([v_1]^s, \dots, [v_n]^s)$. We define a system of sparse equations $E^{ssi}_{sparse}$ as follows:

\begin{itemize}

\item For each instruction at a program point $i$ that defines (resp. uses) a variable $v$, we let $a \dots b$ be its set of used (resp. defined) variables. Because of the LINK property, $F^s_v$ depends only on some $[a]^s \dots [b]^s$.
Thus, there exists a function $G^s_v$ defined as the restriction of $F^s_v$ on ${\cal L}_a\times \dots \times{\cal L}_b$, i.e. $G^s_v([a], \dots, [b]) = F^s_v([v_1],\dots, [v_n])$.

\item The sparse constrained system associates with each variable $v$, and each definition (resp. use) point $s$ of $v$, the corresponding constraint $[v]  \sqsubseteq G_v^s([a], \ldots, [b])$ where $a,\dots, b$ are used (resp. defined) at $s$.
\end{itemize}

\end{definition}

The propagation engine discussed in Chapter~\ref{XXX:chap:constant_propagation_is_easier} propagates information forwardly along the def-use chains naturally formed by the SSA-form program.
If a give program fulfills the SSI property for a backward analysis, we can use a very similar propagation algorithm to communicate information backwardly.
Figure~\ref{fig.propback} shows an algorithm based on chaotic iterations that propagates information forwardly.
A slightly modified version of this algorithm, seen in Figure~\ref{fig.propforward} propagates information backwardly.

\def\1{\qquad}
\def\2{\1\1}
\def\3{\2\1}
\def\4{\2\2}
\def\5{\3\2}
\def\6{\4\2}
\def\7{\5\2}
\def\8{\6\2}
\def\9{\7\2}
\def\If{{\sf  if }}
\def\Let{{\sf  let }}
\def\Then{{\sf  then }}
\def\Else{{\sf  else}}
\def\Foreach{{\sf foreach }}
\def\For{{\sf for }}
\def\While{{\sf while }}


\newcommand\val[1]{[#1]}
\begin{figure}[t!]
\begin{tabular}{rl}
$_1$ & \textsf{function back\_propagate}(transfer\_functions $\cal G$)\\
$_2$ & \1$\var{worklist} = \emptyset$\\
$_3$ & \1\Foreach $\var{v} \in \textrm{vars}$: $\val{v}=\top$\\
$_4$ & \1\Foreach $\var{i} \in \textrm{insts}$: $\var{worklist}\ +\hspace{-0.3em}= i$\\
$_5$ & \1\While $\var{worklist}\neq \emptyset$:\\
$_6$ & \1\1 \Let $i \in \var{worklist}$; $\var{worklist}\ -\hspace{-0.3em}= \var{i}$\\
$_7$ & \1\1 \Foreach $v \in \var{i.uses}()$:\\
$_8$ & \1  \2  $\val{v}_{new} = \val{v} \wedge G_v^i(\val{\var{i.defs}()})$\\
$_9$ &  \1 \2  \If $\val{v} \neq \val{v}_{new}$: \\
$_{10}$& \1   \3 $\var{stack}\ +\hspace{-0.3em}=\var{v.defs}()$\\
$_{11}$& \1   \3 $\val{v} = \val{v}_{new}$\\
\end{tabular}
\caption{\label{fig.propback} Backward propagation engine under SSI}
\end{figure}

\begin{figure}[t!]
\begin{tabular}{rl}
$_1$ & \textsf{function forward\_propagate}(transfer\_functions $\cal G$)\\
$_2$ & \1$\var{worklist} = \emptyset$\\
$_3$ & \1\Foreach $\var{v} \in \textrm{vars}$: $\val{v}=\top$\\
$_4$ & \1\Foreach $\var{i} \in \textrm{insts}$: $\var{worklist}\ +\hspace{-0.3em}= i$\\
$_5$ & \1\While $\var{worklist}\neq \emptyset$:\\
$_6$ & \1\1 \Let $i \in \var{worklist}$; $\var{worklist}\ -\hspace{-0.3em}= \var{i}$\\
$_7$ & \1\1 \Foreach $v \in \var{i.defs}()$:\\
$_8$ & \1  \2  $\val{v}_{new} = \val{v} \wedge G_v^i(\val{\var{i.uses}()})$\\
$_9$ &  \1 \2  \If $\val{v} \neq \val{v}_{new}$: \\
$_{10}$& \1   \3 $\var{stack}\ +\hspace{-0.3em}=\var{v.uses}()$\\
$_{11}$& \1   \3 $\val{v} = \val{v}_{new}$\\
\end{tabular}
\caption{\label{fig.propforward} Forward propagation engine under SSI}
\end{figure}

The SSI constrained system might have several inequations for the same left-hand-side.
Our definition of the SSI property does not ensure the Static Single Assignment or the Static Single Use properties, because such a guarantee is not necessary to every sparse analysis.
For instance, the sparse constraint system for dead-code elimination has several equations (one per variable use) for the same left-hand-side (one for each variable)
To solve dead-code elimination sparsely, we can convert the target program to standard SSA-form, and then replace $G_v^i$ in Figure~\ref{fig.propback} by ``\emph{i is a useful instruction or one of its definitions is marked as useful}''. 

\subsection{Examples of sparse data-flow analyses}
\label{sub:examples}

As we have mentioned before, many data-flow analyses can be classified as PVP/PVL problems.
In this section we present some meaningful examples.

\paragraph{Range analysis revisited}
We start this section revising this chapter's initial example of data-flow analysis, given in Figure~\ref{fig:ProgramPoint}.
A range analysis acquires information from either the points where variables are defined, or from the points where variables are tested.
In Figure~\ref{fig:ProgramPoint} we know that $i$ must be bound to the interval $[0, 0]$ at program point 1, for it has been assigned zero by the instruction immediately before.
Similarly, we know that this variable is upper bounded by 100 at program point 3, due to the conditional test that happens before.
Therefore, in order to achieve the SSI property, we should split the live ranges of variables at their definition points, or at the conditionals where they are used.
Figure~\ref{fig:rangeAnalysis}(a) shows the original example after live range splitting.
In order to ensure the SSI property in this example, the live range of variable $i$ must be split at its definition, and at the conditional test.
The live range of $s$, on the other hand, must be split only at its definition point, as it is not used in the conditional.
Splitting at conditionals is done via $\sigma$-functions.
Figure~\ref{fig:rangeAnalysis}(b) shows the solution to the range analysis problem.
We can see that each variable is associated with a unique range interval.
The representation that we obtain by splitting live ranges at definitions and conditionals is called the Extended Static Single Assignment (e-SSA) form.
This very representation can be used to solve many problems different than range analysis, such as taint analysis.

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{RangeAnalysis}
\caption{(a) The example from Figure~\ref{fig:ProgramPoint} after live range splitting.
(b) A solution to this instance of the range analysis problem.}
\label{fig:taintAnalysis}
\end{figure}


\paragraph{Taint analysis} The objective of taint analysis~\cite{Rimsa11} is to find program vulnerabilities.
In this case, a harmful attack is possible when input data reaches sensitive program sites without going through special functions called sanitizers.
Figure~\ref{fig:taintAnalysis} illustrates this type of analysis.
We have used $\phi$ and $\sigma$-functions to split the live ranges of the variables in Figure~\ref{fig:taintAnalysis}(a) producing the program in Figure~\ref{fig:taintAnalysis}(b).
Lets assume that {\em echo} is a sensitive function, because it is used to generate web pages.
For instance, if the data passed to {\em echo} is a JavaScript program, then we could have an instance of cross-site scripting attack.
Thus, the statement $\mathit{echo} \ v_1$ may be a source of vulnerabilities, as it outputs data that comes directly from the program input.
On the other hand, we know that $\mathit{echo} \ v_2$ is always safe, for variable $v_2$ is initialized with a constant value.
The call $\mathit{echo} \ v_5$ is always safe, because variable $v_5$ has been sanitized; however, the call $\mathit{echo} \ v_4$ might be tainted, as variable $v_4$ results from a failed attempt to sanitize $v$.
The def-use chains that we derive from the program representation leads naturally to a constraint system, which we show in  Figure~\ref{fig:taintAnalysis}(c).

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{taintAnalysis}
\caption{Taint analysis as an example of forward data-flow analysis that takes information from the definitions of variables and conditional tests on these variables.}
\label{fig:taintAnalysis}
\end{figure}

\paragraph{Class Inference} Some dynamically typed languages, such as Python, Java\-Scrip, Ruby or Lua, represent objects as tables containing methods and fields.
It is possible to improve the execution of programs written in these languages if we can replace these simple tables by actual classes with virtual tables.
A class inference engine tries to assign a class to a variable $v$ based on the ways that $v$ is used.
The Python program in Figure~\ref{fig:classInference}(a) illustrates this optimization.
Our objective is to infer the correct suite of methods for each object bound to variable $v$.
Figure~\ref{fig:classInference}(b) shows the control flow graph of the program, and Figure~\ref{fig:classInference}(c) shows the results of a dense implementation of this analysis.
Because type inference is a backward analysis that extracts information from use sites, we split live ranges at these program points, and rely on $\sigma$-functions to merge them back, as we see in Figure~\ref{fig:classInference}(e).
The use-def chains that we derive from the program representation lead naturally to a constraint system, which we show in Figure~\ref{fig:classInference}(f), where $[v_j]$ denotes the set of methods associated with variable $v_j$.
A fix-point to this constraint system is a solution to our data-flow problem.
This instance of class inference is a Partitioned Variable Problem~(PVP), because the data-flow information associated with a variable $v$ can be computed independently from the other variables.

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{classInference}
\caption{Class inference analysis as an example of backward data-flow analysis that takes information from the uses of variables.}
\label{fig:classInference}
\end{figure}

\paragraph{Constant Propagation}

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{ConstPropSsaBook}
\caption{Constant propagation as an example of forward data-flow analysis that takes information from the definitions of variables.}
\label{fig:constantProp}
\end{figure}

There exist many data-flow analyses that are not Partitioned Variable Problems.
Constant propagation is an example:
in this analysis, the abstract state of a variable $v$ is determined by the abstract state of the variables used to define $v$.
Figure~\ref{fig:constantProp} illustrates constant propagation.
We want to find out which variables in the program of Figure~\ref{fig:constantProp}(a) can be replaced by constants.
The CFG of this program is given in Figure~\ref{fig:constantProp}(b).
Constant propagation has a very simple lattice, which we show in Figure~\ref{fig:constantProp}(c).
In this data-flow problem, information is produced at the program points where variables are defined.
Thus, in order to provide Property~\ref{pro:ssi}, we must guarantee that each program point is dominated by a single definition of a variable -- exactly what the SSA form does.
Figure~\ref{fig:constantProp}(d) shows the intermediate representation that we create for the program in Figure~\ref{fig:constantProp}(b).
The def-use chains implicit in our program representation lead 
to the constraint system shown in Figure~\ref{fig:constantProp}(e).

\paragraph{Null pointer analysis} The objective of null pointer analysis is to determine which references may hold null values.
This analysis allows compilers to remove redundant null-exception tests and helps developers to find null pointer dereferences.
Figure~\ref{fig:nullAnalysis} illustrates this analysis.
Because information is produced at use sites, we split live ranges after each variable is used, as we show in Figure~\ref{fig:nullAnalysis}(b).
For instance, we know that the call $v_2.m()$ cannot result in a null pointer dereference exception, otherwise an exception would have been thrown during the invocation $v_1.m()$.
On the other hand, in Figure~\ref{fig:nullAnalysis}(c) we notice that the state of $v_4$ is the meet of the state of $v_3$, definitely not-null, and the state of $v_1$, possibly null, and we must conservatively assume that $v_4$ may be null.


\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{nullAnalysis}
\caption{Null pointer analysis as an example of forward data-flow analysis that takes information from the definitions and uses of variables.} \label{fig:nullAnalysis}
\end{figure}


\section{Building the Intermediate Program Representation}
\label{sec:building}
\def\Sdown{\downarrow}
\def\Sup{\uparrow}
\def\SS{{\cal P}}
\def\Out{\mathrm{Out}}
\def\In{\mathrm{In}}
\def\Defs{\mathrm{Defs}}
\def\Def{\mathrm{Def}}
\def\Uses{\mathrm{Uses}}

A {\em live range splitting strategy} \
$\SS_v = I_\uparrow \cup I_\downarrow$ over a variable $v$ consists of a set
of ``oriented'' program points.
We let $I_\downarrow$ denote a set of points $i$ with forward direction.
Similarly, we let $I_\uparrow$ denote a set of points $i$ with backward
direction.
The live-range of $v$ must be split at least at every point in $\SS_v$.
Going back to the examples from Section~\ref{sub:examples}, we have the live range splitting strategies enumerated below.
The list in Figure~\ref{fig:splittingSt} gives further examples of live range splitting strategies.

\begin{itemize}
\item Taint analysis, like range analysis, is a forward analysis that takes information from points where variables are defined, and conditional tests that use these variables.
For instance, in Figure~\ref{fig:taintAnalysis}, we have that $\SS_{v} = \{l_1, l_2, \Out(l_5)\}_\downarrow$ where $\Out(l_i)$ denotes the exit of $l_i$.

\item Class inference is a backward analysis that takes information from the uses of variables; thus, for each variable, the live-range splitting strategy is characterized by the set $\mathit{Uses}_\uparrow$ where $\mathit{Uses}$ is the set of use points.
For instance, in Figure~\ref{fig:classInference}(e), we have that
$\SS_{v} = \{l_4, l_6,l_7\}_\uparrow$.

\item Constant propagation is a forward analysis that takes information from definition sites.
Thus, for each variable $v$ the live-range splitting strategy is characterized by the set $\mathit{Defs}_\downarrow$ where $\mathit{Defs}$ is the set of
definition points.
For instance, in Figure~\ref{fig:constantProp}, we have that
$\SS_{b} = \{l_2, l_5\}_\downarrow$.

\item Null pointer analysistakes information from definitions and uses and propagates this information forwardly.
For instance, in Figure~\ref{fig:nullAnalysis}, we have that
$\SS_{v} = \{l_1, l_2, l_3, l_4\}_\downarrow$.
\end{itemize}

%TODO: check the PRE described in the citations here; add a reference for alias analysis. Check other references if possible.
\begin{figure}[t!]
\begin{center}
\begin{small}
\renewcommand\arraystretch{1.4}
\begin{tabular}{| c | c |} \hline
{\bf Client} & {\bf Splitting strategy $\SS$} \\ \hline 
Alias analysis, reaching definitions & $\mathit{Defs}_\downarrow$ \\ 
cond. constant propagation &  \\ \hline  
Partial Redundancy Elimination & $\mathit{Defs}_\downarrow \bigcup \mathit{LastUses}_\uparrow$ \\ \hline 
ABCD, taint analysis,  & $\mathit{Defs}_\downarrow \bigcup \mathit{\Out(Conds)}_\downarrow$ \\ 
range analysis & \\ \hline 
Stephenson's bitwidth analysis & $\mathit{Defs}_\downarrow \bigcup \mathit{\Out(Conds)}_\downarrow \bigcup \mathit{Uses}_\uparrow$  \\ \hline 
Mahlke's bitwidth analysis & $\mathit{Defs}_\downarrow \bigcup \mathit{Uses}_\uparrow$  \\ \hline 
An's type inference, Class inference & $\mathit{Uses}_\uparrow$ \\ \hline 
Hochstadt's type inference & $\mathit{Uses}_\uparrow \bigcup \mathit{\Out(Conds)}_\uparrow$ \\ \hline 
Null-pointer analysis & $\mathit{Defs}_\downarrow \bigcup\mathit{Uses}_\downarrow$ \\ \hline
\end{tabular} \end{small} 
\caption{Live range splitting strategies for different data-flow analyses.
We let $\mathit{Defs}$ ($\mathit{Uses}$) denote the set of instructions that define (use) the variable. $\mathit{Conds}$ denotes the set of instructions that apply a conditional test on a variable; $\Out(\mathit{Conds})$ denotes the exits of the corresponding basic blocks; $\mathit{LastUses}$ denotes the set of instructions where a variable is used, and after which it is no longer live.}
\label{fig:splittingSt} \end{center} \end{figure}

\def\SSIfy{\textsf{SSIfy}}

The algorithm \textsf{\SSIfy} in Figure~\ref{fig:SSIfy} implements a
live range splitting strategy in three steps.
Firstly, it splits live ranges, inserting new definitions of variables into the
program code.
Secondly, it renames these newly created definitions; hence, ensuring that
the live ranges of two different re-definitions of the same variable do not
overlap.
Finally, it removes dead and non-initialized definitions from the program code.
We describe each of these phases in the rest of this section.

\begin{figure}[htbp]
\begin{tabular}{rl}
$_1$& \textsf{function \SSIfy}(var \var{v}, Splitting\_Strategy $\SS_v$)\\
$_2$& \1\textsf{split}($v$, $\SS_v$)\\
$_3$& \1\textsf{rename}($v$)\\
$_4$& \1\textsf{clean}($v$)\\
\end{tabular}
\caption{\label{fig:SSIfy} Split the live ranges of $v$ to convert it to SSI form}
\end{figure}

\paragraph{Splitting live ranges through the creation of new definitions
of variables}

In order to implement $\SS_v$ we must split the live ranges of $v$ at each
program point listed by $\SS_v$.
However, these points are not the only ones where splitting might be
necessary.
As we have pointed out in Section~\ref{sub:split}, we might have, for the same original variable, many different sources of information reaching a common program point.
For instance, in Figure~\ref{fig:constantProp}(b), there exist two definitions of variable $b$, e.g., $l_2$ and $l_5$, that reach the use of $b$ at $l_3$.
The information that flows forward from $l_2$ and $l_5$ collides at $l_3$,
the merge point of the if-then-else.
Hence the live-range of $b$ has to be split immediately before $l_3$ -- at $\In(l_3)$ --, leading,
in our example, to a new definition $b_1$.
In general, the set of program points where information collides can be easily
characterized by the notion of join sets and iterated dominance frontier ($\mathit{DF^+}$) seen in Chapter~\ref{chap:alternative_ssa_construction_algorithms}.
Similarly, split sets created by the backward propagation of information can
be over-approximated by the notion of {\em iterated post-dominance
frontier} ($\mathit{pDF^+}$), which is the dual of
$\mathit{DF^+}$.
That is, the post-dominance frontier is the dominance frontier in a CFG where
direction of edges have been reversed.

\begin{figure}[t!]
\begin{small}
\begin{tabular}{rl}
$_1$&{\sf function split}(var \var{v}, Splitting\_Strategy
$\SS_v = I_\downarrow \cup I_\uparrow$)\\
$_2$&\1 ``compute the set of split points"\\
$_3$&\1$S_\uparrow = \emptyset$\\
$_4$&\1\Foreach $i \in I_\uparrow$:\\
$_5$&\1\1 \If $i.is\_join$:\\
$_6$&\1  \2 \Foreach $e\in incoming\_edges(i)$:\\
$_7$&\1     \3  $S_\uparrow = S_\uparrow \bigcup \Out(pDF^+(e))$\\
$_8$&\1\1 \Else:\\
$_9$&\1  \2 $S_\uparrow = S_\uparrow \bigcup \Out(pDF^+(i))$\\
$_{10}$&\1$S_\downarrow = \emptyset$\\
$_{11}$&\1\Foreach $i \in S_\uparrow \bigcup \Defs(v) \bigcup I_\downarrow$:\\
$_{12}$&\1\1 \If $i.is\_branch$:\\
$_{13}$&\1  \2 \Foreach $e \in outgoing\_edges(i)$\\
$_{14}$&\1      \3 $S_\downarrow = S_\downarrow \bigcup \In(DF^+(e))$\\
$_{15}$&\1\1 \Else:\\
$_{16}$&\1  \2 $S_\downarrow = S_\downarrow \bigcup \In(DF^+(i))$\\
$_{17}$&\1$S = \SS_v \bigcup S_\uparrow \bigcup S_\downarrow$\\
$_{18}$&\1 ``Split live range of $v$ by inserting $\phi$, $\sigma$, and copies"\\
$_{19}$&\1\Foreach  $i \in S$:\\
$_{20}$&\1\1 \If $i$ does not already contain any definition of $v$:\\
$_{21}$&\1   \2  \If $i.is\_join$: insert ``$v=\phi(v,...,v)$" at $i$\\
$_{22}$&\1   \2  \Else \If $i.is\_branch$: insert ``$(v,...,v)= \sigma(v)$" at  $i$\\
$_{23}$&\1   \2 else: insert a copy ``$v=v$" at $i$\\
\end{tabular}
\caption{\label{fig:Spliting} Live range splitting. We use $\In(l)$ to denote a program point immediately before $l$, and $\Out(l)$ to denote a program point immediately after $l$.} 
\end{small}
\end{figure}

Figure~\ref{fig:Spliting} shows the algorithm that we use to create new
definitions of variables.
This algorithm has three main phases.
First, in lines 3-9 we create new definitions to split the live ranges
of variables due to backward collisions of information.
These new definitions are created at the iterated post-dominance
frontier of points that originate information.
If a program point is a join node, then each of its predecessors
will contain the live range of a different definition of $v$, as we ensure
in line 6 of our algorithm.
Notice that these new definitions are not placed parallel to an instruction,
but in the region immediately after it, which we denote by $\Out(\dots)$.
In lines 10-16 we perform the inverse operation: we create new definitions of
variables due to the forward collision of information.
Our starting points, in this case, include also the original definitions of
$v$, as we see in line 11, because we want to stay in SSA form in order to
have access to a fast liveness check.
Finally, in lines 17-23 we actually insert the new definitions of $v$.
These new definitions might be created by $\sigma$ functions (due exclusively
to the splitting in lines 3-9); by $\phi$-functions (due exclusively to the
splitting in lines 10-16); or by parallel copies.

\begin{figure}[t!]
\begin{small}
\begin{tabular}{rl}
$_{1}$&{\sf function rename(var $v$)}\\
$_{2}$&\1 ``Compute use-def \& def-use chains"\\
$_{3}$&\1 ``We consider here that ${\sf stack.peek}()=\bot$ if
{\sf stack.isempty()},\\
$_{4}$&\1~~~and that $def(\bot)=entry$"\\
$_{5}$&\1$stack = \emptyset$\\
$_{6}$&\1\Foreach CFG node $n$ in dominance order:\\
$_{7}$&\1\1 \If exists $v =\phi(v: l^1, \ldots, v: l^q)$ in $\In(n)$:\\
$_{8}$&\1  \2 ${\sf stack.set\_def}(v =\phi(v: l^1, \ldots, v: l^q))$\\
$_{9}$&\1\1 \Foreach instruction $u$ in $n$ that uses $v$:\\
$_{10}$&\1  \2 ${\sf stack.set\_use}(u)$\\
$_{11}$&\1\1 \If exists instruction $d$ in $n$ that defines $v$:\\
$_{12}$&\1  \2 ${\sf stack.set\_def}(d)$\\
$_{13}$&\1\1 \Foreach instruction $(\ldots) =\sigma(v)$ in $\Out(n)$:\\
$_{14}$&\1  \2 ${\sf stack.set\_use}((\ldots) =\sigma(v))$\\
$_{15}$&\1\1 \If exists $(v: l^1, \ldots, v: l^q) =\sigma(v)$ in $\Out(n)$:\\
$_{16}$&\1  \2 \Foreach $v: l^i = v$ in $(v: l^1, \ldots, v: l^q) =\sigma(v)$:\\
$_{17}$&\1     \3 ${\sf stack.set\_def}(v: l^i = v)$\\
$_{18}$&\1\1 \Foreach $m$ in $successors(n)$:\\
$_{19}$&\1  \2 \If exits $v =\phi(\dots, v:l^n, \ldots)$ in $\In(m)$:\\
$_{20}$&\1     \3 ${\sf stack.set\_use}(v = v: l^n)$\\  
\end{tabular}

\begin{tabular}{rl}
$_{21}$&{\sf function stack.set\_use(instruction \var{inst})}:\\
$_{22}$&\1\While $def({\sf stack.peek()})$ does not dominate inst: {\sf stack.pop()}\\
$_{23}$&\1$v_i = {\sf stack.peek()}$\\
$_{24}$&\1replace the uses of $v$ by $v_i$ in inst\\
$_{25}$&\1\If $v_i\neq \bot$: set $\Uses(v_i)=\Uses(v_i) \bigcup$ inst
\end{tabular}

\begin{tabular}{rl}
$_{26}$&{\sf function stack.set\_def}(instruction inst):\\
$_{27}$&\1let $v_i$ be a fresh version of $v$\\
$_{28}$&\1replace the defs of $v$ by $v_i$ in inst\\
$_{29}$&\1set $\Def(v_i)=$ inst\\
$_{30}$&\1${\sf stack.push}(v_i)$
\end{tabular}
\end{small}
\caption{\label{fig:Rename} Versioning} 
\end{figure}

\paragraph{Variable Renaming}
The algorithm in Figure~\ref{fig:Rename} builds def-use and use-def chains
for a program after live range splitting.
This algorithm is similar to the classic algorithm used to rename variables
during the SSA construction that we saw in Chapter~\ref{chap:classical_construction}.
To rename a variable $v$ we traverse the program's dominance tree, from top to
bottom, stacking each new definition of $v$ that we find.
The definition currently on the top of the stack is used to replace all the
uses of $v$ that we find during the traversal.
If the stack is empty, this means that the variable is not defined at this point.
The renaming process replaces the uses of undefined variables by $\bot$ (line~3). 
We have two methods, {\sf stack.set\_use} and {\sf stack.set\_def} that build
the chains of relations between variables.
Notice that sometimes we must rename a single use inside a $\phi$-function,
as in lines~19-20 of the algorithm.
For simplicity we consider this single use as a simple
assignment when calling {\sf stack.set\_use}, as one can see in line 20.
Similarly, if we must rename a single definition inside a $\sigma$-function, then we treat it as a simple assignment, like we do in lines 15-16 of the algorithm.

\begin{figure}[t!]
\begin{small}
\begin{tabular}{rl}
$_{1}$ & \textsf{clean}(var $v$)\\
$_{2}$ & \1 \Let web = $\{ v_i | v_i \textrm{ is a version of } v \}$\\
$_{3}$ & \1 \Let defined = $\emptyset$\\
$_{4}$ & \1 \Let active = \{ $\var{inst} \ | \var{inst}$ actual instruction and $\textrm{web}\cap \var{inst}.\textrm{defs}  \neq \emptyset \}$\\
$_{5}$ & \1 \While $\exists \var{inst} \in \textrm{active}$ s.t. $\textrm{web}\cap\var{inst}.\textrm{defs} \backslash  \textrm{defined}\neq\emptyset$:\\
$_{6}$ & \1  \1 \Foreach $v_i \in \textrm{web}\cap\var{inst}.\textrm{defs} \backslash \textrm{defined}$: \\
$_{7}$ & \1     \2 active = active $\cup$ $\Uses(v_i)$ \\
$_{8}$ & \1     \2 defined = defined $\cup$ $\{ v_i \}$ \\
$_{9}$ & \1 \Let used = $\emptyset$\\
$_{10}$ & \1 \Let active = \{ $\var{inst} \ | \var{inst}$ actual instruction and $\textrm{web}\cap\var{inst}.\textrm{uses} \neq \emptyset \}$\\
$_{11}$ & \1 \While $\exists \var{inst} \in \textrm{active}$ s.t. $\var{inst}.\textrm{uses} \backslash  \textrm{used}\neq\emptyset$:\\
$_{12}$ & \1  \1 \Foreach $v_i \in \textrm{web}\cap\var{inst}.\textrm{uses} \backslash \textrm{used}$: \\
$_{13}$ & \1     \2 active = active $\cup$ $\Def(v_i)$ \\
$_{14}$ & \1     \2 used = used $\cup$ $\{ v_i \}$ \\
$_{15}$ & \1 \Let live = defined $\cap$ used\\
$_{16}$ & \1 \Foreach non actual $\var{inst} \in \Def(web)$:\\
$_{17}$ & \1  \1 \Foreach $v_i$ operand of \var{inst} s.t. $v_i \notin \textrm{live}$:\\
$_{18}$ & \1             \4 replace $v_i$ by $\bot$\\
$_{19}$ & \1  \1 \If $\var{inst}.\textrm{defs}=\{\bot\}$ or $\var{inst}.\textrm{uses}=\{\bot\}$\\
$_{20}$ & \1  \2 remove \var{inst}\\
\end{tabular}
\end{small}
\caption{\label{fig:clean} Dead and undefined code elimination. Original instructions not inserted by \textsf{split} are called \emph{actual} instruction. We let {\em inst}.defs denote the set of variable(s) defined by {\em inst}, and {\em inst}.uses denote the set of variables used by {\em inst}.}
\end{figure}

\paragraph{Dead and Undefined Code Elimination}

The algorithm in Figure~\ref{fig:clean} eliminates $\phi$-functions that define variables not actually used in the code, $\sigma$-functions that use variables not actually defined in the code, and parallel copies that either define or use variables that do not reach any actual instruction.
We mean by ``actual'' instructions, those instructions that already existed in the program before we transformed it with {\sf split}.
In line~3 we let ``web'' be the set of versions of $v$, so as to restrict the cleaning process to variable~$v$, as we see in lines~4-6 and lines~10-12.
The set ``active'' is initialized to actual instructions in line~4.
Then, during the loop in lines~5-8 we add to active $\phi$-functions, $\sigma$-functions, and copies that can reach actual definitions through use-def chains.
The corresponding version of $v$ is then marked as \emph{defined} (line~8).
The next loop, in lines 11-14 performs a similar process, this time to add to the active set instructions that can reach actual uses through def-use chains.
The corresponding version of $v$ is then marked as \emph{used} (line~14).
Each non live variable (see line~15), i.e. either undefined or dead (non used) is replaced by $\bot$ in all $\phi$, $\sigma$, or copy functions where it appears in.
This is done by lines~15-18.
Finally every useless $\phi$, $\sigma$, or copy functions are removed by lines~19-20. 
As a historical curiosity, the algorithm originally proposed by Cytron {\em et al.}'s procedure to build SSA form produced what is called {\em the minimal representation}.
Some of the $\phi$-functions in the minimal representation define variables
that are never used.
If we remove these variables, then we produce what compiler writers normally call {\em pruned SSA-form}.

\subsection{Implementing parallel copies, $\phi$ and $\sigma$-functions}
\label{sub:special}
%TODO: we need to explain how to get rid of copies in parallel with another instruction here.
Traditional instruction sets, such as x86 or PowerPC, do not provide $\phi$-functions nor $\sigma$-functions.
Thus, before producing an executable program, the compiler must implement these instructions.
Normally, $\phi$-functions and parallel copies are replaced by ordinary copy instructions, as explained in Section~\ref{sec:classical_destruction}.
The $\sigma$-functions can be implemented as single arity $\phi$-functions.
As an example, Figure~\ref{fig:sigImpl}(b) shows how we would represent the $\sigma$-functions in Figure~\ref{fig:taintAnalysis}(b).
If $l$ is a branch point with $n$ successors that would contain a $\sigma$-function $(v_1:l^1, \ldots, v_n:l^n) =\sigma(v)$, then, for each successor $l^j$ of $l$, we insert at the beginning of $l^j$ an instruction $v_j = \phi(v:l)$.
Notice that it is possible that $l^j$ already contains a $\phi$-function for $v$.
This case happens when the control flow edge $l \rightarrow l^j$ is {\em critical}.
A critical edge links a basic block with several successors to a basic block with several predecessors.
If $l^j$ already contains a $\phi$-function $v' = \phi(\ldots, v_j, \ldots)$, then we rename $v_j$ to $v$.


\begin{figure}[t!]
\centering
\includegraphics[scale=0.9]{sigImpl}
\caption{(a) getting rid of copies and $\sigma$-functions; (b) implementing $\sigma$-functions via single arity $\phi$-functions.}
\label{fig:sigImpl}
\end{figure}


\subsection{Deriving dense information from sparse analyses}
\label{sub:dense}

%TODO: this part is not clear. Is missing the fact that some analysis do not require the information at every live points. Is also missing the fact that for other analysis we do not have the mapping but we have the liveness. Actually ``require the information at every program point'' is very vague.
We can use our sparse data-flow analysis framework to solve even some data-flow problems that demand information at every program point.
Let's consider, for instance, a bit-width aware register allocator, that assigns to a variable $v$ the smallest register $r$ that is large enough to receive that variable.
In this setting, the register pressure at a program point $p$ is the sum of the bit sizes of all the variables live at $p$.
Hence, to know the number of registers at a give program point we must know the width of all the variables live at that point.
A dense analysis would give us this information immediately.
In a sparse setting, we may resort to a quick liveness check in order to find it.
In this case, it is fundamental that the intermediate program representation preserves the SSA-form properties.

Preserving the SSA properties is key for two reasons, which we have already discussed in Chapter~\ref{XXX:chap:ssa_tells_nothing_of_liveness}.
Firstly, liveness analysis has a non-iterative implementation for SSA-form programs linear on the program size.
Secondly, if we only need liveness information for some specific variables, at some specific program points, then there is a fast liveness check for
SSA-form programs.
The problem of answering the question ``is variable $v$ live at program point $p$" has an algorithm that is $O(U)$, where $U$ is the number of times that $v$ is used in the program code.

\section{Further Reading}

The original description of the intermediate program representation known as Static Single Information was given by Ananian in his Master's thesis~\cite{Ananian99}.
In particular, we borrow the notation of $\sigma$-functions, used in this chapter, from Ananian's work.
This program representation was later on revisited by Jeremy Singer in his PhD thesis~\cite{Singer06}.
Singer proposed new uses for the SSI form, and new algorithms to build it.
Finally, Boissinot {\em et al.}~\cite{Benoit09} analyzed these two previous works, seeking to clarify some misconceptions related to the process of converting a program to SSI form.
The Extended Static Single Assignment form was introduced by Bodik {\em et al.} in order to provide a fast algorithm to eliminate array bound checks in the context of a JIT compiler~\cite{Bodik00}.

The notion of {\em Partitioned Variable Problem} was introduced by Zadeck, in his PhD dissertation~\cite{Zadeck84}.
Zadeck proposed fast ways to build data-structures that allow one to solve these problems in a faster way.
He also discussed a number of data-flow analyses that fit this category.
When re-stating Zadeck's definition we mentioned that there are examples of data-flow problems that cannot be classified as Partitioned Variable Lattice.
Among these examples we cite abstract interpretation problems on relational domains, such as Polyhedrons~\cite{Cousot78}, Octagons~\cite{Mine06} and Pentagons~\cite{Logozzo08}.

The data-flow analyses discussed in this chapter are well-known in the literature.
Class inference was used by Chambers {\em et al.} in order to compile Self programs more efficiently~\cite{Chambers89}.
Nanda and Sinha have used a variant of null-pointer analysis to find which method dereferences may throw exceptions, and which may not~\cite{Nanda09}.
Ananian~\cite{Ananian99}, and later Singer~\cite{Singer06}, have showed how to use the SSI representation to do partially redundancy elimination sparsely.
In addition to being used to eliminate redundant array bound checks~\cite{Bodik00}, the e-SSA form has been used to solve Taint Analysis~\cite{Rimsa11}, and range analysis~\cite{Su05,Gawlitza09}.
Stephenson {\em et al.}~\cite{Stephenson00} described a bit-width analysis that is both forward, and backwards, taking information from definitions, uses and conditional tests.
For another example of bidirectional bitwidht analysis, see Mahlke {\em et al.}'s algorithm~\cite{Mahlke01}.
The type inference analysis that we mentioned in Figure~\ref{fig:splittingSt} was taken from Hochstadt {\em et al.}'s work~\cite{Hochstadt08}.