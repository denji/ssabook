\chapter{Hardware Compilation using SSA
\Author{P. C. Diniz \andAuthor P. Brisk}}
\label{chapter:hardware_compilation}
\inputpath[Figs]{part4}{hardware_compilation}
\inputprogress
% \makeatletter
% \show\input@path
% \makeatother
%\renewcommand{\adjustbox}[1]{#1}

This \index{hardware compilation}chapter describes the use of SSA-based high-level program representation for the realization of the corresponding computation using hardware digital circuits. 
  We begin by highlighting the benefits of using a compiler SSA-based intermediate representation in this hardware mapping process using an illustrative example. 
  The subsequent sections describe hardware translation schemes for discrete hardware logic structures or data-paths of hardware circuits. 
  In this context we also outline several compiler transformations that benefit from the SSA-based representation of a computation. 
  We conclude with a brief survey of various hardware compilation efforts both from academia as well as industry that have adopted SSA-based internal representations.


\renewcommand{\comment}[1]{}

\section{Brief history and overview}

Hardware compilation is the process by which a high-level language, or behavioral, description of a computation is translated into a hardware-based implementation i.e., a circuit expressed in a hardware design language such as VHDL\index{VHDL} or Verilog\index{Verilog} which can be directly realized as an electrical (often digital) circuit. 

Hardware-oriented languages such as VHDL or Verilog allow programmers to develop such digital circuits either by structural composition of blocks using abstractions such as wires and ports or behaviorally  by definition the input-output relations of signals in these blocks. A mix of both design approach is often found in medium to large designs. 
Using a structural description approach, a circuit description  will typically include discrete elements such as registers (Flip-Flops) that capture the state of the computation at specific events, such as clock edges, and combinatorial elements that transform the values carried by electrical wires. The composition of these elements, allows programmers to build sequential machines, such as Finite-State-Machines (FSMs) that control the processing of inputs and the data stored in internal registers often aggregated in the form or RAM blocks. The architecture of these circuits can support sequential execution of operations akin to the execution of high-level programs.

A common vehicle for the realization of hardware designs in a Field-Programmable Gate Arrays or FPGA. These devices include a large quantity of configurable logic blocks (or CLBs)
each of which can be individually programmed to realize an arbitrary combinatorial function of k-inputs  whose outputs can be latched in Flip-Flops and connected via an internal interconnection network to any subset of the CLBs in the device.  Given their  design regularity and simple structure, these devices, popularized in the 80s as fast hardware prototyping have taken advantage of Moore's law, to grow to large programmable devices with which programmer can define custom computing machines capable of TFlops/Watt performance metrics thus making them the vehicle of choice for very power efficient custom computing machines. 

While, initially, developers were forced to design hardware circuits exclusively using schematic-capture tools, high-level behavioral synthesis allowed them over the years to leverage the wealth of hardware-mapping and design exploration techniques to realize substantial productivity gains.  

As an example Figure~\ref{fig:Fig.4.1} illustrates these concepts of hardware mapping for the computation expressed as $x \gets (a \times b) - (c \times d) + f$. 
In figure~\ref{fig:Fig.4.1-b} a graphical representation of a circuit that directly implements this computation is presented. 
Here there is a direct mapping between hardware operators such as adders and multipliers and the operations in the computation. 
Input values are stored in the registers at the top of the circuit diagram and the entire computation is carried out during a single (albeit long) clock cycle, at the end of which the results propagated through the various hardware operators are captured (or latched) in the registers at the bottom of the circuit diagram. 
Overall this direct implementation uses two multipliers, two adders/subtractors and six registers, five registers to hold the computation's input values and one register to capture the computation's output result.
The execution using this hardware implementation  requires a simple control scheme, as it just needs to record the input values, wait for a single clock cycle at the end of which it stores the outputs of the operations in the output register. 
In figure~\ref{fig:Fig.4.1-c} we depict a different implementation variant of the same computation, this time using nine registers and the same amount of adders and subtractors.\footnote{It may be apparent that the original computation lacks any temporal specification in terms of the relative order in which data-independent operation can be carried out. Implementation variants exploit this property.} 
The increased number of registers allows for the circuit to be clocked at higher frequency as well as to be executed in a pipelined fashion. 
Finally, in figure~\ref{fig:Fig.4.1-d} we depict yet another possible implementation of the same computation but using a single multiplier operator. 
This last version allows for the reuse in time of the multiplier operator and required thirteen register as well as multiplexers to route the inputs to the multiplier in two distinct control steps. 
As it is apparent, the reduction of number of operators, in this particular case the multipliers carries a penalty in an increased number of registers, multiplexers~\footnote{A $2\times1$ multiplexor is a combinatorial circuit with two data inputs, a single output and a control input, where the control input selects which of the two data inputs is transmitted to the output. 
  It can be viewed as a hardware implementation of the C programming language selection operator : 
  $\textit{out} = (q ? 
    \textit{in}_1 : 
    \textit{in}_2)$.} 
and increased complexity of the control scheme.\\

\begin{figure}[htbp]
  \defineheight{%
      \tikzfigure{fig-4-1-b}
    }
  \centering
  \subfloat[source code]{\label{fig:Fig.4.1-a}
    \adjustbox{valign=m}{
      \centerheight{
      \begin{minipage}{0.3\textwidth}
        \begin{algorithm}[H]
          \Let $A:\ \textrm{std\_logic\_vector}(0\dots 7)$\;
          \dots\;
          $x\gets (a\times b)-(c\times d)+f$\;
        \end{algorithm}
      \end{minipage}
      }
    }
  }
  \subfloat[simple hardware design]{\label{fig:Fig.4.1-b}
    \adjustbox{valign=m}{
      \useheightbox{}
    }
  }
  
  \subfloat[design sharing resources]{\label{fig:Fig.4.1-c}
    \tikzfigure{fig-4-1-c}
  }
  
  \subfloat[pipelined hardware design]{\label{fig:Fig.4.1-d}
    \tikzfigure{fig-4-1-d}
  }
%\includegraphics[scale=0.5]{Fig-4-1}
\caption{Different variants of mapping a computation to hardware} 
\label{fig:Fig.4.1}
\end{figure}

This example illustrates the many degrees of freedom in high-level behavioral hardware synthesis. 
Synthesis techniques perform the classical tasks of allocation, binding and scheduling of the various operations in a computation given specific target hardware resources. 
For instance, a designer can use behavioral synthesis tools (e.g., Xilinx's Vivado$^{\textregistered}$\index{Vivado}) to automatically derive an implementation for a computation as expressed in the example in Figure~\ref{fig:Fig.4.1-a} by declaring that it pretends to use a single adder and a single multiplier automatically deriving an implementation that resembles the one depicts in figure~\ref{fig:Fig.4.1-d}. 
The tool then derives the control scheme required to route the data from registers to the selected units so as to meet the designers' goals. 

Despite the introduction of high-level behavioral synthesis techniques in commercially available tools, hardware synthesis and thus hardware compilation has never enjoyed the same level of success as traditional, software compilation. 
Sequential programming paradigms popularized by programming languages such as C/C++ and more recently by Java, allow programmers to easily reason about program behavior as a sequence of program memory state transitions. 
The underlying processors and the corresponding system-level implementations present a number of simple unified abstractions -- such as a unified memory model, a stack, and a heap that do not exist (and often do not make sense) in customized hardware designs.

Hardware compilation, in contrast, has faced numerous obstacles that have hampered its wide adoption. When developing hardware solutions, designers must understand the concept of spatial concurrency that hardware circuits offer. 
Precise timing and synchronization between distinct hardware components are key abstractions in hardware. 
Solid and robust hardware design implies a detailed understanding of the precise timing of specific operations, including I/O, that simply cannot be expressed in languages such as C, C++, or Java. Alternatives, such as SystemC\index{SystemC} have emerged in recent years, which give the programmer considerably more control over these issues. 
The inherent complexity of hardware designs has hampered the development of robust synthesis tools that can offer high-level programming abstractions enjoyed by tools that target traditional architecture and software systems, thus substantially raising the barrier of entry for hardware designers in terms of productivity and robustness of the generated hardware solutions. 
At best, today hardware compilers can only handle certain subsets of mainstream high-level languages, and at worst, are limited to purely arithmetic sequences of operations with strong restrictions on control flow.

Nevertheless, the emergence of multi-core processing has led to the introduction of new parallel programming languages and parallel programming constructs that may be more amenable to hardware compilation than traditional languages 
%and, similarly, abstractions such as a heap (e.g., pointer-based data structures) and a unified memory address space are proving to be bottlenecks with respect to effective parallel programming. 
% * <peddiniz@gmail.com> 2018-05-25T01:38:24.534Z:
% 
% This sentence is really out of place here. One this is the contained control flow athat helps hardware synthesis the other is the inability to uncover data dependences that hinder the discovery of concurrency. This sentence is misleading at best, so I would just remove it.
% 
% ^ <peddiniz@gmail.com> 2018-05-25T01:40:05.458Z.
For example, MapReduce, originally introduced by Google to spread parallel jobs across clusters of servers, has been an effective programming model for FPGAs\index{FPGA} as it naturally exposes task-level concurrency with data independence. Similarly, high-level languages based on parallel models of computation such as synchronous data flow\index{synchronous data flow}, or functional single-assignment languages\index{functional language} have also been shown to be good choices for hardware compilation as not only they made data independence obvious, but in many cases the natural data partitioning is a natural match for the spatial concurrency that FPGAs exhibit.

Although the remainder of this chapter will be limited primarily to hardware compilation for imperative high-level programming languages with an obvious focus on SSA Form, many of the emerging parallel languages while including sequential constructs, such as control-flow graphs, they also support truly concurrent constructs. These could be a natural fit to exploit the spatial and customization opportunities of FPGA-based computing architectures.
% * <peddiniz@gmail.com> 2018-05-25T01:43:04.542Z:
%
% ^.
The extension of SSA Form, and SSA-like constructs, to these emerging languages, is an open area for future research; however, the fundamental uses of SSA Form for hardware compilation, as discussed in this chapter, are likely to remain generally useful.

\section{Why use SSA for hardware compilation?}
Hardware compilation, unlike its software counterpart, offers a spatially oriented computational infrastructure that presents opportunities that can leverage information exposed by the SSA representation. 
We illustrate the direct connection between SSA representation form and hardware compilation using the mapping of a computation example in Figure~\ref{fig:Fig.4.2-a}. 
Here the value of a variable $v$ depends on the control flow of the computation as the temporary variable $t$ can be assigned different values depending on the value of the $p$ predicate. 
The representation of this computation is depicted in Figure~\ref{fig:Fig.4.2-b} where a \phifun is introduced to capture the two possible assignments to the temporary variable $t$ in both control branches of the {\tt if-then-else} construct. 
Lastly, we illustrate in Figure~\ref{fig:Fig.4.2-c} the corresponding mapping to hardware.

\begin{figure}[htbp]
  \centering
  \subfloat[original code]{\label{fig:Fig.4.2-a}
    \adjustbox{valign=b}{
      \begin{minipage}{0.3\textwidth}
        \begin{algorithm}[H]
          $p\gets \dots$\;
          \eIf{$p$}{
            $t\gets a$\;
          }{
            $t\gets b$\;
          }
          \;
          $v\gets t+\dots$\;
        \end{algorithm}
      \end{minipage}
    }
  }
  \subfloat[SSA form]{\label{fig:Fig.4.2-b}
    \adjustbox{valign=b}{
      \begin{minipage}{0.3\textwidth}
        \begin{algorithm}[H]
          $p\gets \dots$\;
          \eIf{$p$}{
            $t_1\gets a$\;
          }{
            $t_2\gets b$\;
          }
          $t_3\gets \phi(t_1,t_2)$\;
          $v\gets t_3+\dots$\;
        \end{algorithm}
      \end{minipage}
    }
  }
  \subfloat[hardware mapping]{\label{fig:Fig.4.2-c}
     \adjustbox{valign=b}{
       \tikzfigure{fig-4-2}
       }
  }
%\includegraphics[scale=0.45]{Fig-4-2}
\caption{Basic hardware mapping using SSA representation.}
\label{fig:Fig.4.2}
\end{figure}

The basic observation is that the confluence of values for a given program variable leads to the use of a \phifun. This \phifun abstraction thus corresponds in terms of hardware implementation of the insertion of a multiplexer logic circuit\index{multiplexer, logic circuit}. This logic circuit uses the Boolean value of a control input to select which of its 
input's value is to be propagated to its output. The selection or control input of 
a multiplexer thus acts as a gated transfer of value that parallels the actions of 
an {\tt if-then-else} construct in software. Notice, also that in the case of a backwards control-flow (e.g., associated with a back-edge of a loop),  the possible indefinition of one of the  \phifun's inputs is transparently ignored by the fact that in a correct execution the predicate associated with the true control-flow path will yield the value associated with a defined input of the SSA representation.

Equally important in this mapping is the notion that the computation in hardware can now take a spatial dimension.  In the hardware circuit in Figure~\ref{fig:Fig.4.2-c} the computation derived from the statement in both branches of the {\tt if-then-else} construct can be evaluated concurrently by distinct logic circuits.  After the evaluation of both circuits the multiplexer will define which set of values are used based on the value of its control input, in this case of the value of the computation associated with $p$.

In a sequential software execution environment, the predicate $p$ would be evaluated first, and then either branches of the {\tt if-then-else} construct would be evaluated, based on the value of $p$;  as long as the register allocator is able to assign $t_1$, $t_2$, and $t_3$ to the same register, then the \phifun is executed implicitly;  if not, it is executed as a register-to-register copy.

There have been some efforts that could automatically convert the sequential program above into a semi-spatial representation that could obtain some speedup if executed on a VLIW\index{VLIW} ((Very Long Instruction Word) type of processor. 
For example, if-conversion (See Chapter~\ref{chapter:if_conversion}) would convert the control dependency\index{control dependency} into a data dependency\index{data dependency}:  statements from the {\tt if}- and {\tt else} blocks could be interleaved, as long as they do not overwrite one another's values, and the proper result (the \phifun) could be selected using a conditional-move instruction. In the worst case, however, this approach would effectively require the computation of both branch sides, rather than one, so it could actually lengthen the amount of time required to resolve the computation. 
In a spatial representation, in contrast, the correct result can be output as soon as two of the three inputs to the multiplexer are known ($p$, and one of $t_1$ or $t_2$, depending on the value of $p$).

%%% need to fix sentences below.
% * <peddiniz@gmail.com> 2018-05-25T02:24:33.427Z:
%
% ^.
%When targeting a hardware platform, one advantage of the SSA representation is that assigning a value to each scalar variable exactly once makes the sequences of definitions and uses of each variable both formal and explicit. 
%A spatially-oriented infrastructure can leverage this information to perform the computation in ways that would make no sense in a traditional processor. 
%For example, in an FPGA~\footnote{Field-Programmable Gate Arrays are layers of two dimensional topology integrated circuits designed to be configurable after manufacturing}\index{FPGA}, one can use multiple registers in space and in time to hold the same variable, and even simultaneously assign distinct values to it; 
%then, based on the outcome of specific predicates in the program, the hardware implementation can select the appropriate register to hold the outcome of the computation. 
%In fact, this is precisely what was done using a multiplexer in the example above.

The computation mapping example depicted above highlights the potential benefits of a SSA-based intermediate representation, when targeting hardware architectures that can easily exploit spatial computation, namely:
\begin{itemize}
\item Exposes the data dependences\index{data dependency} in each variable computation by explicitly incorporating into the representation each variable def-use chains\index{def-use chain}. 
  This allows a compiler to isolate the specific values and thus possible registers that contribute to each of the assumed values of the variable. 
  Using separate registers for disjoint live ranges, allows hardware generation to reduce the amount of resources in multiplexer circuits, for example.
\item Exposes the potential for the sharing of hardware register not only in time and but also in space providing insights for the high-level synthesis steps of allocation, binding and scheduling.
\item Provides insight into control dependent regions where control predicates\index{predicate} and thus the corresponding circuitry is identical and can thus be shared. 
  This aspect has been so far neglected but might play an important role in the context of energy minimization. 
\end{itemize}

While the Electronic Design Automation (EDA) community had for several decades now exploited similar information regarding data and control dependences for the generation of hardware circuits from increasingly higher-level representations (e.g., Behavioral HDL), SSA-based representations make these dependences explicit in the intermediate representation itself.  Similarly, the more classical compiler representations, using three-address instructions augmented with the def-use chains already exposes the data-flow information as for the SSA-based representation. The later however, and as we will explore in the next section, facilitates the mapping and selection of hardware resources.


\section{Mapping a control-flow graph to hardware}
In this section we focus on hardware implementations or circuit that are spatial in nature.
We, therefore, do not address the mapping to architectures such as VLIW or Systolic Arrays. 
While these architectures pose interesting and challenging issues, namely scheduling and resource usage, we are more interested in exploring and highlighting the benefits of SSA representation which, we believe, are more naturally (although not exclusively) exposed in the context of spatial hardware computations.

\subsection{Basic block mapping}
As a basic block is a straight-line sequence of three-address instructions, a simple hardware mapping approach consists in composing or evaluating the operations in each instruction as a data-flow graph. 
The inputs and outputs of the instructions are transformed into registers~\footnote{As a first approach these registers are virtual and then after synthesis some of them are materialized to physical registers in a process similar to register allocation in software-oriented compilation.}  connected by nodes in the graph that represent the operators. 

As a result of the "evaluation" of the instructions in the basic block this algorithm constructs a hardware circuit that has as input registers that will hold the values of the input variables to the various instructions and will have as outputs registers that hold only variables that are live outside the basic block. 


\subsection{Basic control-flow graph mapping}
\label{sec:cfg_mapping}
One can combine the various hardware circuits corresponding to a control-flow graph in two basic approaches, respectively, {\em spatial} and {\em temporal}. 
The spatial\index{circuits, spatial combination of} form of combining the hardware circuits consists in laying out the various circuits spatially by connecting variables that are live at the output of a basic block, and therefore the output registers of the corresponding hardware circuit, to the registers that will hold the values of those same variables in subsequents hardware circuits of the basic blocks that execute in sequence.

In the temporal\index{circuits, temporal combination of} approach the hardware circuits corresponding to the various CFG basic blocks are not directly interconnected. 
Instead, their input and output registers are connected via dedicated buses to a local storage module. 
An execution controller "activates" a basic block or a set of basic blocks by transferring data between the storage and the input registers of the hardware circuits to be activated. 
Upon execution completion the controller transfers the data from the output registers of each hardware circuit to the storage module. 
These data transfers do not need necessarily to be carried out sequentially but instead can leverage the aggregation of the outputs of each hardware circuits to reduce transfer time to and from the storage module via dedicated wide buses.

\begin{figure}[htbp]
  \centering
  \subfloat[CFG]{
    \tikzfigure{Fig-4-4-a}
  }
  \subfloat[spatial mapping]{
    \tikzfigure{Fig-4-4-b}
  }

  \subfloat[temporal mapping]{
    \tikzfigure{Fig-4-4-c}
  }
%\includegraphics[scale=0.4]{Fig-4-4}
\caption{Combination of hardware circuits for multiple basic blocks.}
\label{fig:Fig.4.4}
\end{figure}

The temporal approach described above is well suited for the scenario where the target hardware architecture does not have sufficient resources to simultaneously implement the hardware circuits corresponding to all basic blocks of interest as it trades off execution time for hardware resources.

In such a scenario, where hardware resources are very limited or the hardware circuit corresponding to a set of basic blocks is exceedingly large, one could opt for partitioning a basic block or set of basic blocks into smaller blocks until the space constraints for the realization of each hardware circuit are met. 
In reality this is the common approach in every processor today. 
It limits the hardware resources to the resources required for each of the ISA instructions and schedules them in time at each step saving the state (registers) that were the output of the previous instruction. 
The computation thus proceed as described above by saving the values of the output registers of the hardware circuit corresponding to each smaller block.

These two approaches, illustrated in Figure~\ref{fig:Fig.4.4}, can obviously be merged in a hybrid implementation. 
As they lead to distinct control schemes for the orchestration of the execution of computation in hardware, their choice depends heavily on the nature and granularity of the target hardware architecture. 
For fine-grain hardware architectures such as FPGAs a spatial mapping can be favored, for coarse-grain architectures a temporal mapping is common.

While the overall execution control for the temporal mapping approach is simpler, as the transfers to and from the storage module are done upon the transfer of control between hardware circuits, a spatial mapping approach makes it more amenable to take advantage of pipelining execution techniques and speculation~\footnote{Speculation is also possible in the temporal mode by activating the inputs and execution of multiple hardware blocks and is only limited by the available storage bandwidth to restore the input context in each block which in the spatial approach is trivial.}. 
The temporal mapping approach can be, however, area-inefficient, as often only one basic block will execute at any point in time. 
This issue can, nevertheless, be mitigated by exposing additional amounts of instruction-level parallelism by merging multiple basic blocks into a single {\em hyper-block}\index{hyper-block} and combining this aggregation with loop unrolling. 
Still, as these transformations and their combination, can lead to a substantial increase of the required hardware resources, a compiler can exploit resource sharing between the hardware units corresponding to distinct basic blocks to reduce the pressure on resource requirements and thus lead to feasible hardware implementation designs. 
As these optimizations are not specific to the SSA representation, we will not discuss them further here. 

\subsection{Control-flow graph mapping using SSA}
\label{sec:cfg_ssa_mapping}
In the case of the spatial mapping approach, the SSA form plays an important role in the minimization of multiplexers and thus in the simplification of the corresponding data-path logic and execution control.

Consider the illustrative example in Figure~\ref{fig:Fig.4.5-a}. 
Here basic block {\tt BB0} defines a value for the variables $x$ and $y$. 
One of the two subsequent basic blocks {\tt BB1} redefines the value of $x$ whereas the other basic block {\tt BB2} only reads them.

A naive implementation based exclusively on liveness analysis (see Chapter~\ref{chapter:ssa_tells_nothing_of_liveness}) would use for both variables $x$ and $y$ multiplexers\index{multiplexer} to merge their values as inputs to the hardware circuit implementing basic block {\tt BB3} as depicted in Figure~\ref{fig:Fig.4.5-b}. 
As can be observed, however, the SSA-form representation captures the fact that such a multiplexer is only required for variable $x$. 
The value for the variable $y$ can be propagated either from the output value in the hardware circuit for basic block {\tt BB0} (as shown in Figure~\ref{fig:Fig.4.5-c}) or from any other register that has a valid copy of the $y$ variable. 
The direct flow of the single definition point to all its uses, across the hardware circuits corresponding to the various basic blocks in the SSA form thus allows a compiler to use the minimal number of multiplexer strictly required\footnote{Under the scenarios of a spatial mapping and with the common disclaimers about static control-flow analysis.}.

\begin{figure}[htbp]
\centering
  \subfloat[CFG]{\label{fig:Fig.4.5-a}
    \tikzfigure{Fig-4-5-a}
  }
  
  \subfloat[naive multiplexer placement using liveness]{\label{fig:Fig.4.5-b}
    \tikzfigure{Fig-4-5-b}
  }
  \subfloat[multiplexer placement using \phifun]{\label{fig:Fig.4.5-c}
    \tikzfigure{Fig-4-5-c}
  }
%\includegraphics[scale=0.4]{Fig-4-5}
\caption{Mapping of variable values across hardware circuit using spatial mapping.}
\label{fig:Fig.4.5}
\end{figure}

An important aspect regarding the implementation of a multiplexer associated with a \phifun is the definition and evaluation of the predicate associated with each multiplexer's control (or selection) input signal. 
In the basic SSA representation the selection predicates are not explicitly defined, as the execution of each \phifun is implicit when the control flow reaches it. 
When mapping a computation to hardware, however, a \phifun clearly elicits the need to  to define a predicate to be included as part of the hardware logic circuit that defines the value of the multiplexer circuit's selection input signal.  
To this effect, hardware mapping must rely on a variant of SSA, named Gated-SSA\index{gated SSA}  (see Chapter~\ref{chapter:vsdg}), which explicitly captures the symbolic predicate information in the representation.\footnote{As with any SSA representation,   variable names  fulfills the referential transparency\index{referential transparency}. }
The generation of the hardware circuit simply uses the register that holds the corresponding variable's version value of the predicate.
% the sentence below is repeating the notion of referential transparency...
%, as opposed to the base name of the variable as this might have been assigned a value that does not correspond to the actual value used in the gate. 
Figure~\ref{fig:Fig.4.6} illustrates an example of a mapping using the information provided by the Gated-SSA form.

\begin{figure}[htbp]
  \centering
  \adjustbox{valign=b}{
  \begin{minipage}{0.3\textwidth}
  \subfloat[original code]{
    \begin{algorithm}[H]
      \eIf{$x>0$}{
        $x\gets \dots$\;}{
        $x\gets \dots$\;}
      $v\gets x+\dots$
  \end{algorithm}}
  
  \subfloat[Gated-SSA form]{
    \begin{algorithm}[H]
      $p_0\gets (x_0>0)$\;
      \eIf{$p_0$}{
        $x_1\gets \dots$\;}{
        $x_2\gets \dots$\;}
      $x_3\gets \phiif(p, x_1, x_2)$\;
      $v_1\gets x_3+\dots$
  \end{algorithm}}
  \end{minipage}}
  \hfill
  \adjustbox{valign=b}{
  \subfloat[Hardware circuit implementation using spatial mapping]{
    \hspace{4em}\tikzfigure{Fig-4-6}
  }}
%\includegraphics[scale=0.375]{Fig-4-6}
\caption{Hardware generation example using Gated-SSA form.}
\label{fig:Fig.4.6}
\end{figure}
When combining multiple predicates in the Gated-SSA form, it is often desirable to leverage the control-flow representation in the form of the Program Dependence Graph\index{program dependence graph} (PDG) described in Chapter~\ref{chapter:vsdg}. 
In the PDG representation, basic blocks that share common execution predicates (i.e., both execute under the same predicate conditions) are linked to the same {\em region} nodes\index{region node}. 
Nested execution conditions are easily recognized as the corresponding nodes are hierarchically organized in the PDG representation. 
As such, when generating code for a given basic block, an algorithm will examine the various region nodes associated with a given basic block and compose (using AND operators) the outputs of the logic circuits that implement the predicates associated with these nodes. 
If an hardware circuit already exists that evaluates a given predicate that corresponds to a given region, the implementation can simply reuses its output signal.  
This lazy code generation and predicate composition achieves the goal of hardware circuit sharing as illustrated by the example in Figure~\ref{fig:Fig.4.7} where some of the details were omitted for simplicity.
When using the PDG representation, however, care must be taken regarding the potential lack of referential transparency. To this effect, it is often desirable to combine the SSA information with the PDG's regions to ensure correct reuse of the hardware that evaluates the predicates associated with each control-dependence region.

\begin{figure}[htb]
  \centering
  \adjustbox{valign=b}{
    \begin{minipage}{0.3\textwidth}
      \subfloat[source code]{
        \begin{algorithm}[H]
          \eIf{$P(x,y)$}{
            $\dots$\;
            \eIf{$Q(x,y)$}{
              $\dots$\;
            }{$\dots$}
          }{$\dots$\;}
        \end{algorithm}
      }
      
      \subfloat[PDG skeleton. Region nodes omitted.]{
        \tikzfigure{Fig-4-7-b}
      }
    \end{minipage}
  }
  \adjustbox{valign=b}{
    \subfloat[hardware design implementation]{
      \tikzfigure{Fig-4-7-c}
    }
  }
%\includegraphics[scale=0.35]{Fig-4-7}
\caption{Use of the predicates in region nodes of the PDG for mapping into the multiplexers associated with each \phifun.}
\label{fig:Fig.4.7}
\end{figure}


\subsection{$\phi$-function and multiplexer optimizations}
We now describe a set of hardware-oriented transformations that can be applied to possibly reduce the amount of hardware resources devoted to multiplexer implementation or to use multiplexers change the temporal features of the execution and thus enable other aspects of hardware execution to be more effective (e.g., scheduling).
Although these transformations are not specific to the mapping of computations to hardware, the explicit representation of the selection constructs in SSA makes it very natural to map and therefore manipulate/transform the resulting hardware circuit using multiplexer. 
Other operations in the intermediate representation (e.g., predicated instructions) can also yield multiplexers in hardware without the explicit use of SSA Form.

A first transformation is motivated by a well-known result in computer arithmetic: 
integer addition scales with the number of operands. 
Building a large unified k-input integer addition circuit is more efficient than adding k integers two at a time. 
Moreover, hardware multipliers naturally contain multi-operand adders as building blocks: 
a partial product generator (a layer of AND gates) is followed by a multi-operand adder called a {\em partial product reduction tree}\index{partial product reduction tree}. 
For these reasons, there have been several efforts in recent years to apply high-level algebraic transformations to source code with the goal of merging multiple addition operations with partial product reduction trees of multiplication operations. 
The basic flavor of these transformations is to push the addition operators toward the outputs of a data-flow graph, so that they can be merged at the bottom. 
Example of these transformations that use multiplexers are depicted in Figure~\ref{fig:Fig.4.8}(a,b). 
In the case of Figure~\ref{fig:Fig.4.8}(a) the transformation leads to the fact that an addition is always executed unlike in the original hardware design. This can lead to more predictable timing or more uniform power draw signatures\footnote{An important issue in security-related aspects of the execution.}.
Figure~\ref{fig:Fig.4.8}(c) depicts a similar transformation that merges two multiplexers sharing a common input, while exploiting the commutative property of the addition operator. 
The SSA-based representation facilitates these transformations as it explicitly indicates which values (by tracing backwards in the representation) are involved in the computation of the corresponding values. 
For the example in Figure~\ref{fig:Fig.4.8}(b) a compiler could quickly detect the variable {\tt a} to be common  to the two expressions associated with the \phifun.

A second transformation that can be applied to multiplexers is specific to FPGA whose basic building block consists of a k-input lookup table (LUT)\index{lookup table} logic element which can be programmed to implement any k-input logic function \footnote{A typical k-input LUT will include an arbitrary combinatorial functional block of those k inputs followed by an optional register element (e.g., \index{XilinxLUT} )}.
For example, a 3-input LUT (3-LUT) can be programmed to implement a multiplexer with two data inputs and one selection bit.  Similarly, a 6-LUT can be programmed to implement a multiplexer with four data inputs and two selection bits, thus enable the implementation of a tree of 2-input multiplexers.\\
% I understand what you are saying but it is very confusing.... I summarized it above.
%In particular, many FPGAs devices are organized using 4-LUTs, which are too small to implement a multiplexer with four data inputs, but leave one input unused when implementing multiplexers with two data inputs. 
%These features can be explored to reduce the number of 4-LUTs required to implement a tree of multiplexers.\\

\begin{figure}[thbp]
\centering
% \includegraphics[scale=0.5]{Fig-4-8}
  \adjustbox{valign=b}{
    \subfloat[]{
      \tikzfigure{Fig-4-8-a}
    }
  }
  \adjustbox{valign=b}{
    \subfloat[]{
      \tikzfigure{Fig-4-8-b}
    }
  }
  \adjustbox{valign=b}{
    \subfloat[]{
      \tikzfigure{Fig-4-8-c}
    }
  }
\caption{Multiplexer-Operator transformations: 
  juxtaposition of a multiplexer and an adder (a,b); 
  reducing the number of multiplexers placed on the input of an adder (c).}
\label{fig:Fig.4.8}
\end{figure}

\subsection{Implications of using SSA-form in floor-planing}
For spatial oriented hardware circuits, moving a \phifun from one basic block to another can alter the length of the wires that are required to transmit data from the hardware circuits corresponding to the various basic blocks. 
As the boundaries of basic blocks are natural synchronization points, where values are captured in hardware registers, the length of wires dictate the maximum allowed hardware clock rate for synchronous designs. 
We illustrate this effect via an example as depicted in Figure~\ref{fig:Fig.4.9}. 
In this figure each basic block is mapped to a distinct hardware unit, whose spatial implementation is approximated by a rectangle. 
A floor-planning\index{floor-planing} algorithm must place each of the units in a two-dimensional plane while ensuring that no two units overlap. 
As can be seen in Figure~\ref{fig:Fig.4.9}(a) placing the block 5 on the right-hand-side of the plane will results in several mid-range and one long-range wire connections. 
However, placing block 5 at the center of the design will virtually eliminate all mid-range connections as all connections corresponding to the transmission of the values for variable $x$ are now next-neighboring connections.

\begin{figure}[thbp]
\centering
\includegraphics[scale=0.4]{Fig-4-9}
\caption{Example of the impact of \phifun movement in
reducing hardware wire length.}
\label{fig:Fig.4.9}
\end{figure}

As illustrated by this example, moving a multiplexer from one hardware unit to another can significantly change the dimensions of the resulting unit, which is not under the control of the compiler. 
Changing the dimensions of the hardware units fundamentally changes the placement of modules, so it is very difficult to predict whether moving a \phifun will actually be beneficial. 
For this reason, compiler optimizations that attempt to improve the physical layout must be performed using a feedback loop so that the results of the lower-level CAD tools that produce the layout can be reported back to the compiler.

%\section{Further readings}
\section{Existing SSA-based hardware compilation efforts}
Several research projects have relied on SSA-based intermediate representation 
that leverage control- and data-flow information to exploit fine grain parallelism. 
Often, but not always, these efforts have been geared towards mapping computations 
to fine-grain hardware structure such as the ones offered by FPGAs.

The standard approach followed in these compilers has been to translate a high-level programming language such as Java in the case of the Sea Cucumber~\cite{Tripp:FPL02} compiler or C in the case of the DEFACTO [14] and ROCCC~\cite{Najjar:ROCCC08} compilers that translate C code to sequences of intermediate instructions. These sequences are then organized in basic blocks that compose the control-flow graph (CFG). For each basic block a data-flow graph (DFG) is typically extracted followed by conversion in to SSA representation, possibly using predicates associated with the control flow in the CFG thus explicitly using Predicated SSA representation (see the Gated-SSA representation\cite{Tu-SC95} and the Predicate SSA~\cite{Carter:PACT99,deFerriere:SCOPES07,Stoutchinin:2001:MICRO}).

% reference [14] above.
%[1]4] P. Diniz et al., "DEFACTO Compilation and Synthesis System", Elsevier Journal on Microprocessors and Microsystems, 29(2):51-62, (2005).

As an approach to increase the potential amount of exploitable instruction-level parallelism (ILP) at the instruction level, many of these efforts (as well as others such as the earlier Garp compiler~\cite{Callahan:Computer00}) restructure the CFG into hyper-blocks~\cite{Mahlke:Micro92}.  An hyper-block consists on a single-entry multi-exit regions derived from the aggregation of multiple basic blocks thus serializing longer sequences of instruction. 
As not all instructions are executed in a hyper-block (due to early exit of a block), hardware circuit implementation must rely on predication to exploit the potential for additional ILP. 

The CASH compiler~\cite{Budiu:FPL02} uses an augmented predicated SSA representation with tokens to explicitly express synchronization and handle {\em may-dependences} thus supporting speculative execution.  This fine-grain synchronization mechanism is also used to serialize the execution of consecutive hyper-blocks, thus greatly simplifying the code generation. Other efforts, also exploit instruction-level optimizations or algebraic properties of the operators for minimization of expensive hardware resources such as multipliers, adders and in some cases even multiplexers~\cite{Verma08,Nancekievill05}. For a comprehensive description of a wide variety of hardware-oriented high-level program transformations the reader is refereed to [19].

\paragraph{Further Readings}
Despite their promise in terms of high-performance and high computational efficiency  hardware devices such as FPGAs have long been beyond the reach of the \"average\" software programmer. To effectively program them using hardware-oriented programming languages such as  VHDL~\cite{VHDLBook}, Verilog~\cite{VerilogBook} or SystemC~\cite{SystemC:ISSS01} developers must assume the role of both software and hardware designs. 

To address the semantic gap between a hardware-oriented programming model and high-level software programming models various research projects, first in academia and later in industry developed prototype tools that could bridge this gap and make the promising technology of  configurable logic approachable to a wider audience of programmers.  In these efforts, loosely labeled as C-to-Gates, compiler carry out the traditional phases of program data- and control-dependence analysis to uncover opportunities for concurrent and/or pipelined execution and directly translated the underlying data-flow to Verilog/VHDL description alongside the corresponding control logic.   In practice, these compilers, of which Vivado HLS [16] and LegUp [15] are its most notable examples, focused on loop constructs with significant execution time weight (the so called “hot-spots”) are derive hardware pipelines (often guided by user-provided compilation directives) that executed them efficiently in hardware. When the target architecture is a “raw” FPGA (rather than an overlay architecture) this approach invariably incurs in long compilation and synthesis times.

The inherent difficulties and limitations in extracting enough Instruction-Level Parallelism in these approach couple of the increase of the devices' capacities (e.g. Intel'a Arria [17] and Xilinx's Virtex UltraScale+[18], has prompted the search for programming models with a more natural concurrency that would facilitate the mapping of high-level computation to hardware. One such example, is the MapReduce, originally introduced by Google to distributed naturally concurrent  
jobs across clusters of servers~\cite{Dean:CACM08}, has been an effective 
programming model for FPGAs~\cite{Yeung:FCCM08}. Similarly, high-level 
languages based on parallel models of computation such as synchronous data 
flow~\cite{Lee:ProcIEEE87}, or functional single-assignment languages have 
also been shown to be good choices for hardware  compilation~\cite{Hormati:CASES08,Hagiescu:DAC09,SAC:IJS02}.

% References for the paragraph above.
 % [15] LegUp: High-Level Synthesis for FPGA-Based Processor/Accelerator Systems FPGA’11, February 27–March 1, 2011, Monterey, Calif ornia, USA. 
 % [16] Xilinx Inc., “Vivado High-Level Synthesis,” http://www.xilinx.com/products/design- tools/vivado/integration/esl-design/index    .htm, [Accessed: June 2014].
 %[17]  Intel® Arria10® Device Overview, March 15, 2017 (https://www.altera.com/products/fpga/arria-series/arria-10/features.html)
% [18] Xilinx Inc., Virtex UltraScale+ FPGA devices (https://www.xilinx.com/products/silicon-devices/fpga/virtex-ultrascale-plus.html)
%[19] João M.P. Cardoso e Pedro C. Diniz, “Compilation Techniques for Reconfigurable Architectures”, Springer, 2008, ISBN: 978-0-387-09670-4.

