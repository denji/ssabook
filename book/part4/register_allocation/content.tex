% vim:spell:spelllang=en
\chapter{Register Allocation\Author{F. Bouchez, S. Hack}}
\label{chap:register-allocation}

{

%% Local macros for personal use
\newcount\todocount \todocount=0
\def\todo#1{\global\advance\todocount by 1 {\color{blue} {\bf TODO:} #1}}
\newlinechar=`\^^J
\def\endofchapter{\ifnum\todocount>0\immediate\write16{^^JLaTeX Warning: There 
was still TODO macros: \the\todocount.^^J}\fi}


\def\ac#1{#1}
\def\dom{\preceq}
\def\ssa{SSA\xspace}
\def\maxlive{Maxlive\xspace}
\def\irc{Iterated Register Coalescing\xspace}


\chapterauthor{Bouchez, Hack}

{\sl
We will start by giving an overview and intuitive understanding why SSA helps 
register allocation. Then we present the sub-problems of spilling and 
coloring/coalescing, as two separate phases, under the light of SSA. Finally, 
practical issues like register constraints and critical edge splitting are 
discussed.

%% Intro by Hack
%We will start by giving an overview and intuitive understanding why SSA helps register allocation.
%Then, we present the principal architecture of an SSA-based register allocator.

%We discuss spilling, assignment, and coalescing in further detail and review the proposed techniques.
%Finally, we present the corresponding complexity results for each phase.
}


\todo{Longer introduction}

The goal of register allocation is to map the variables of a program to 
physical memory locations. The compiler must indeed decide, in advance, in 
which locations will be held the values necessary for the computations of the 
program, and so for each instruction of the program. Registers are a very fast 
memory, hence are preferred for holding these values directly needed by the 
CPU. But there is a limited, small number of registers available in a 
processor, for instance only 8 registers for the IA-32 architecture (X86, 32 
bits), or 64 for the ST200, a Very Long Instruction Word (VLIW) processor.  On 
the other hand, in the initial program representation and until very late in 
the compiler back-end, values are stored in variables or temporaries, which are 
unbounded in number.


\todo{Explain reg-alloc = coloring, i.e., color = register.}


\section{Why does SSA help for register allocation?}


\subsection{Classical register allocation}

{\sl
\begin{itemize}
  \item based on graph coloring, linear scan
  \item spilling \& coloring are dependent
  \item heuristics are used to find a working solution
  \item avoiding spills is more important than coalescing
  \item Fab: It is important to outline that with decoupled reg-alloc, spilling and coloring become much simpler and more efficient.
\end{itemize}
}

Traditionally, register allocation schemes come in two flavors: based on 
graph-coloring, for better results but more time and memory consuming, suitable 
for off-line compilation; and based on ``linear scans,'' for easiness of the 
scheme and quick compilation, suitable for Just-in-Time (JIT) compilation.  The 
goal of any register allocation algorithm is to make the best use of registers 
possible, i.e., minimizing the \emph{spilling} (storing variables to memory), and to 
reduce data movements between registers by performing \emph{coalescing} (putting 
variables linked by copy instructions in the same register).

\subsubsection{Spilling \& coalescing inter-dependence.}
For all modern architectures, reducing the spilling is more important that 
maximizing the coalescing, since loading or storing a variable in memory takes 
more cycles than performing a register-to-register {\tt move} instruction. 
However, the two optimisations are often performed at the same time (\todo{true 
for linear-scan too?}) because they both depends on whether there is enough 
registers for the variables or not. For instance, in traditional graph-coloring 
register allocation, spilling allows to remove some node of the interference 
graph, hopefully reducing the number of colors required. Conjointly, coalescing 
two variables merges the corresponding nodes, which can have positive effects
or negative effects: on the one hand, it will remove a copy in the program and 
diminishes the degree of the nodes that are common neighbours, hence augmenting 
their chances to be simplified (\todo{define simplification, or link to iterated 
register coalescing?}), i.e., increasing the chances that the graph becomes 
colorable; on the other hand, the created node will have more neighbours, hence 
diminishing its chances to be simplified and augmenting its chances of being 
spilled, in which case two variables are being spilled (the coalesced ones) 
instead of just one, or even maybe none. 

Since there is no way to know the minimum number of colors required to color a 
general graph, coalescing strategies are usually \emph{conservative}, to avoid 
transforming a $k$-colorable graph into a non $k$-colorable one. This means 
opportunities to improve the spilling and the coalescing are missed.

\subsubsection{Decoupling register allocation.}
We will see that, under SSA form, we have a polynomial algorithm that can tell 
whether $R$ registers are sufficient or not. This allows to decouple register 
allocation into two phases: the first phase spills variables until we know that 
$R$ registers are sufficient, at which point no coalescing can help reducing 
the coloring number any further; then, the remaining variables are assigned to 
registers while performing as much coalescing as possible as long as it does 
not induce more spill.

This for one allows for much simpler register allocation schemes, as each phase 
must be handled separately, which relieves the compiler of the worries of 
coalescing affecting the spilling. As such, each phase can be optimized more 
easily and the resulting register allocation phase can be more efficient.
\todo{finish explanations}





\subsection{Programs under SSA have chordal interference graphs}
{\sl 
\begin{itemize}
  \item Reminder that programs under SSA are chordal
  \item What difference does it make? Knowing the minimum number of colors is 
    easy: we know if spilling is mandatory or not
  \item why are the graphs chordal? View of SSA variables as subtrees of the 
    dominance tree; made possible because of \phifuns
  \item \phifuns allows colors to be ``re-arranged'' on incoming edges: this 
    is implicit shuffle code. For this to work, remember that semantics of 
    \phifun is they are \emph{parallel copies}. Hence, the SSA-variables of 
    same initial variable in different branches of dominance tree are 
    color-independent.

  \item Fab: the chordality and the parallel semantic of phi have already been considered in the first chapters of parts I (which does not mean that you cannot recall it).

\end{itemize}
}

It is known since 2005 that, under SSA, the interference graph of a program is 
chordal \cite{Brisk05,Pereira05:chordal,Hack06}.  Note that the interference 
graph depends on the interference notion, and we need that two variables alive 
at the same time interfere even if they have the same value.\footnote{Note that, under 
SSA, there is only one definition, hence, if variable $b$ is defined as $b\gets 
a$, it is possible to replace every occurrence of $b$ by $a$ since they will 
always be equal.
%Note that this is akin to aggressive coalescing but is safe to 
%do under  \ssa as it will not increase the coloring number.  Constant 
%propagation can do this efficiently in a first pass, then it is not worth 
%considering the interference definition of Chaitin 
%(Definition~\ref{def:chaitin-interf}) as there is no remaining copy in the 
%program.
}
This chordal property was already mentioned in 
Chapter~\ref{chap:properties-and-flavours}. We recall here the theorem since it 
is a vital property for register allocation:

\begin{theorem}
  \label{thm:ssa-chordal}
  The interference graph~$G$ of a program under \ac{SSA} with dominance 
  property is chordal.
\end{theorem}

The major impact for register allocation is that, while coloring a general graph 
with the minimum number of colors in NP-complete, it is polynomial in the case 
of chordal graphs. Since register allocation amounts to coloring the 
interference graph of a program, this means that, under SSA, we can have an
algorithm that answers in polynomial time whether $R$ registers are sufficient 
or not to allocate all variables, i.e., \emph{whether spilling is mandatory 
or not.} This is a fundamental breakthrough as, up to know, register 
allocation algorithms all depend on coloring \emph{heuristics} to decide 
whether to spill or not.




\subsubsection{Why so chordal?}

The key observation leading to this theorem was that the dominance graph of a 
program under SSA is a tree, and since every definition of a variable dominates  
its uses, the live-ranges of variables are subtrees of the dominance tree. This 
is a characterisation of chordal graphs (see \cite[Thm.~4.8]{Golumbic} and 
Appendix~\ref{app:chordal-graphs}).


Since Chaitin et al.'s NP-completeness proof on register allocation in 
1981~\cite{Chaitin81}, we know that the interference graphs of programs are 
general, i.e., any graph can be the interference graph of a program. The 
question is: Why, under SSA form, the interference graph of a program is not 
general anymore but instead becomes chordal? The answer of course lies in the 
element that modify the program structure: the \phifuns.

To visualize how the \phifuns transform a general graph into a chordal graph, 
let us take the dominance tree of a program under SSA. Before going to SSA, the 
live-ranges of at least some variables are not subtrees of this tree,
spanning across multiple branches and not following only paths on 
the tree. These other paths are in fact all control-flow edges leading to basic 
blocks with multiple predecessors (hence, not ``dominance edges''). This means 
that when going to SSA, every variable crossing these edges (and being 
different on other incoming edges) will be ``cut'' there by the insertion of a 
\phifun. The parallel semantics of \phifuns means indeed that these variables 
are now ``split'' into at least two parts: one before the control-flow edge and 
one after. These two live-ranges can be allocated to different registers and do 
not ``cross'' the control-flow edge anymore. At runtime, this means that when 
removing the \phifun, shuffle code will be inserted on the edge to move 
variables that have sub-parts colored differently on their right registers.



\subsection{Maxlive and colorability of graphs}

{\sl
\begin{itemize}
  \item definition of Maxlive
  \item show that Maxlive is the minimum number of colors required
  \item scan of the dominance tree sufficient to know Maxlive: pseudo-code
  \item Fab: MAXLIVE minimum if interference graph==intersection graph. Clearly we do not want to go into such details but be careful to use precise and correct terms.
\end{itemize}
}
    
The register pressure at a program point is the number of variables alive at 
this point.\footnote{Considering all simultaneously alive variables interfere.}
The greatest register pressure over all a program is called \maxlive, 
since it is the maximum number of simultaneously alive variables. Obviously, 
spilling is mandatory if \maxlive is strictly greater than $R$, the number of 
registers.
Under SSA, the good news is that this is the only case where spilling is 
required, i.e., if \maxlive $\leq R$, there is no need for spilling. Indeed, 
\maxlive is the coloring number of the chordal interference graph of the 
program (\todo{ref to chapter 2?}). So, we can devise a polynomial test to know 
whether spilling is necessary or not by computing \maxlive. This can be done by 
checking on every program point the number of variables that are alive. If 
liveness information is available on the whole program, the order does not 
matter. However, in the case of SSA, this is not required as \maxlive can 
easily be computed without liveness information by performing a scan of the 
dominance tree, starting from the root. We only need to know which uses are 
``last-uses.'' The pseudo-code is given on Figure~\ref{code:compute-maxlive}.
\todo{check \maxlive scan needs last uses}.


\begin{figure}[ht]
  \begin{verbatim}
  function compute-branch (p, live)
    live <- live - { uses in p that are last uses }
    live <- live + { definitions of p }
    maxlive <- max( maxlive, #live )
    foreach child c of p
      compute-branch (c, live)

  function compute-maxlive
    maxlive <- 0
    compute-branch (root, {})
  \end{verbatim}
  \caption{Pseudo-code for \maxlive}
  \label{code:compute-maxlive}
\end{figure}




\section{The problem of spilling}
\subsection{Lowering Maxlive}

{\sl
\begin{itemize}
  \item The min number of colors is Maxlive. Variables must be stored in memory 
    to lower the register pressure at points where it is $>R$.
  \item Whenever, Maxlive $\leq R$, we know for sure that spilling more in 
    unnecessary.
  \item Fab: if MAXLIVE$\leq R$ spilling is unecessary under some conditions (we color SSA code, going out of SSA code might require some additionnal spilling. Also without naming constraints). So again be careful about what you say. I think that the idea is to have a register allocator under SSA as simple as possible with a very simple modeling and nice properties. Then we cope with real world issues during the colored-SSA destruction.
\end{itemize}
}

Usually, there are too many variables in a program and \maxlive is greater than 
$R$. In that case, we need to lower the register pressure at program points 
where it exceeds $R$ by spilling, i.e., storing variables in memory instead of 
registers. Whenever we reach a point where \maxlive $\leq R$, it will be 
possible to color all SSA variables with the $R$ registers. For now, the 
difficulty lies in choosing which variables to spill, and where to spill them. 
Indeed, a careful choice will lead to performance, while making bad decisions 
can lead to big slowdowns. As accesses to variables in memory require more time 
that accesses to registers, it is for instance better to avoid spilling 
variables used inside frequently executed code regions like loops.

\todo{spilling while keeping SSA property}


\subsection{Spilling is a difficult problem}

The problem of spilling is a difficult one in the literature, even under one of 
its simpler forms, the spill everywhere problem, where spilled variables stay 
so on their entire live-range~\cite{todo}. This spilling is advocated on the 
first graph-coloring algorithms~\cite{Chaitin81} and even in subsequent 
algorithms~\cite{IRC} because it fits better the graph-coloring representation. 
In these schemes, this amounts to removing nodes from a non $R$-colorable graph 
until it becomes $R$-colorable. Although this node deletion problem is known to 
be NP-complete, this allows the writing of simple heuristics based on the 
number of neighbours in the graph and a cost attached to nodes representing the 
overhead incurred in case they were to be spilled. An added problem is the fact 
that, for many architectures like RISC, spilled variables cannot be accessed 
directly from memory but must instead by copied back to the registers, when 
they are used, or which must be in a register when defined, just before being 
copied to memory. This creates new variables with very short live-ranges at the 
definition and use points, which must be accounted for register allocation. 
This explains why algorithms such as \irc must rebuild the interference graph 
after a phase of spilling and start again, until no more spilling is required.


The actual spilling problem is even more difficult, as variables do not have to 
be spilled on their entire live-range.
% borrowed from sebastian's CC'09 paper
Consider a loop with excessive register pressure and a variable that is defined 
before the loop and used afterwards. Ideally, a compiler would store (spill) 
the variable in front of the loop and load (reload) the variable after the 
loop. If the variable was reloaded inside the loop, the reload would be 
executed in each loop iteration. Another example is a variable that is used in 
a loop but has already been spilled before the loop. Reloading this variable 
directly before its use in the loop will cause memory traffic in each loop 
iteration. Thus, it is preferable to put the reload in front of the loop.
% end of borrowing
This poses a new problem on top of choosing which variables to spill: now we 
need also to choose where to spill them, i.e., where to put {\tt load} and {\tt 
store} instructions. This more general problem where the goal is to minimize 
the overhead of these added instructions is called the load-store optimization 
problem and is known to be NP-complete, even for a 
basic-block~\cite{Liberatore00}.  


\subsection{SSA does not help for spilling}
{\sl
\begin{itemize}
  \item easy spill method: spill everywhere
  \item SSA split points are good for coloring, but not really for spilling 
    (see article spill everywhere under SSA)
  \item other split points to avoid spill everywhere are probably better
  \item Fab: if you need an example to illustrate that SSA does not really help 
    in practice for spilling ask Quentin. It does help a little bit (dynamic 
    programming possible for spill everywhere with few registers; further first 
    can be generalized). But what you want to point it here is the fact that 
    the disadvantages seem more important than advantages.
  \item Fab: the LCTES spill everywhere article does not discuss practical issues (such as the fact that it might insert a load or a store inside a loop...). It is more algorithmical issues.
\end{itemize}
}

For the spilling problem, the SSA is not really of much help. Over a blind 
spill everywhere algorithm on the initial program, it has however the slight 
advantage of allowing the spilling of SSA variables, i.e., sub-variables of the 
initial variables. One major disadvantage is the added complexity when spilling
variables defined in \phifuns \todo{explain the problem if not all args are 
spilled, or if spilled in $\neq$ locations}.

{\sl Examples by Quentin}
\begin{minipage}{.45\textwidth}
\begin{verbatim}
CSSA :
   a =
   b =
 /       \
c= a   d = b
 \       /
 e = phi(c,d)
 = e
\end{verbatim}
\end{minipage}
\begin{minipage}{.45\textwidth}
After spilling spill :
\begin{verbatim}
   a =
   b =
 /          \
c= a     d = b
C=st c  D = st d
 \          /
 E = phi(C,D)
 e = ld E
 = e
\end{verbatim}
\end{minipage}
In that case, memory locations C, D and E can be the same. But this does not 
mean that placement of {\tt store} instruction is the best one.

\begin{minipage}{.45\textwidth}
\begin{verbatim}
SSA :
   a =
   b =
 /         \
 \         /
 e = phi(a,b)
 = e
\end{verbatim}
\end{minipage}
\begin{minipage}{.45\textwidth}
Spill supposing {\tt store}s are after definition
\begin{verbatim}
   a =
   A = st a
   b =
   B = st b
 /         \
 \ <--     / <--
 E = phi(A,B)
 e = ld E
 = e
\end{verbatim}
\end{minipage}

On the two edges pointed by arrows (or on the preceding basic block if they are 
not critical), additional memory transactions must reconcile the values:
\begin{verbatim}
tmp = ld X
Y = st tmp
\end{verbatim}

This can induce more spill, unforeseen.
Without SSA, this problem does not exist: for instance, with spill everywhere, 
the same memory slot is affected to all definitions of the same variable so no 
post-treatment is required as it is under SSA.



\subsection{Spilling under SSA}
Still, we give a possibility to perform spilling under SSA
\begin{itemize}
  \item Hack's algorithm: simplified version?
  \item \`a la Belady following dominance tree?

  \item Fab: The last Algorithm from Sebastian is Belady like. So yes try to simplify it, provide the pseudo code and give the intuitions on how the full version improves it.

\end{itemize}
\todo{Ask Sebastian to do it.}


\cite{Braun09:spilling-ssa}\todo{file braun09cc.pdf in biblio}


\section{Coloring and coalescing}
\subsection{Chordal property for coloring}

{\sl
\begin{itemize}
  \item two simplicial nodes
  \item color in the reverse order of simplification
  \item one such order considers subtrees from the root of the tree
  \item no need to construct interference graph
  \item Fab: make it as more intuitive as possible. Avoid as much as possible 
    chordal stuffs which are useless. Greedy coloring and its relationship with 
    the subtree representation is important on the other hand. See with Philip 
    to avoid redundancies between your chapter and his (SSA properties). It is 
    important to outline here that this property that is due to decoupled+SSA 
    enables more aggressive coalescing heuristics. But also simpler 
    (tree-scan).
\end{itemize}
}

\subsubsection{The greedy coloring scheme.}

In traditional graph coloring register allocation algorithms, the assignment is 
done by coloring the interference graph using a greedy heuristic. This greedy 
scheme is based on the observation that, given $R$ registers (colors), if a 
node in the graph has at most $R-1$ neighbours, there will always be one color 
available for this node for any coloring of the remaining of the graph. Hence 
this node can be \emph{simplified}, i.e., removed from the graph and placed on 
a stack. This process can be iterated with the remaining nodes, and if the 
graph becomes empty, we know it is possible to color the graph with $R$ colors. 
Such a coloring can be obtained by assigning colors to nodes in the reverse 
order of their simplification, i.e., by popping nodes from the stack and, since 
they have at least $R-1$ colored neighbours assigning them one available color. 
We call this algorithm the \emph{greedy coloring scheme}. Whenever this scheme 
gets stuck, i.e., if all remaining nodes have degree at least $R$, we do not 
know whether the graph is $R$-colorable or not since it is just a heuristic.


\subsubsection{Greedy-$k$-colorable graphs.}

An interesting fact is that the success or failure of this greedy scheme is 
independent of the order of simplification. In other words, if the greedy 
scheme fails at some point, there is no simplification order that could 
simplify entirely the graph. This is because the remaining nodes have degree 
greater than~$R$ at all time and there is no way to reduce their degree than 
simplifying one of them, which is impossible. The side effect is that the 
greedy scheme defines a class of graphs that we call the 
\emph{greedy-$k$-colorable} graphs. These are the graphs $k$-colorable using 
the greedy scheme. The pseudo-code for testing if a graph is 
greedy-$k$-colorable is given on Figure~\ref{code:is-k-greedy}.



\begin{comment}
\begin{function}[h]
\captionlabel{\labelkgreedy}{Is\_kGreedy}{($G$)}
\KwData{Undirected graph $G=(V,E)$; $\forall v\in V$, degree[$v$] = \#neighbors 
of $v$ in $G$, $k$ number of colors}
% \lForEach{$v\in V$}{set degree[$v$] to degree of $v$ in $G$ \;}
stack = $\emptyset$ ;
$\mbox{worklist} = \{ v \in V \mid \mbox{degree[$v$]} < k\}$ \;
\While{$\mbox{worklist} \neq \emptyset$}{
  \KwLet $v \in \mbox{worklist}$ \;
  \ForEach{$w$ neighbor of $v$}{
     degree[$w$] \<- degree[w]-1 \;
     \lIf{$\mbox{degree[$w$]} = k-1$}{worklist \<- $\mbox{worklist} \cup \{w\}$}
  }
  push $v$ on stack ;
  worklist \<- $\mbox{worklist} \setminus \{v\}$  \tcc*{Remove $v$ from  $G$}
}
\lIf{$V = \emptyset$}{\Return{\KwTrue}}
\lElse{\Return{\KwFalse}}
\end{function} 
\end{comment} 

\begin{figure}
\begin{verbatim}
Function Is_kGreedy(G)
  Data: Undirected graph G = (V, E);
  ∀v ∈ V, degree[v] = #neighbors of v in G,
  k number of colors
stack = {} ; worklist = {v ∈ V | degree[v] < k} ;
while worklist != {} do
  let v in worklist ;
  foreach w neighbor of v do
    degree[w] = degree[w]-1 ;
    if degree[w] = k - 1 then worklist = worklist U {w}
  push v on stack ; worklist = worklist \ {v} ; /* Remove v from G */
if V = {} then return TRUE else return FALSE
\end{verbatim}
\caption{Iskgreedy function}
\label{code:is-k-greedy}
\end{figure}

\subsubsection{Simplification on chordal graphs.}

Chordal graphs are easily colorable using a \emph{simplicial elimination 
scheme}. There is indeed a property that chordal graph have at least two 
simplicial vertices, i.e., nodes whose neighbours in the graph are a clique (a 
complete subgraph) \todo{check if already said in chapter 2, or cf. Appendix}.  
A simplicial elimination scheme is a particular order of simplification of 
nodes in which only simplicial nodes may be chosen. Such an order gives the 
minimum number of colors to the graph in polynomial time, but can be a bit 
tedious to obtain. In practice, in register allocation, we do not need the 
minimum number of colors, we just need that at most $R$ colors are used. Since 
enough spilling has already been done in the previous section, we know that 
\maxlive is at most $R$, hence that the interference graph is $k$-chordal with 
$k\leq R$. This means that every simplicial node in the simplicial elimination 
scheme has a degree at most $k-1$ in the remaining graph, hence any simplicial 
elimination order is also a valid simplification order for a 
greedy-$k$-coloring. Hence a $k$-chordal graph is also greedy-$k$-colorable, 
hence greedy-$R$-colorable and any simplification scheme for $R$ registers will 
manage to color it with at most $R$ colors.


This means we do not need a fancy coloring algorithm tuned for chordal graphs, 
the original simplification scheme of Chaitin et al.\ does the job, without any 
modification. Following a call to Is\_k\_Greedy, it is possible to re-use the 
stack to assign the colors by making a call to Assign\_colors, 
Figure~\ref{code:assign-color}. Among the good things, this also means that 
existing register allocators can be easily modified to handle SSA code and that 
register allocation can benefit from powerful coalescing strategies such as 
aggressive or conservative ones. Moreover, the fact that register allocation 
can be decoupled into a phase of spilling first and then a phase of 
coloring/coalescing allows the writing of more involved coalescing \todo{cite 
examples}.


\begin{figure}
\begin{verbatim}
Function Assign_colors(G)
  available = new array of size R and values True
  while stack != {} do
    v = pop stack ;
    for each neighbour w in G
      available[color(w)] = False
    col = 0;
    for each color c from 1 to R
      if available[c]
        col = c
      available[c] = True /* prepare for next round */
    color(v) = col
    add v to G
\end{verbatim}
\caption{Assign\_color function}
\label{code:assign-color}
\end{figure}





\subsection{Fast tree-scan solution}
{\sl
\begin{itemize}
  \item On the dominance tree, possible to scan from top and assign colors to 
    variables as they come.
  \item Biased coloring is used to coalesce variables linked by \phifuns.
  \item see next section for more involved coalescing
  \item Fab: I think that you already talked about scan coloring. Ok to talk about biased.
  \item Fab: You can talk about biased coloring that uses the result of an aggressive coalescing (see with Quentin). It is not more complicated and it improves results.
\end{itemize}
}

One of the advantages of SSA is to make things simpler, faster, and use less 
memory. For the register assignment problem, we have seen that the existing 
greedy scheme based on simplification still works, however, this still requires 
the construction and maintaining of an interference graph, a structure judged 
to big and cumbersome to be used in JIT context, where compilation time and 
memory prints matters more. This is one of the reasons linear-scan allocators 
are being developed, even if their performance does not equal those of 
aggressive time-consuming off-line compilers. We will now present a method to 
perform fast register assignment for SSA programs which does not need more than 
the dominance tree and def-use chains.

Let us get back at the dominance tree representation, with live-ranges being 
subtrees of this tree. This representation reflects the chordal property of the 
interference graph. Consider the variables alive at a leaf of the dominance 
tree (the ``end'' of a branch), they form a clique of the graph and because of 
the dominance property, the definition of one of them, say $v$, is dominated by 
the definitions of all the others.  This variable has a ``smaller'' live-range, 
compared to the other variables, which are all already alive at its definition.  
This means that every variable interfering with $v$ also interfere with the 
other variables. In other words, all neighbours of $v$ form a clique, i.e., $v$ 
is a simplicial node in the interference graph, and it has at most $R-1$ 
neighbours.  This means that all ``leaf'' variables, i.e., the ones that have 
their definition the closest to the end of their branch in the dominance tree, 
are candidate for simplification by the greedy algorithm. Once they are 
simplified, other variables become leaf variables and the simplification 
process can continue again, until we reach the root of the dominance tree.

Following the idea of the greedy algorithm, the stack is now filled with 
variables ordered by appearance of their definition, i.e., if the definition of 
$a$ appears before the definition of $b$ on the path from the root to $b$, $a$ 
is higher in the stack, hence will be colored before $b$. In short, it is 
possible to color all subtrees of the dominance tree by starting at the root 
and scanning the dominance tree in a top-down fashion, assigning colors to 
variable as their definition is encountered. This is possible since, under SSA,  
variables are subtrees hence do not interfere if they belong to different 
branches. A pseudo-code to assign colors is given in procedure Tree\_Scan,
Figure~\ref{code:assign-tree-scan}.


\begin{figure}
  The {\tt colors} array is boolean and represent available colors.

  \begin{verbatim}
Tree_Scan(T)

  function assign_color(p,colors)
    for each v last use at p
      colors[color(c)] = True   /* colors not used anymore */

    for each v defined at p
      choose_color(v,colors)    /* choose available color */
      colors[c] = False
      color(v) = c

    for each child p' of p
      assign_color(p', colors)

  color(root(T))

  \end{verbatim}
  \caption{Tree scan coloring algorithm for SSA.}
  \label{code:assign-tree-scan}
\end{figure}


This coloring algorithm is really fast as it only needs one traversal of the 
dominance tree. Since the number of colors is fixed and small, it can be 
implemented as a bit-set so as to speed-up updates of which colors are 
available or not. The pseudo-code for function {\tt choose\_color} is 
deliberately not given yet. A very basic implementation can just scan the 
color array until it finds one that is not taken (this is assured since we are 
under SSA and spilling has already been done). We will now see why it might be 
beneficial to bias the choosing of colors so as to perform some coalescing.



\subsubsection{Biased coalescing.}

The goal of coalescing is to minimize the number of register-to-register {\tt 
move} instructions in the final code. While there may not be so many such 
``copies'' at the high-level (e.g., instruction {\tt a = b} in C), especially 
after a phase of copy propagation under SSA (\todo{See chapter X}), there can 
be much more of these after the register allocation phase. One reason is that 
it helps treating register constraints, we will explain this point in 
Section~\ref{sec:practical-regalloc}. One even more obvious and unavoidable 
reason is the presence of \phifuns due to SSA form. Indeed, remember that a 
\phifun represents in fact parallel copies on incoming edges of basic blocks. 
For instance, if the function is $a \gets \phi(b,c)$, then it means that 
instructions $a\gets b$ should be executed on the edge coming from the left, 
and $a\gets c$ on the one from the right. Suppose now that register allocation 
decided to put $a$ and $b$ in register $R_1$, but $c$ in register $R_2$, then 
the copy $a\gets b$ does not need to be executed anymore, but $a\gets c$ does 
since the value of variable $c$, will be contained in $R_2$ and needs to 
transfered to $R_1$, which means the final code will contain instruction {\tt 
mov $R_1$, $R_2$} or the like.

Obviously, it is better to assign variables linked by a \phifun, as definition 
and argument, to the same register. This concerns for instance all subscripts 
of the same variable: it is usually better to assign $a_1$, $a_2$, etc.\ to the 
same register since they come from the same variable $a$. In fact we can define 
a notion of \emph{affinity}, acting as the converse of the relation of 
interference, expressing how much two variables ``want'' to have the same 
color, i.e., share the same register. A metric can be added to this notion, 
measuring the benefit one could get if the two variables are assigned to the 
same register. Hence, for two variables $a$ and $b$, count 1 for each time they 
appear in the same copy instruction ($a\gets b$ or $b\gets a$) or \phifun 
($a\gets\phi(\ldots,b,\ldots)$ or $b\gets\phi(\ldots,a,\ldots)$). These numbers 
can of course be weighted using empirically parameters (classically, times ten 
for each level of nested loop, halved if in a conditional branch, etc.) or 
using actual execution frequencies based on profiling.


Since the number of affinities is sparse compared to the number of possible 
pairs of variables, we propose to keep for each variable a list affinities, 
where each element is a pair composed of a variable and the weight of the 
affinity with this variable. In that case, the function {\tt choose\_color} can 
be written as in Figure~\ref{code:choose-color}. This is a very simple bias, 
where at each point the color chosen is the one that maximizes, under the 
current knowledge, the coalescing for the variable being colored. It is not of 
course optimal since the general problem of coalescing is NP-complete 
(\todo{ref}), and biased algorithms are known to be easily misguided.


\begin{figure}
  The {\tt colors} array is boolean and represent available colors.

  \begin{verbatim}
global variables
  count = array of size #colors and elements 0

choose_color(v, colors)
  for each (u,w) in affinity_list(v)
    count[color(u)] += w
  max_w = -1
  col = 0
  for each color c
    if colors[c] = true /* color is not used by a neighbour of v */
      if count[c] > max_w
        col = c
        max_w = count[c]
    count[c] = 0 /* prepare array for next call to choose_color */
  \end{verbatim}
  \caption{Choosing a color with a bias for coalescing.}
  \label{code:choose-color}
\end{figure}


\subsubsection{Aggressive coalescing to improve biased coalescing.}

There is another simple and interesting idea to improve biased coalescing. We 
still consider a JIT context where compilation speed is very important.
\todo{what kind of aggressive? Ask fab\ldots}






\section{Practical and advanced discussions}
\label{sec:practical-regalloc}


\subsection{Handling registers constraints}
{\sl
\begin{itemize}
  \item ABI contraints
  \item split beforehand solution: many parallel copies, need good biased 
    coloring
  \item repair afterwards solution: biased try to give right color, else, 
    actual copies are added
  \item Fab: For the repair afterward at the time we will publish the book you 
    will be able to cite the current paper we write on tree-scan coalescing. We 
    plan to elaborate on the repairing mechanism and illustrate it in the 
    context of tree-scan.
  \item Fab: For the repair afterward you will build an interference graph with 
    negative affinity weights. Current coalescers do not handle it. A 
    workaround (but costly) for affinity adge (a,b) with weight W<0 is replace 
    it by an interference edge (a,ab) and an affinity edge (ab,b) of weight -W 
    with ab a dummy node. For iterated reg-alloc (Briggs \& George's test) you 
    can rewrite the different rules by doing as if you had such a dummy node 
    (you virtualize it). You can discuss this with Quentin. Of course the idea 
    is not to describe it here but it can be interesting to discuss a little 
    bit the implications.
\end{itemize}
}

In theory, register allocation algorithms are always nicely working with nodes 
and colors. In practice, however, not all variables or registers are 
equivalent. Depending on the architecture, some registers might be dedicated 
to perform memory operations, some instructions expects their operands to 
reside in particular registers, and in general conventions are defined to 
simplify for example the writing of libraries, by specifying how parameters to 
functions are passed and how results are returned. This adds constraints to the 
register allocation, usually by restricting the coloring possibilities of 
variables. Some examples are given in Figure~\label{fig:reg-constraints}.

\begin{figure}
  \begin{tabular}{ccc}
    Constraint  & Instruction & Effect on reg. alloc. \\
    Division in registers $R_x$ and $R_y$ & $a / b$ & $a$ in $R_x$, $b$ in $R_y$\\
    Memory load cannot use $R_z$ & $a = load(b)$ & $b$ cannot be in $R_z$\\
    Functions arguments in $R_1$, $R_2$\ldots & $a = f(b,c)$ & $b$ in $R_1$, 
    $c$ in $R_2$ \\
  \end{tabular}
  \caption{Examples of register constraints.}
  \label{fig:reg-constraints}
\end{figure}


The problem with such constraints is that they cannot be expressed directly in 
the register allocation problem. For instance, if variable $a$ and $b$ must 
absolutely reside in register $R_1$, it is not possible to pre-color them with 
color $R_1$ as the interference graph could not be chordal anymore.  
Furthermore, $a$ and $b$ maybe interfere, for instance if they are both the 
first parameter of successive function calls, it that case putting them in the 
same register without check would break the program. We propose two solutions 
to deal with this problem. The first is classic in the literature, and the 
second is newer and promising.


\subsubsection{Splitting variables to handle register constraints.}

Traditionally, variables involved in constraining operations are split before 
and after the operation. For instance, if $a$ is involved in a division on an 
x86, the instructions $a'\gets a$ and $a\gets a'$ are issued before and after 
the division so that, if $a$ is not in the right register, it can be moved in 
it for the operation. This however still poses the problem of variable $a'$ 
which still interfere with every variable alive during the division, and hence 
is part of the interference graph. Moreover, $a$ is redefined which breaks the 
SSA. A workaround solution is to split \emph{all} variables alive before and 
after the operation using a \emph{parallel copy}. New subscripted variables 
must be defined by the second split to keep the SSA property (and subsequent 
uses must be changed accordingly). The use of parallel copies assures that 
the locally created variables, with very short live-ranges (span of only one 
instruction), are completely disconnected from the interference graph. Their 
only relation with the other, ``normal'' variable are affinities that each 
variable share with the two other parts coming from the same original variable.
In that case, the danger is that many copy instructions can be added if the 
coalescing is not good enough to assign most of the created copies to the same 
register.


\subsubsection{Repairing problems afterwards.}

Another possibility is to let register allocation do its job, and then 
intervene to repair the coloring whenever it does not fit the constraints, by 
adding copies afterwards around mismatches. To minimize the number of conflicts 
on constraints, it is however important to drive the coloring so that it still 
gives the right color whenever possible. For example, if variable $a$ must 
reside in register $R_1$, it is possible to consider a clique of $R$ nodes 
otherwise disconnected from the graph, one for each register, and adding $R_1$ 
in the affinity list of $a$. On the contrary, if $a$ cannot be assigned to a 
register, say $R_2$, an affinity of \emph{negative weight} is added to the 
list, representing the added cost it will required if $a$ is still put in 
register $R_2$. Existing coalescing algorithms in the literature currently do 
not support negative weight \todo{see Fab's hack}.








\subsection{Out-of-SSA and critical edge splitting}

{\sl
\begin{itemize}
  \item \phifuns are not machine instructions, they are replaced by actual 
    copies in out-of-SSA phase
  \item in our case, colored SSA: Sreedhar not possible
  \item SSA implicitly 
    actual ``parallel copies'' are placed on edges
  \item problem with some critical edges: abnormal edges, back-edge of loop
  \item Fab: Sreedhar IS possible. It generates local variables not colored.
The point is that its coloring might be stucked (Chaitin reduction). So the possible need to split edges...
\end{itemize}
}

As mentioned previously, the \phifuns are not actual machine instructions.  
They must be removed when going ``out-of-SSA.'' In traditional translation 
out-of-SSA, this happens before register allocation, but in our case, variables 
are already allocated to memory or registers, and arguments and results of 
\phifuns are not variables anymore but registers.


If the definition and arguments 
of a \phifun have been allocated to the same register, then it can be safely 
removed. If this is not the case, however, the some {\tt move} instructions 
must be added to keep the behaviour of the program. 



\subsection{More repairing on \phifuns}
\begin{itemize}
  \item Biased coloring is fast but not very good at coalescing
  \item Basic parallel copy motion: move to next edge.
  \item More involved problem: spill variables
  \item Better solution: look at biblio (parallel copy motion)
  \item Fab: Yes make it as simple as possible. Local pcopy motion works in practice. It is useless to go further. The ultimate solution (Sreedhar based) might have been discussed in the previous section
\end{itemize}


\endofchapter
}
