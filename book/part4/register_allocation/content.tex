% vim:spell:spelllang=en
\chapter{Register Allocation\Author{F. Bouchez, S. Hack}}
\label{chap:register-allocation}

{

%% Local macros for personal use
\newcount\todocount \todocount=0
\def\todo#1{\global\advance\todocount by 1 {\bf TODO:} #1}
\newlinechar=`\^^J
\def\endofchapter{\ifnum\todocount>0\immediate\write16{^^JLaTeX Warning: There 
was still TODO macros: \the\todocount.^^J}\fi}


\def\ac#1{#1}
\def\dom{\preceq}
\def\ssa{SSA\xspace}

\numberofpages{15}
Page count: 15


\chapterauthor{Bouchez, Hack}

{\sl
We will start by giving an overview and intuitive understanding why SSA helps 
register allocation. Then we present the sub-problems of spilling and 
coloring/coalescing, as two separate phases, under the light of SSA. Finally, 
practical issues like register constraints and critical edge splitting are 
discussed.

%% Intro by Hack
%We will start by giving an overview and intuitive understanding why SSA helps register allocation.
%Then, we present the principal architecture of an SSA-based register allocator.

%We discuss spilling, assignment, and coalescing in further detail and review the proposed techniques.
%Finally, we present the corresponding complexity results for each phase.
}


\todo{Longer introduction}

The goal of register allocation is to map the variables of a program to 
physical memory locations. The compiler must indeed decide, in advance, in 
which locations will be held the values necessary for the computations of the 
program, and so for each instruction of the program. Registers are a very fast 
memory, hence are preferred for holding these values directly needed by the 
CPU. But there is a limited, small number of registers available in a 
processor, for instance only 8 registers for the IA-32 architecture (X86, 32 
bits), or 64 for the ST200, a Very Long Instruction Word (VLIW) processor.  On 
the other hand, in the initial program representation and until very late in 
the compiler back-end, values are stored in variables or temporaries, which are 
unbounded in number.



\section{Why does SSA help for register allocation?}


\subsection{Classical register allocation}

{\sl
\begin{itemize}
  \item based on graph coloring, linear scan
  \item spilling \& coloring are dependent
  \item heuristics are used to find a working solution
  \item avoiding spills is more important than coalescing
  \item Fab: It is important to outline that with decoupled reg-alloc, spilling and coloring become much simpler and more efficient.
\end{itemize}
}

Traditionally, register allocation schemes come in two flavors: based on 
graph-coloring, for better results but more time and memory consuming, suitable 
for off-line compilation; and based on ``linear scans,'' for easiness of the 
scheme and quick compilation, suitable for Just-in-Time (JIT) compilation.  The 
goal of any register allocation algorithm is to make the best use of registers 
possible, i.e., minimizing the \emph{spilling} (storing variables to memory), and to 
reduce data movements between registers by performing \emph{coalescing} (putting 
variables linked by copy instructions in the same register).

\subsubsection{Spilling \& coalescing inter-dependence.}
For all modern architectures, reducing the spilling is more important that 
maximizing the coalescing, since loading or storing a variable in memory takes 
more cycles than performing a register-to-register {\tt move} instruction. 
However, the two optimisations are often performed at the same time (\todo{true 
for linear-scan too?}) because they both depends on whether there is enough 
registers for the variables or not. For instance, in traditional graph-coloring 
register allocation, spilling allows to remove some node of the interference 
graph, hopefully reducing the number of colors required. Conjointly, coalescing 
two variables merges the corresponding nodes, which can have positive effects
or negative effects: on the one hand, it will remove a copy in the program and 
diminishes the degree of the nodes that are common neighbours, hence augmenting 
their chances to be simplified (\todo{define simplification, or link to iterated 
register coalescing?}), i.e., increasing the chances that the graph becomes 
colorable; on the other hand, the created node will have more neighbours, hence 
diminishing its chances to be simplified and augmenting its chances of being 
spilled, in which case two variables are being spilled (the coalesced ones) 
instead of just one, or even maybe none. 

Since there is no way to know the minimum number of colors required to color a 
general graph, coalescing strategies are usually \emph{conservative}, to avoid 
transforming a $k$-colorable graph into a non $k$-colorable one. This means 
opportunities to improve the spilling and the coalescing are missed.

\subsubsection{Decoupling register allocation.}
We will see that, under SSA form, we have a polynomial algorithm that can tell 
whether $R$ registers are sufficient or not. This allows to decouple register 
allocation into two phases: the first phase spills variables until we know that 
$R$ registers are sufficient, at which point no coalescing can help reducing 
the coloring number any further, then, the remaining variables are assigned to 
registers while performing as much coalescing as possible, forbidding any more 
spill.

This for one allows for much simpler register allocation schemes, as each phase 
must be handled separately, which relieves the compiler of the worries of 
coalescing affecting the spilling. This makes also for a simpler architecture, 
where the two phases are cleanly separated (compare to IRC).

\todo{STOPPED HERE}





\subsection{Programs under SSA have chordal interference graphs}
{\sl 
\begin{itemize}
  \item Reminder that programs under SSA are chordal
  \item What difference does it make? Knowing the minimum number of colors is 
    easy: we know if spilling is mandatory or not
  \item why are the graphs chordal? View of SSA variables as subtrees of the 
    dominance tree; made possible because of \phifuns
  \item \phifuns allows colors to be ``re-arranged'' on incoming edges: this 
    is implicit shuffle code. For this to work, remember that semantics of 
    \phifun is they are \emph{parallel copies}. Hence, the SSA-variables of 
    same initial variable in different branches of dominance tree are 
    color-independent.

  \item Fab: the chordality and the parallel semantic of phi have already been considered in the first chapters of parts I (which does not mean that you cannot recall it).

\end{itemize}
}

It is known since 2005 that, under SSA, the interference graph of a program is 
chordal \cite{Brisk05,Pereira05:chordal,Hack06}.  Note that the interference 
graph depends on the interference notion, and we need that two variables alive 
at the same interfere even if they have the same value.\footnote{Note that, under 
SSA, there is only one definition, hence, if variable $b$ is defined as $b\gets 
a$, it is possible to replace every occurrence of $b$ by $a$ since they will 
always be equal.
%Note that this is akin to aggressive coalescing but is safe to 
%do under  \ssa as it will not increase the coloring number.  Constant 
%propagation can do this efficiently in a first pass, then it is not worth 
%considering the interference definition of Chaitin 
%(Definition~\ref{def:chaitin-interf}) as there is no remaining copy in the 
%program.
}
This was already mentioned in Chapter~\ref{chap:properties-and-flavours}. We 
recall here the theorem since it is a vital property for register allocation:

\begin{theorem}
  \label{thm:ssa-chordal}
  The interference graph~$G$ of a program under \ac{SSA} with dominance 
  property is chordal.
\end{theorem}

The major impact for register allocation is that, while coloring a general graph 
with the minimum number of colors in NP-complete, it is polynomial in the case 
of chordal graphs. Since register allocation amounts to coloring the 
interference graph of a program, this means that, under SSA, we can have an
algorithm that answer in polynomial time whether $R$ registers are sufficient 
or not for allocation all variables, i.e., \emph{whether spilling is mandatory 
or not.} This is a fundamental breakthrough as, up to know, register 
allocation algorithms all depends on coloring \emph{heuristics} to decide 
whether to spill or not.




\subsubsection{Why so chordal?}

The key observation leading to this theorem was that the dominance graph of a 
program under SSA is a tree, and since every definition of a variable dominates  
its uses, the live-ranges of variables are subtrees of the dominance tree. This 
is a characterisation of chordal graphs (see \cite[Thm.~4.8]{Golumbic} and 
Appendix~\ref{app:chordal-graphs}).


Since Chaitin et al.'s NP-completeness proof on register allocation in 
1981~\cite{Chaitin81}, we know that the interference graphs of programs are 
general, i.e., any graph can be the interference graph of a program. The 
question is: Why, under SSA form, the interference graph of a program is not 
general anymore but instead becomes chordal? The answer of course lies in the 
element that modify the program structure: the \phifuns.

To visualize how the \phifuns transform an interference graph into a chordal 
graph, let us take the dominance tree of a program under SSA. Before going to 
SSA, the variables had no reason to be subtree of this tree, meaning that 
live-ranges can span across multiple branches, not following only paths on the 
tree but also using paths of the control flow that are not part of the 
dominance tree. These paths are in fact all control-flow edges leading to basic 
blocks with multiple predecessors (hence, not ``dominance edges''). This means 
that when going to SSA, every variable crossing these edges (and being 
different on other incoming edges) will be ``cut'' there by the insertion of a 
\phifun. The parallel semantics of \phifuns means indeed that these variables 
are now ``split'' into at least two parts one before the control-flow edge and 
one after. These two live-ranges can be allocated to different registers and do 
not ``cross'' the control-flow edge anymore.



\subsection{Maxlive and colorability of graphs}
\begin{itemize}
  \item definition of Maxlive
  \item show that Maxlive is the minimum number of colors required
  \item scan of the dominance tree sufficient to know Maxlive: pseudo-code
  \item Fab: MAXLIVE minimum if interference graph==intersection graph. Clearly we do not want to go into such details but be careful to use precise and correct terms.

\end{itemize}
    


\section{The problem of spilling}

\subsection{Lowering Maxlive}
\begin{itemize}
  \item The min number of colors is Maxlive. Variables must be stored in memory 
    to lower the register pressure at points where it is $>R$.
  \item Whenever, Maxlive $\leq R$, we know for sure that spilling more in 
    unnecessary.
  \item Fab: if MAXLIVE<=R spilling is unecessary under some conditions (we color SSA code, going out of SSA code might require some additionnal spilling. Also without naming constraints). So again be careful about what you say. I think that the idea is to have a register allocator under SSA as simple as possible with a very simple modeling and nice properties. Then we cope with real world issues during the colored-SSA destruction.


\end{itemize}

\subsection{SSA does not help for spilling}
\begin{itemize}
  \item easy spill method: spill everywhere
  \item SSA split points are good for coloring, but not really for spilling 
    (see article spill everywhere under SSA)
  \item other split points to avoid spill everywhere are probably better
  \item Fab: if you need an example to illustrate that SSA does not really help in practice for spilling ask Quentin. It does help a little bit (dynamic programming possible for spill everywhere with few registers; further first can be generalized). But what you want to point it here is the fact that the disadvantages seem more important than advantages.
  \item Fab: the LCTES spill everywhere article does not discuss practical issues (such as the fact that it might insert a load or a store inside a loop...). It is more algorithmical issues.


\end{itemize}


\subsection{Spilling under SSA}
Still, we give a possibility to perform spilling under SSA
\begin{itemize}
  \item Hack's algorithm: simplified version?
  \item \`a la Belady following dominance tree?

  \item Fab: The last Algorithm from Sebastian is Belady like. So yes try to simplify it, provide the pseudo code and give the intuitions on how the full version improves it.

\end{itemize}



\section{Coloring and coalescing}
\subsection{Chordal property for coloring}
\begin{itemize}
  \item two simplicial nodes
  \item color in the reverse order of simplification
  \item one such order considers subtrees from the root of the tree
  \item no need to construct interference graph
  \item Fab: make it as more intuitive as possible. Avoid as much as possible chordal stuffs which are useless. Greedy coloring and its relationship with the subtree representation is important on the other hand. See with Philip to avoid redundancies between your chapter and his (SSA properties). It is important to outline here that this property that is due to decoupled+SSA enables more aggressive coalescing heuristics. But also simpler (tree-scan).
\end{itemize}

\subsection{Fast tree-scan solution}
\begin{itemize}
  \item On the dominance tree, possible to scan from top and assign colors to 
    variables as they come.
  \item Biased coloring is used to coalesce variables linked by \phifuns.
  \item see next section for more involved coalescing
  \item Fab: I think that you already talked about scan coloring. Ok to talk about biased.
  \item Fab: You can talk about biased coloring that uses the result of an aggressive coalescing (see with Quentin). It is not more complicated and it improves results.
\end{itemize}



\section{Practical and advanced discussions}

\subsection{Handling registers constraints}
\begin{itemize}
  \item ABI contraints
  \item split beforehand solution: many parallel copies, need good biased 
    coloring
  \item repair afterwards solution: biased try to give right color, else, 
    actual copies are added
  \item Fab: For the repair afterward at the time we will publish the book you will be able to cite the current paper we write on tree-scan coalescing. We plan to elaborate on the repairing mechanism and illustrate it in the context of tree-scan.
  \item Fab: For the repair afterward you will build an interference graph with negative 
affinity weights. Current coalescers do not handle it. A workaround (but 
costly) for affinity adge (a,b) with weight W<0 is replace it by an 
interference edge (a,ab) and an affinity edge (ab,b) of weight -W with ab a 
dummy node. For iterated reg-alloc (Briggs \& George's test) you can rewrite the different rules by doing as if you had such a dummy node (you virtualize it). You can discuss this with Quentin. Of course the idea is not to describe it here but it can be interesting to discuss a little bit the implications.
\end{itemize}


\subsection{Out-of-SSA and critical edge splitting}
\begin{itemize}
  \item \phifuns are not machine instructions, they are replaced by actual 
    copies in out-of-SSA phase
  \item in our case, colored SSA: Sreedhar not possible
  \item SSA implicitly 
    actual ``parallel copies'' are placed on edges
  \item problem with some critical edges: abnormal edges, back-edge of loop
  \item Fab: Sreedhar IS possible. It generates local variables not colored.
The point is that its coloring might be stucked (Chaitin reduction). So the possible need to split edges...
\end{itemize}


\subsection{More repairing on \phifuns}
\begin{itemize}
  \item Biased coloring is fast but not very good at coalescing
  \item Basic parallel copy motion: move to next edge.
  \item More involved problem: spill variables
  \item Better solution: look at biblio (parallel copy motion)
  \item Fab: Yes make it as simple as possible. Local pcopy motion works in practice. It is useless to go further. The ultimate solution (Sreedhar based) might have been discussed in the previous section
\end{itemize}


\endofchapter
}
