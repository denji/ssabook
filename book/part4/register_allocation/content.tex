% vim:spell:spelllang=en
\chapter{Register Allocation\Author{F. Bouchez, S. Hack}}
\label{chap:register-allocation}

{

%% Local macros for personal use
\newcount\todocount \todocount=0
\def\todo#1{\global\advance\todocount by 1 {\color{blue} {\bf TODO:} #1}}
\newlinechar=`\^^J
\def\endofchapter{\ifnum\todocount>0\immediate\write16{^^JLaTeX Warning: There 
was still TODO macros: \the\todocount.^^J}\fi}


\def\ac#1{#1}
\def\dom{\preceq}
\def\ssa{SSA\xspace}
\def\maxlive{Maxlive\xspace}
\def\irc{Iterated Register Coalescing\xspace}

\numberofpages{15}
Page count: 15


\chapterauthor{Bouchez, Hack}

{\sl
We will start by giving an overview and intuitive understanding why SSA helps 
register allocation. Then we present the sub-problems of spilling and 
coloring/coalescing, as two separate phases, under the light of SSA. Finally, 
practical issues like register constraints and critical edge splitting are 
discussed.

%% Intro by Hack
%We will start by giving an overview and intuitive understanding why SSA helps register allocation.
%Then, we present the principal architecture of an SSA-based register allocator.

%We discuss spilling, assignment, and coalescing in further detail and review the proposed techniques.
%Finally, we present the corresponding complexity results for each phase.
}


\todo{Longer introduction}

The goal of register allocation is to map the variables of a program to 
physical memory locations. The compiler must indeed decide, in advance, in 
which locations will be held the values necessary for the computations of the 
program, and so for each instruction of the program. Registers are a very fast 
memory, hence are preferred for holding these values directly needed by the 
CPU. But there is a limited, small number of registers available in a 
processor, for instance only 8 registers for the IA-32 architecture (X86, 32 
bits), or 64 for the ST200, a Very Long Instruction Word (VLIW) processor.  On 
the other hand, in the initial program representation and until very late in 
the compiler back-end, values are stored in variables or temporaries, which are 
unbounded in number.


\todo{Explain reg-alloc = coloring, i.e., color = register.}


\section{Why does SSA help for register allocation?}


\subsection{Classical register allocation}

{\sl
\begin{itemize}
  \item based on graph coloring, linear scan
  \item spilling \& coloring are dependent
  \item heuristics are used to find a working solution
  \item avoiding spills is more important than coalescing
  \item Fab: It is important to outline that with decoupled reg-alloc, spilling and coloring become much simpler and more efficient.
\end{itemize}
}

Traditionally, register allocation schemes come in two flavors: based on 
graph-coloring, for better results but more time and memory consuming, suitable 
for off-line compilation; and based on ``linear scans,'' for easiness of the 
scheme and quick compilation, suitable for Just-in-Time (JIT) compilation.  The 
goal of any register allocation algorithm is to make the best use of registers 
possible, i.e., minimizing the \emph{spilling} (storing variables to memory), and to 
reduce data movements between registers by performing \emph{coalescing} (putting 
variables linked by copy instructions in the same register).

\subsubsection{Spilling \& coalescing inter-dependence.}
For all modern architectures, reducing the spilling is more important that 
maximizing the coalescing, since loading or storing a variable in memory takes 
more cycles than performing a register-to-register {\tt move} instruction. 
However, the two optimisations are often performed at the same time (\todo{true 
for linear-scan too?}) because they both depends on whether there is enough 
registers for the variables or not. For instance, in traditional graph-coloring 
register allocation, spilling allows to remove some node of the interference 
graph, hopefully reducing the number of colors required. Conjointly, coalescing 
two variables merges the corresponding nodes, which can have positive effects
or negative effects: on the one hand, it will remove a copy in the program and 
diminishes the degree of the nodes that are common neighbours, hence augmenting 
their chances to be simplified (\todo{define simplification, or link to iterated 
register coalescing?}), i.e., increasing the chances that the graph becomes 
colorable; on the other hand, the created node will have more neighbours, hence 
diminishing its chances to be simplified and augmenting its chances of being 
spilled, in which case two variables are being spilled (the coalesced ones) 
instead of just one, or even maybe none. 

Since there is no way to know the minimum number of colors required to color a 
general graph, coalescing strategies are usually \emph{conservative}, to avoid 
transforming a $k$-colorable graph into a non $k$-colorable one. This means 
opportunities to improve the spilling and the coalescing are missed.

\subsubsection{Decoupling register allocation.}
We will see that, under SSA form, we have a polynomial algorithm that can tell 
whether $R$ registers are sufficient or not. This allows to decouple register 
allocation into two phases: the first phase spills variables until we know that 
$R$ registers are sufficient, at which point no coalescing can help reducing 
the coloring number any further; then, the remaining variables are assigned to 
registers while performing as much coalescing as possible as long as it does 
not induce more spill.

This for one allows for much simpler register allocation schemes, as each phase 
must be handled separately, which relieves the compiler of the worries of 
coalescing affecting the spilling. As such, each phase can be optimized more 
easily and the resulting register allocation phase can be more efficient.
\todo{finish explanations}





\subsection{Programs under SSA have chordal interference graphs}
{\sl 
\begin{itemize}
  \item Reminder that programs under SSA are chordal
  \item What difference does it make? Knowing the minimum number of colors is 
    easy: we know if spilling is mandatory or not
  \item why are the graphs chordal? View of SSA variables as subtrees of the 
    dominance tree; made possible because of \phifuns
  \item \phifuns allows colors to be ``re-arranged'' on incoming edges: this 
    is implicit shuffle code. For this to work, remember that semantics of 
    \phifun is they are \emph{parallel copies}. Hence, the SSA-variables of 
    same initial variable in different branches of dominance tree are 
    color-independent.

  \item Fab: the chordality and the parallel semantic of phi have already been considered in the first chapters of parts I (which does not mean that you cannot recall it).

\end{itemize}
}

It is known since 2005 that, under SSA, the interference graph of a program is 
chordal \cite{Brisk05,Pereira05:chordal,Hack06}.  Note that the interference 
graph depends on the interference notion, and we need that two variables alive 
at the same time interfere even if they have the same value.\footnote{Note that, under 
SSA, there is only one definition, hence, if variable $b$ is defined as $b\gets 
a$, it is possible to replace every occurrence of $b$ by $a$ since they will 
always be equal.
%Note that this is akin to aggressive coalescing but is safe to 
%do under  \ssa as it will not increase the coloring number.  Constant 
%propagation can do this efficiently in a first pass, then it is not worth 
%considering the interference definition of Chaitin 
%(Definition~\ref{def:chaitin-interf}) as there is no remaining copy in the 
%program.
}
This chordal property was already mentioned in 
Chapter~\ref{chap:properties-and-flavours}. We recall here the theorem since it 
is a vital property for register allocation:

\begin{theorem}
  \label{thm:ssa-chordal}
  The interference graph~$G$ of a program under \ac{SSA} with dominance 
  property is chordal.
\end{theorem}

The major impact for register allocation is that, while coloring a general graph 
with the minimum number of colors in NP-complete, it is polynomial in the case 
of chordal graphs. Since register allocation amounts to coloring the 
interference graph of a program, this means that, under SSA, we can have an
algorithm that answers in polynomial time whether $R$ registers are sufficient 
or not to allocate all variables, i.e., \emph{whether spilling is mandatory 
or not.} This is a fundamental breakthrough as, up to know, register 
allocation algorithms all depend on coloring \emph{heuristics} to decide 
whether to spill or not.




\subsubsection{Why so chordal?}

The key observation leading to this theorem was that the dominance graph of a 
program under SSA is a tree, and since every definition of a variable dominates  
its uses, the live-ranges of variables are subtrees of the dominance tree. This 
is a characterisation of chordal graphs (see \cite[Thm.~4.8]{Golumbic} and 
Appendix~\ref{app:chordal-graphs}).


Since Chaitin et al.'s NP-completeness proof on register allocation in 
1981~\cite{Chaitin81}, we know that the interference graphs of programs are 
general, i.e., any graph can be the interference graph of a program. The 
question is: Why, under SSA form, the interference graph of a program is not 
general anymore but instead becomes chordal? The answer of course lies in the 
element that modify the program structure: the \phifuns.

To visualize how the \phifuns transform a general graph into a chordal graph, 
let us take the dominance tree of a program under SSA. Before going to SSA, the 
live-ranges of at least some variables are not subtrees of this tree,
spanning across multiple branches and not following only paths on 
the tree. These other paths are in fact all control-flow edges leading to basic 
blocks with multiple predecessors (hence, not ``dominance edges''). This means 
that when going to SSA, every variable crossing these edges (and being 
different on other incoming edges) will be ``cut'' there by the insertion of a 
\phifun. The parallel semantics of \phifuns means indeed that these variables 
are now ``split'' into at least two parts: one before the control-flow edge and 
one after. These two live-ranges can be allocated to different registers and do 
not ``cross'' the control-flow edge anymore. At runtime, this means that when 
removing the \phifun, shuffle code will be inserted on the edge to move 
variables that have sub-parts colored differently on their right registers.



\subsection{Maxlive and colorability of graphs}

{\sl
\begin{itemize}
  \item definition of Maxlive
  \item show that Maxlive is the minimum number of colors required
  \item scan of the dominance tree sufficient to know Maxlive: pseudo-code
  \item Fab: MAXLIVE minimum if interference graph==intersection graph. Clearly we do not want to go into such details but be careful to use precise and correct terms.
\end{itemize}
}
    
The register pressure at a program point is the number of variables alive at 
this point.\footnote{Considering all simultaneously alive variables interfere.}
The greatest register pressure over all a program is called \maxlive, 
since it is the maximum number of simultaneously alive variables. Obviously, 
spilling is mandatory if \maxlive is strictly greater than $R$, the number of 
registers.
Under SSA, the good news is that this is the only case where spilling is 
required, i.e., if \maxlive $\leq R$, there is no need for spilling. Indeed, 
\maxlive is the coloring number of the chordal interference graph of the 
program (\todo{ref to chapter 2?}). So, we can devise a polynomial test to know 
whether spilling is necessary or not by computing \maxlive. This can be done by 
checking on every program point the number of variables that are alive. If 
liveness information is available on the whole program, the order does not 
matter. However, in the case of SSA, this is not required as \maxlive can 
easily be computed without liveness information by performing a scan of the 
dominance tree, starting from the root. We only need to know which uses are 
``last-uses.'' The pseudo-code is given on Figure~\ref{code:compute-maxlive}.
\todo{check \maxlive scan needs last uses}.


\begin{figure}[ht]
  \begin{verbatim}
  function compute-branch (p, live)
    live <- live - { uses in p that are last uses }
    live <- live + { definitions of p }
    maxlive <- max( maxlive, #live )
    foreach child c of p
      compute-branch (c, live)

  function compute-maxlive
    maxlive <- 0
    compute-branch (root, {})
  \end{verbatim}
  \caption{Pseudo-code for \maxlive}
  \label{code:compute-maxlive}
\end{figure}




\section{The problem of spilling}
\subsection{Lowering Maxlive}

{\sl
\begin{itemize}
  \item The min number of colors is Maxlive. Variables must be stored in memory 
    to lower the register pressure at points where it is $>R$.
  \item Whenever, Maxlive $\leq R$, we know for sure that spilling more in 
    unnecessary.
  \item Fab: if MAXLIVE$\leq R$ spilling is unecessary under some conditions (we color SSA code, going out of SSA code might require some additionnal spilling. Also without naming constraints). So again be careful about what you say. I think that the idea is to have a register allocator under SSA as simple as possible with a very simple modeling and nice properties. Then we cope with real world issues during the colored-SSA destruction.
\end{itemize}
}

Usually, there are too many variables in a program and \maxlive is greater than 
$R$. In that case, we need to lower the register pressure at program points 
where it exceeds $R$ by spilling, i.e., storing variables in memory instead of 
registers. Whenever we reach a point where \maxlive $\leq R$, it will be 
possible to color all SSA variables with the $R$ registers. For now, the 
difficulty lies in choosing which variables to spill, and where to spill them. 
Indeed, a careful choice will lead to performance, while making bad decisions 
can lead to big slowdowns. As accesses to variables in memory require more time 
that accesses to registers, it is for instance better to avoid spilling 
variables used inside frequently executed code regions like loops.

\todo{spilling while keeping SSA property}


\subsection{Spilling is a difficult problem}

The problem of spilling is a difficult one in the literature, even under one of 
its simpler forms, the spill everywhere problem, where spilled variables stay 
so on their entire live-range~\cite{todo}. This spilling is advocated on the 
first graph-coloring algorithms~\cite{Chaitin81} and even in subsequent 
algorithms~\cite{IRC} because it fits better the graph-coloring representation. 
In these schemes, this amounts to removing nodes from a non $R$-colorable graph 
until it becomes $R$-colorable. Although this node deletion problem is known to 
be NP-complete, this allows the writing of simple heuristics based on the 
number of neighbours in the graph and a cost attached to nodes representing the 
overhead incurred in case they were to be spilled. An added problem is the fact 
that, for many architectures like RISC, spilled variables cannot be accessed 
directly from memory but must instead by copied back to the registers, when 
they are used, or which must be in a register when defined, just before being 
copied to memory. This creates new variables with very short live-ranges at the 
definition and use points, which must be accounted for register allocation. 
This explains why algorithms such as \irc must rebuild the interference graph 
after a phase of spilling and start again, until no more spilling is required.


The actual spilling problem is even more difficult, as variables do not have to 
be spilled on their entire live-range.
% borrowed from sebastian's CC'09 paper
Consider a loop with excessive register pressure and a variable that is defined 
before the loop and used afterwards. Ideally, a compiler would store (spill) 
the variable in front of the loop and load (reload) the variable after the 
loop. If the variable was reloaded inside the loop, the reload would be 
executed in each loop iteration. Another example is a variable that is used in 
a loop but has already been spilled before the loop. Reloading this variable 
directly before its use in the loop will cause memory traffic in each loop 
iteration. Thus, it is preferable to put the reload in front of the loop.
% end of borrowing
This poses a new problem on top of choosing which variables to spill: now we 
need also to choose where to spill them, i.e., where to put {\tt load} and {\tt 
store} instructions. This more general problem where the goal is to minimize 
the overhead of these added instructions is called the load-store optimization 
problem and is known to be NP-complete, even for a 
basic-block~\cite{Liberatore00}.  


\subsection{SSA does not help for spilling}
{\sl
\begin{itemize}
  \item easy spill method: spill everywhere
  \item SSA split points are good for coloring, but not really for spilling 
    (see article spill everywhere under SSA)
  \item other split points to avoid spill everywhere are probably better
  \item Fab: if you need an example to illustrate that SSA does not really help 
    in practice for spilling ask Quentin. It does help a little bit (dynamic 
    programming possible for spill everywhere with few registers; further first 
    can be generalized). But what you want to point it here is the fact that 
    the disadvantages seem more important than advantages.
  \item Fab: the LCTES spill everywhere article does not discuss practical issues (such as the fact that it might insert a load or a store inside a loop...). It is more algorithmical issues.
\end{itemize}
}

For the spilling problem, the SSA is not really of much help. Over a blind 
spill everywhere algorithm on the initial program, it has however the slight 
advantage of allowing the spilling of SSA variables, i.e., sub-variables of the 
initial variables. One major disadvantage is the added complexity when spilling
variables defined in \phifuns \todo{explain the problem if not all args are 
spilled, or if spilled in $\neq$ locations}.

{\sl Examples by Quentin}
\begin{minipage}{.45\textwidth}
\begin{verbatim}
CSSA :
   a =
   b =
 /       \
c= a   d = b
 \       /
 e = phi(c,d)
 = e
\end{verbatim}
\end{minipage}
\begin{minipage}{.45\textwidth}
After spilling spill :
\begin{verbatim}
   a =
   b =
 /          \
c= a     d = b
C=st c  D = st d
 \          /
 E = phi(C,D)
 e = ld E
 = e
\end{verbatim}
\end{minipage}
In that case, memory locations C, D and E can be the same. But this does not 
mean that placement of {\tt store} instruction is the best one.

\begin{minipage}{.45\textwidth}
\begin{verbatim}
SSA :
   a =
   b =
 /         \
 \         /
 e = phi(a,b)
 = e
\end{verbatim}
\end{minipage}
\begin{minipage}{.45\textwidth}
Spill supposing {\tt store}s are after definition
\begin{verbatim}
   a =
   A = st a
   b =
   B = st b
 /         \
 \ <--     / <--
 E = phi(A,B)
 e = ld E
 = e
\end{verbatim}
\end{minipage}

On the two edges pointed by arrows (or on the preceding basic block if they are 
not critical), additional memory transactions must reconcile the values:
\begin{verbatim}
tmp = ld X
Y = st tmp
\end{verbatim}

This can induce more spill, unforeseen.
Without SSA, this problem does not exist: for instance, with spill everywhere, 
the same memory slot is affected to all definitions of the same variable so no 
post-treatment is required as it is under SSA.



\subsection{Spilling under SSA}
Still, we give a possibility to perform spilling under SSA
\begin{itemize}
  \item Hack's algorithm: simplified version?
  \item \`a la Belady following dominance tree?

  \item Fab: The last Algorithm from Sebastian is Belady like. So yes try to simplify it, provide the pseudo code and give the intuitions on how the full version improves it.

\end{itemize}
\todo{Ask Sebastian to do it.}


\cite{Braun09:spilling-ssa}\todo{file braun09cc.pdf in biblio}


\section{Coloring and coalescing}
\subsection{Chordal property for coloring}
\begin{itemize}
  \item two simplicial nodes
  \item color in the reverse order of simplification
  \item one such order considers subtrees from the root of the tree
  \item no need to construct interference graph
  \item Fab: make it as more intuitive as possible. Avoid as much as possible 
    chordal stuffs which are useless. Greedy coloring and its relationship with 
    the subtree representation is important on the other hand. See with Philip 
    to avoid redundancies between your chapter and his (SSA properties). It is 
    important to outline here that this property that is due to decoupled+SSA 
    enables more aggressive coalescing heuristics. But also simpler 
    (tree-scan).
\end{itemize}

\subsection{Fast tree-scan solution}
\begin{itemize}
  \item On the dominance tree, possible to scan from top and assign colors to 
    variables as they come.
  \item Biased coloring is used to coalesce variables linked by \phifuns.
  \item see next section for more involved coalescing
  \item Fab: I think that you already talked about scan coloring. Ok to talk about biased.
  \item Fab: You can talk about biased coloring that uses the result of an aggressive coalescing (see with Quentin). It is not more complicated and it improves results.
\end{itemize}



\section{Practical and advanced discussions}

\subsection{Handling registers constraints}
\begin{itemize}
  \item ABI contraints
  \item split beforehand solution: many parallel copies, need good biased 
    coloring
  \item repair afterwards solution: biased try to give right color, else, 
    actual copies are added
  \item Fab: For the repair afterward at the time we will publish the book you will be able to cite the current paper we write on tree-scan coalescing. We plan to elaborate on the repairing mechanism and illustrate it in the context of tree-scan.
  \item Fab: For the repair afterward you will build an interference graph with negative 
affinity weights. Current coalescers do not handle it. A workaround (but 
costly) for affinity adge (a,b) with weight W<0 is replace it by an 
interference edge (a,ab) and an affinity edge (ab,b) of weight -W with ab a 
dummy node. For iterated reg-alloc (Briggs \& George's test) you can rewrite the different rules by doing as if you had such a dummy node (you virtualize it). You can discuss this with Quentin. Of course the idea is not to describe it here but it can be interesting to discuss a little bit the implications.
\end{itemize}


\subsection{Out-of-SSA and critical edge splitting}
\begin{itemize}
  \item \phifuns are not machine instructions, they are replaced by actual 
    copies in out-of-SSA phase
  \item in our case, colored SSA: Sreedhar not possible
  \item SSA implicitly 
    actual ``parallel copies'' are placed on edges
  \item problem with some critical edges: abnormal edges, back-edge of loop
  \item Fab: Sreedhar IS possible. It generates local variables not colored.
The point is that its coloring might be stucked (Chaitin reduction). So the possible need to split edges...
\end{itemize}


\subsection{More repairing on \phifuns}
\begin{itemize}
  \item Biased coloring is fast but not very good at coalescing
  \item Basic parallel copy motion: move to next edge.
  \item More involved problem: spill variables
  \item Better solution: look at biblio (parallel copy motion)
  \item Fab: Yes make it as simple as possible. Local pcopy motion works in practice. It is useless to go further. The ultimate solution (Sreedhar based) might have been discussed in the previous section
\end{itemize}


\endofchapter
}
