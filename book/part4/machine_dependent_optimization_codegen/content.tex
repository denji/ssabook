\chapter{SSA form and code generation \Author{B. Dupont de Dinechin}}
\inputprogress
\label{chapter:ssa-codegen}

In a compiler for imperative languages like C, C++, or FORTRAN, the code
generator covers the set of code transformations and optimizations that operate
on a program representation close to the target processor ISA, and produce an
assembly source or relocatable file with debugging information as result.

The main duties of code generation are: lowering the program intermediate
representation to the target processor instructions and calling conventions;
laying out data objects in sections and composing the stack frames; allocating
variable live ranges to architectural registers; scheduling instructions to
exploit micro-architecture; and producing assembly source or object code.

Precisely, the 1986 edition of
the ``Compilers Principles, Techniques, and Tools'' Dragon Book by Aho et al.
lists the tasks of code generation as:
\begin{itemize}
\item Instruction selection and calling conventions lowering.
\item Control flow (dominators, loops) and data flow (variable liveness) analyses.
\item Register allocation and stack frame building.
\item Peephole optimizations.
\end{itemize}
Ten years later, the 1997 textbook ``Advanced Compiler Design \& Implementation''
by Muchnich extends code generation with the following: \begin{itemize}
\item Loop unrolling and basic block replication.
\item Instruction scheduling and software pipelining.
\item Branch optimizations and basic block alignment.
\end{itemize}
In current releases of compilers such as the Open64 or GCC, code generation
techniques have significantly evolved, as they are mainly responsible for
exploiting the performance-oriented features of processor architectures and
micro-architectures. In these compilers, code generator optimizations
include: \begin{itemize}
\item If-conversion using SELECT, conditional move, or predicated, instructions.
\item Use of specialized addressing modes such as auto-modified and modulo.
\item Exploitation of hardware looping or static branch prediction hints.
\item Matching fixed-point arithmetic and SIMD idioms by special instructions.
\item Memory hierarchy optimizations, including pre-fetching and pre-loading.
\item VLIW instruction bundling, that may interfere with instruction
scheduling.
\end{itemize}

This increasing sophistication of compiler code generation motivates the
introduction of the SSA form in order to simplify analyses and optimizations.
However, engineering the SSA form on machine code raises a number of issues,
some of which we present in Section~\ref{sec:ssa-codegen-engineering}.  The
suitability of the SSA form for main code generation phases is discussed in
Section~\ref{sec:ssa-codegen-suitability}.

\section{SSA form engineering issues}
\label{sec:ssa-codegen-engineering}

\subsection{About instructions, operands, operations, and operators}

In this chapter, we adopt the following terminology. An \emph{instruction} is a
member of the processor Instruction Set Architecture (ISA). Instructions access
values and modify the processor state through \emph{operand}s. We distinguish
\emph{explicit operands}, which are associated with a specific bit-field in the
instruction encoding, from \emph{implicit operands}, without any encoding bits.
Explicit operands correspond to allocatable architectural registers, immediate
values, or instruction modifiers. Implicit operands correspond to single
instance architectural registers and to registers implicily used by some
instructions, such as the processor status register, the procedure link
register, or even the stack pointer.

An \emph{operation} is an instance of an instruction that compose a program. It
is seen by the compiler as an \emph{operator} applied to a list of operands
(explicit \& implicit), along with operand naming constraints, and has a set of
clobbered registers. The compiler view of operations also involves
\emph{indirect operands}, which are not apparent in the instruction behavior,
but are required to connect the flow of values between operations.  Implicit
operands correspond to the registers used for passing arguments and returning
results at function call sites, and may also be used for the registers
encoded in register mask immediates.

% of instructions such as PUSH/POP on the ARM.

\subsection{Representation of instruction semantics}

Unlike IR operators, there is no straightforward mapping between machine
instruction and their operational semantics. For instance, a subtract
with operands $(a,b,c)$ may either compute $c \leftarrow a-b$ or $c
\leftarrow b-a$ or any such expression with permuted operands. Yet basic SSA
form code cleanups such as constant propagation and sign extension removal need
to know what is actually computed by machine instructions.  Machine instructions
may also have multiple target operands, such as memory accesses with
auto-modified addressing, or combined division-modulus instructions.
There are at least two ways to address this issue. \begin{itemize}
\item Add properties to the instruction operator and to its operands, a
technique used by the Open64 compiler. Typical operation properties include
'isAdd', 'isLoad', etc. Typical operand properties include 'isLeft', 'isRigh',
'isBase', 'isOffset', 'isPredicated', etc. Extended properties that involve the
instruction operator and some of its operands include 'isAssociative',
'isCommutative', etc.
\item Associate a 'semantic combinator', that is, a tree of IR-like operators,
to each target operand of a machine instruction. This more ambitious alternative
was implemented in the SML/NJ \cite{Leung:1999:PLDI} compiler and the LAO
compiler \cite{Dinechin:2000:CASES}.  \end{itemize}

An issue related to the representation of instruction semantics is how to factor
it. Most information can be statically tabulated by the instruction operator,
yet properties such as safety for control speculation, or being equivalent to a
simple IR instruction, can be refined by the context where the instruction
appears. For instance, range propagation may ensure that an addition cannot
overflow, that a division by zero is impossible, or that a memory access is safe
for control speculation.  Alternate semantic combinators, or modifiers of the
instruction operator semantic combinator, need to be associated with each
machine instruction of the code generator internal representation.

Finally, code generation for some instruction set architectures require that
pseudo-instructions with known semantics be available, besides variants of
$\phi$-functions and parallel COPY instructions. \begin{itemize}

\item Machine instructions that operate on register pairs, such as the long
multiplies on the ARM, or more generally on register tuples, are common. In such
cases there is a need for pseudo-instructions to compose wide operands in
register tuples, and to extract independently register allocatable operands from
wide operands.

\item Embedded processor architectures such as the Tensilica Xtensa provide
hardware loops, where an implicit conditional branch back to the loop header is
taken whenever the program counter match some address. The implied loop-back
branch is also conveniently materialized by a pseudo-instruction.

\item Register allocation for predicated architectures need that the live-ranges
of pseudo-registers or SSA variables with predicated definitions be contained by
kill pseudo-instructions \cite{Gillies:1996:MICRO}.

\end{itemize}

\subsection{Operand naming constraints}

Implicit operands and indirect operands are constrained to specific
architectural registers either by the instruction set architecture (ISA
constraints), or by application binary interface (ABI constraints). An effective
way to deal with these constraints in the SSA form is by inserting parallel COPY
instruction that write to the constrained source operands or read from the
constrained target operands of the instructions. The new SSA variables thus
created are pre-colored with the required architectural register. The parallel
COPY operations are coalesced away or sequentialized when going out of SSA form
\cite{Boissinot:2009:CGO}.

Explicit instruction operands may also be constrained to use the same register
between a source and a target operand, or even to use different registers
between two source operands (MUL instructions on the ARM). Operand constraints
between one source and the target operand are the general case on popular
instruction set architectures such as x68 and the ARM~Thumb. In the setting of
\cite{Boissinot:2009:CGO}, these constraints are represented by inserting a COPY
between the constrained source operand and a new variable, then using this new
variable as the constrained source operand. The COPY is a parallel copy in case
of multiple constrained source operands. Again, the COPY operations are
processed when going out of SSA form.

A difficult case of ISA or ABI operand constraint is when a variable must be
bound to a specific register at all points in the program. This is the case of
the stack pointer, as interrupt handling may reuse the program run-time stack.
One possibility is to inhibit the promotion of the stack pointer to a SSA
variable. Stack pointer definitions including memory allocations through
\verb|alloca()|, activation frame creation/destruction, are then encapsulated in
a specific pseudo-instruction. Instructions that use the stack pointer must be
treated as special cases as far as the SSA form analyses and optimizations are
concerned.

\subsection{Non-kill target operands} \label{sec:non-kill-target}

The SSA form assumes that variable definitions are kills. This is not the case
for target operands such as a processor status register that contains several
independent bit-fields. Moreover, some instruction effects on bit-field may be
'sticky', that is, with an implied OR with the previous value. Typical sticky
bits include exception flags of the IEEE~754 arithmetic, or the integer overflow
flag on DSPs with fixed-point arithmetic. When mapping a processor status
register to a SSA variable, any operation that partially reads or modifies the
register bit-fields should appear as reading and writing the corresponding
variable.

Predicated execution and conditional execution are the other main source of
definitions that do not kill the target register. The execution of predicated
instructions is guarded by the evaluation of a single bit operand. The execution
of conditional instructions is guarded by the evaluation of a condition on a
multi-bit operand. We extend the classification of \cite{Mahlke:1995:ISCA} and
distinguish four classes of ISA: \begin{description}

\item[\bf Partial predicated execution support] SELECT instructions like those
of the Multiflow TRACE architecture \cite{Colwell:1987:ASPLOS} are provided. The
Multiflow TRACE 500 architecture was to include predicated store and
floating-point instructions \cite{Lowney:1993:JS}.

\item[\bf Full predicated execution support] Most instructions accept a Boolean
predicate operand which nullifies the instruction effects if the predicate
evaluates to false. EPIC-style architectures also provide predicate define
instructions (PDIs) to efficiently evaluate predicates corresponding to nested
conditions: Unconditional, Conditional, parallel-OR, parallel-AND
\cite{Gillies:1996:MICRO}.

\item[\bf Partial conditional execution support] Conditional move (CMOV)
instructions similar to those introduced in the Alpha AXP architecture
\cite{Blickstein:1992:DTJ} are provided. CMOV instructions are available in the
ia32 ISA since the Pentium Pro.

\item[\bf Full conditional execution support] Most instructions are
conditionally executed depending on the evaluation of a condition of a source
operand. On the ARM architecture, the implicit source operand is a bit-field in
the processor status register and the condition is encoded on 4 bits. On the
VelociTI{\texttrademark} TMS230C6xxx architecture, the source operand is a
general register encoded on 3 bits and the condition is encoded on 1 bit.

\end{description}
Extensions of the SSA form such as the $\psi$-SSA \cite{Stoutchinin:2001:MICRO}
presented in Chapter~\ref{chapter:psi_ssa} specifically address the handling
operations that do not kill the target operand because of predicated or
conditional execution.

Observe however that under the SSA form, a CMOV is equivalent to a SELECT with a
'must be same register' naming constraint between one source and the target
operand. Unlike other predicated or conditional instructions, the SELECT
instructions kill the target register.  Generalizing this observation provides a
simple way to handle predicated or conditional execution in vanilla SSA form:
\begin{itemize}

\item For each target operand of the predicated or conditional instruction, add
a corresponding source operand in the instruction signature.

\item For each added source operand, add a 'must be same same register' naming
constraint with the corresponding target operand.

\end{itemize}
This simple transformation enables SSA form analyses and optimizations to remain
oblivious to predicated and conditional code. The drawback of this solution is
that non-kill definitions of a given variable (before SSA renaming) remain in
dominance order across transformations, as opposed to $\psi$-SSA where predicate
value analysis may enable to relax this order.

%\subsection{Machine description system}
%
%\begin{itemize}
%
%\item Operand constraints
%
%\item Expose implicit operands
%
%\item Instruction semantics, also include speculability
%
%\item Hardware loop pseudo instructions
%
%\end{itemize}

%\subsection{Un-splittable critical edges}
%
%In a compiler code generator, the sequential layout of basic blocks is a
%structural component of the program representation, is the target of specific
%optimizations, and may constrained by several factors, including exception
%handling ranges in compilers such as the Open64.  A number of SSA form
%optimizations require that critical edges be split, however this is not possible
%with hardware loop back edges.
%
% CHECK! Do critical edges really have to be split?
% What about predicated code for critical edge splitting?
% What is the coalescing that Sreedhar can do and Chaintin cannot?

\subsection{Program representation invariants}

% PHIs with memory operands (stack slots), also need parallel spill instructions
% Enable to coalesce memory slots

Enginering a code generator requires decisions about what information is
transient, or belongs to the invariants of the program representation.  By
invariant we mean a property which is ensured before and after each phase.
Transient information is recomputed as needed by some phases from the program
representation invariants.  The applicability of the SSA form only spans the
early phases of the code generation process: from instruction selection, down to
register allocation.  After register allocation, program variables are mapped to
architectural registers or to memory locations, so the SSA form analyses and
optimizations no longer apply. In addition, a program may be only partially
converted to the SSA form. This motivates the engineering of the SSA form
as extensions to a baseline code generator program representation.

Some extensions to the program representation required by the SSA form are
better engineered as invariants, in particular for operands, operations, basic
blocks, and control flow graph.  Operands which are SSA variables need to record
the unique operation that defines them as target operand, and possibly to
maintain the list of where they appear as source operands. Operations such as
$\phi$-functions, $\sigma$-functions of the SSI form \cite{BoissinotBDR12}, and
parallel copies, may appear as regular operations constrained to specific places
in the basic blocks. The incoming arcs of basic blocks need also be kept in the
same order as the source operands of each of its $\phi$-functions.

A program representation invariant that impacts SSA form engineering is the
structure of loops. The modern way of identifying loops in a CFG is the
construction of a loop nesting forest as defined by Ramalingam
\cite{ramalingam:loopforest}. Non-reducible control flow allow for different
loop nesting forest for a given CFG, yet high-level information such as
loop-carried memory dependences, or user-level loop annotations, are provided to
the code generator. This information is attached to a loop structure, which thus
becomes an invariant. The impact on the SSA form is that some loop nesting
forests, such as the Havlak \cite{havlak:loop} loop structure, are better than
others for key analyses such as SSA variable liveness
\cite{Boissinot:2011:APLAS}.

Up-to-date live-in and live-out sets at basic block boundaries are also
candidates for being program representation invariants. However, when using and
updating liveness information under the SSA form, it appears convenient to
distinguish the $\phi$-function contributions from the results of dataflow
fixpoint computation.  In particular, Sreedhar et al.  \cite{SreedharSep99}
introduced the $\phi$-function semantics that became later known as
\emph{multiplexing mode} (see
Chapter~\ref{chapter:alternative_ssa_destruction_algorithm}), where a
$\phi$-function $B_0:a_0=\phi(B_1:a_1,\dots,B_n:a_n)$ makes $a_0$ live-in of
basic block $B_0$, and $a_1,\dots a_n$ live-out of basic blocks $B_1,\dots B_n$.
The classic basic block invariants $\hbox{LiveIn}(B)$ and $\hbox{LiveOut}(B)$
are then complemented with $\hbox{PhiDefs}(B)$ and $\hbox{PhiUses}(B)$
\cite{Boissinot:2011:APLAS}.

Finally, we know one compiler who adopted the invariant that the SSA form be
conventional across the code generation phases. This approach was motivated in
part by the fact that some classic optimizations such as SSAPRE \cite{CCK+97}
require that the 'live ranges of different versions of $h$ [an arbitrary SSA
variable] do not overlap', implying the SSA form is conventional. Other
compilers that use SSA numbers and omit the $\phi$-functions from the program
representation \cite{Lapkowski:1996:CASCON} are similary constrained. Work by
Sreedhar et al. \cite{SreedharSep99} makes it clear how to make the transformed
SSA form conventional wherever required, so there is no reason nowadays for this
property to be an invariant.


%More generally, the code generator intermediate representation should allow
%that machine instruction operands mix SSA variables, ordinary compiler
%temporaries, and possibly architectural registers.


\section{Code generation phases and the SSA form}
\label{sec:ssa-codegen-suitability}

\subsection{Instruction selection}

Instruction selection (Chapter~\ref{chapter:code_selection}). Unlike
classic techniques that match one IR tree or one DAG at a time, using the SSA
form as input extends the scope of pattern matching to more complex IR graphs,
in particular those resulting from control flow constructs.

\subsection{If-conversion}

If-conversion refers to optimization techniques that remove conditional branches
from a program region. The basic idea is to replace conditional branches by
straight-line computations that directly use the condition as a source operand.
The scope and effectiveness of if-conversion depends on ISA support for
predicated execution \cite{Mahlke:1995:ISCA}. In principle, any if-conversion
technique targeted to full predicated or conditional execution support may be
adapted to partial predicated or conditional execution support. For instance,
non-predicated instructions with side-effects such as memory accesses can be
used in combination with SELECT to provide a harmless effective address in case
the operation must be nullified \cite{Mahlke:1995:ISCA}.

Besides predicated execution, architectural support for if-conversion can be
improved by supporting speculative execution. Speculative execution (control
speculation) refers to executing an operation before knowing that its execution
is required, such as when moving code above a branch \cite{Lowney:1993:JS} or
promoting predicates \cite{Mahlke:1995:ISCA}.  Speculative execution assumes
instructions have reversible side effects, so speculating potentially excepting
instructions requires architectural support. On the Multiflow TRACE 300
architecture and later on the Lx VLIW architecture \cite{Faraboschi:2000:ISCA},
non-trapping memory loads known as 'dismissible' are provided. The IMPACT EPIC
architecture speculative execution \cite{August:1998:ISCA} is generalized from
the 'sentinel' model \cite{Mahlke:1992:ASPLOS}.

If-conversion is primarily motivated by instruction scheduling on
instruction-level parallel processors \cite{Mahlke:1995:ISCA}, as removing
conditional branches: \begin{itemize} \item eliminates branch resolution stalls
in the instruction pipeline, \item reduces uses of the branch unit, which is
often single-issue, \item increases the size of the instruction scheduling
regions.  \end{itemize} In case of inner loop bodies, if-conversion further
enables vectorization \cite{Allen:1983:POPL} and software pipelining (modulo
scheduling) \cite{Park:1991:TR58}. Consequently, control flow regions selected
for if-conversion are acyclic, even though seminal techniques
\cite{Allen:1983:POPL, Park:1991:TR58} consider more general control flow.
The result of an if-converted region is typically a hyperblock, that is, a set
of predicated basic blocks in which control may only enter from the top, but may
exit from one or more locations \cite{Mahlke:1992:MICRO}.
\medskip

%The generic steps of if-conversion are \cite{Fang:1996:LCPC}: region selection;
%code hoisting; assignment of predicates to basic blocks; insertion of
%operations to compute basic block predicates; predication or speculation of
%operations and conditional branch removal.

Classic contributions to if-conversion
dot not consider the SSA form: \begin{itemize}

%\item Allen et al. \cite{Allen:1983:POPL} describe how to convert control
%dependences to data dependences, motivated by inner loop vectorization. They
%distinguish forward branches, exit branches, and backward branches, and compute
%Boolean guards accordingly. As this work pre-dates the Program Dependence Graph
%\cite{Ferrante:1987:TOPLAS}, complexity of the resulting Boolean expressions is
%an issue. When comparing to later if-conversion techniques, only the conversion
%of forward branches is relevant.

\item Park \& Schlansker \cite{Park:1991:TR58} propose the RK algorithm based
the control dependences. They assume a fully predicated architecture wit only
Conditional PDIs. The R function assigns a minimal set of Boolean predicates to
basic blocks, and the K function express the way these predicates are computed.
The algorithm is general enough to process cyclic and irreducible rooted flow
graphs, but it practice it is applied to single entry acyclic regions.

\item Blickstein et al. \cite{Blickstein:1992:DTJ} pioneer the use of CMOV
instructions to replace conditional branches in the GEM compilers for the Alpha
AXP architecture.

\item Lowney et al. \cite{Lowney:1993:JS} match the innermost if-then constructs
in the Multiflow compiler in order to generate the SELECT and the predicated
memory store operations.

\item Fang \cite{Fang:1996:LCPC} assumes a fully predicated architecture with
Conditional PDIs. The proposed algorithm is tailored to acyclic regions with
single entry and multiple exits, and as such is able to compute R and K
functions without relying on explicit control dependences.  The main improvement
of this algorithm over \cite{Park:1991:TR58} is that it also speculates
instructions up the dominance tree through predicate promotion, except for
stores and PDIs. This work further proposes a pre-optimization pass to hoist or
sink common sub-expressions before predication and speculation.

\item Leupers \cite{Leupers:1999:DATE} focuses on if-conversion of nested
if-then-else (ITE) statements on architectures with full conditional execution
support. A dynamic programming technique appropriately selects either a
conditional jump or a conditional instruction based implementation scheme for
each ITE statement, and the objective is the reduction of worst-case execution
time (WCET).

\end{itemize} A few contributions to if-conversion only use the SSA form
internally: \begin{itemize}

\item Jacome et al. \cite{Jacome:2001:DAC} propose the Static Single Assignment
-- Predicated Switching (SSA-PS) transformation aimed at clustered VLIW
architectures, with predicated move instructions that operate inside clusters
(internal moves) or between clusters (external moves). The first idea of the
SSA-PS transformation is to realize the conditional assignments corresponding to
$\phi$-functions via predicated switching operations, in particular predicated
move operations. The second idea is that the predicated external moves leverage
the penalties associated with inter-cluster data transfers. The SSA-PS
transformation predicates non-move operations and is apparently restricted to
innermost if-then-else statements.

\item Chuang et al. \cite{Chuang:2003:CGO} introduce a predicated execution
support aimed at removing non-kill register writes from the processor
micro-architecture. They propose SELECT instructions called 'phi-ops',
predicated memory accesses, Unconditional PDIs, and ORP instructions for OR-ing
multiple predicates. A restriction of the RK algorithm to single-entry
single-exit regions is proposed, adapted to the Unconditional PDIs and the ORP
instructions. The other contribution from this work is the generation of
'phi-ops', whose insertion points are computed like the SSA form placement of the
$\phi$-functions. The $\phi$-functions source operands are replaced by
$\phi$-lists, where each operand is associated with the predicate of
its source basic block. The $\phi$-lists are processed by topological order of
the operand predicates to generate the 'phi-ops'.

\end{itemize} Recent contributions to if-conversion input SSA form,
and output $\psi$-SSA form: \begin{itemize}

\item Stoutchinin \& Ferri\`ere \cite{Stoutchinin:2001:MICRO} introduce
$\psi$-functions in order to bring predicated code under the SSA form for a
fully predicated architecture. The $\psi$-functions arguments are paired with
predicates and are ordered in dominance order, a correctness condition
re-discovered later by Chuang et al. \cite{Chuang:2003:CGO} for their phi-ops.

\item Stoutchinin \& Gao \cite{Stoutchinin:2004:EuroPar} propose an
if-conversion technique based on the predication of Fang \cite{Fang:1996:LCPC}
and the replacement of $\phi$-functions by $\psi$-functions. They prove the
conversion is correct provided the SSA form is conventional. The technique is
implemented in the Open64 for the ia64 architecture.

\item Bruel \cite{Bru06} targets VLIW architectures with SELECT and
dismissible load instructions. The proposed framework reduces acyclic
control flow constructs from innermost to outermost, and the monitoring of the
if-conversion benefits provides the stopping criterion. The core technique
control speculates operations, reduces height of predicate computations, and
performs tail duplication. It can also generate $\psi$-functions instead of
SELECT operations. A generalization of this framework, which also accepts
$\psi$-SSA form as input, is described in Chapter~\ref{chapter:if_conversion}.

\item Ferri\`ere \cite{Ferriere:2007:SCOPES} extends the $\psi$-SSA form
algorithms of \cite{Stoutchinin:2001:MICRO} to the partial predicated execution
support, by formulating simple correctness conditions for the predicate
promotion of operations that do not have side-effects. This work also details
how to transform the $\psi$-SSA form to conventional $\psi$-SSA by generating
CMOV operations. A self-contained explanation of these techniques appears in
Chapter~\ref{chapter:psi_ssa}.

\end{itemize}

Thanks to these two latter contributions, virtually all if-conversion techniques
formulated without the SSA form can be adapted to the $\psi$-SSA form, with the
added benefit that already predicated code can be part of input. The main issues
that remain for the selection of an if-conversion technique are the match with
the architectural support, and the approach to if-conversion region formation.

%\paragraph{Chuang:2003:CGO} The phi-predication is motivated by the removal of
%conditional register writes in the architecture. The proposed instruction set
%includes select instructions called phi-op, predicated load and stores with
%loads that always update destination register, wide OR between predicates,
%unconditional compares. The floating-point instructions propagates NaN.
%Arithmetic instructions are not predicated. Start from a non-SSA form program
%representation. Use a modified R-K algorithm on acyclic single-entry single-exit
%regions. The modification is for using unconditional compares and the ORP. Use
%the phi-version of memory operation on basic block with control dependences, and
%convert all compares to unconditional.  Other instructions, such as arithmetic
%or prefetch, are left unchanged. The insertion point of PHI operations is
%computed like the placement of the $\phi$-functions. However the source operands
%of $\phi$-functions are replaced by so-called $\phi$-lists, where each operand
%is associated with the predicate of its source basic block. The $\phi$-lists are
%processed by topological order of the operand predicates to generate the PHI
%operations. The phi-predication algorithm is not an SSA algorithm, however it is
%straightforward to adapt to convert conventional SSA form to $\psi$-SSA form.


% Park \& Schlansker RK algorithm:
% - Minimal in predicates and predicate defines
% - Compute control dependences
% - assign predicate to each unique value
% - insert predicate defines
% - remove control flow

% Open64
%   Algorithm description:
% 
%   The if-conversion algorithm that we will use will essentially be the
%   same as that used by Mahlke, et. al. in the IMPACT compiler.  This is
%   a variant of the RK algorithm from Park and Schlansker.  The algorithm
%   groups blocks into equivalence classes based on control dependence.
%   The blocks that are placed in the same equivalence class will be
%   controlled by the same predicate.  The predicates are set in those
%   predecessor blocks that are the source of the control dependences.
%   The algorithm, then, is roughly as follows:
%   
%       Foreach block in the region being if-converted
%          Determine predicate assignment for the block
%          Insert predicate calculations for the block
%   
%   Each of these actions will be described below.
%   
%   
%   Predicate Assignment
%   --------------------
%   
%   As was mentioned above, predicates are assigned to blocks based on
%   control dependence equivalence classes.  That is, any two blocks that
%   share precisely the same control dependences (same blocks, same edges)
%   can share the same predicate register.  The IMPACT compiler ignores
%   edges that exit the hyperblock when calculating control dependence
%   (this is the primary variation from Park and Schlansker).  Beyond
%   compile time, the benefits of this are unclear to me.  For the moment,
%   we will include the exiting arcs.  The algorithm for predicate assignment
%   for block X would be as follows:
%   
%       Calculate control dependences for X
%       If this set of control dependences has been seen before then
%         assign the associated predicate to X
%       else
%         get next predicate and associate it with X's control dependence set
%       endif
%       change ops in X to predicated form
%       remove branch if not an exit
%   
%   
%   Insert Predicate Calculations
%   -----------------------------
%   
%   Predicate calculations will have to be placed in each of the blocks that 
%   is a source of a control dependence in the equivalence class to which the
%   predicate is assigned.
% 
% 
%   Externally Visible routines:
% 
%       void HB_If_Convert(HB* hb, list<HB_CAND_TREE*>& candidate_regions)
%         Removes non-exit branches and predicates the instructions in
%         the blocks selected for the hyperblock. <candidate_regions> is
%         passed to incorporate any dynamic updates to original CFG.
% 
%       BOOL HB_Safe_For_If_Conversion(HB *hb)
%         Return TRUE if the hyperblock contains no side entrances. This
%         is used to screen out hyperblock candidates during the simple if-conversion
%         phase of the hyperblock formation.

\subsection{Inner loop optimizations}

Non-inner loop transformations, such as
unroll-and-jam, are usually left to the IR loop optimizer. Loop unrolling
replicates the loop body and removes all but one of the replicated loop exit
branches. Loop unwinding replicates the loop body while keeping the loop exit
branches. This style of loop unrolling necessarily applies to counted loops, and
requires that pre-conditioning or post-conditioning code be inserted
\cite{Lowney:1993:JS}. Inner loop unrolling or unwinding is facilitated by using
the loop-closed SSA form (Chapter~\ref{chapter:loop_tree}), the SSA version of the
self assignment technique also pioneered by the Multiflow Trace Scheduling
compiler \cite{Lowney:1993:JS}. A main motivation for inner loop unrolling is to
unlock modulo scheduling benefits \cite{Lavery:1995:MICRO}.

Induction variable classification (Chapter~\ref{chapter:loop_tree}),

%SIMD instruction reselection.

%Hardware loop mapping.

%\subsection{Memory addressing and memory packing}

Memory addressing optimizations include selection of auto-modified addressing
modes. Memory packing optimizations select wider memory access instructions
whenever the effective addresses are provably adjacent, and no side effects such
as possible address misalignment traps are introduced.

\subsection{Code cleanups and instruction re-selection}

Constant propagation, copy folding, and dead code elimination.

Bit-width analysis,

GVN or LICM or PRE

Instruction re-selection. Uses bit width analysis. SIMD idioms.
Required because analyses and code specialization

Reassociation


\subsection{Pre-pass instruction scheduling}

Further down the code generator, the next major phase is pre-pass instruction
scheduling. Innermost loops with a single basic block, super-block or
hyper-block body are candidates for software pipelining techniques such as
modulo scheduling. For innermost loops that are not software pipelined, and for
other program regions, acyclic instruction scheduling techniques apply: basic
block scheduling \cite{Goodman:1988:ICS}; super-block scheduling
\cite{Hwu:1993:JS}; hyper-block scheduling \cite{Mahlke:1992:MICRO}; tree region
scheduling \cite{Havanki:1998:HPCA}; or trace scheduling \cite{Lowney:1993:JS}.

By definition, pre-pass instruction scheduling operates before register
allocation. On a classic code generator, instruction operands are mostly virtual
registers, except for instructions with ISA or ABI constraints that binds them
to specific architectural registers. Moreover, preparation to pre-pass
instruction scheduling include virtual register renaming, also known as register
web construction, in order to reduce the number of anti dependences and output
dependences in the instruction scheduling problem. Other reasons why it seems
there is little to gain to schedule instructions on a SSA form program
representation include: \begin{itemize}

\item Except in case of trace scheduling which pre-dates the use of SSA form in
production compilers, the classic scheduling regions are single-entry and do not
have control flow merge. So there are no $\phi$-functions in case of acyclic
scheduling, and only $\phi$-functions in the loop header in case of software
pipelining. Keeping those $\phi$-functions in the scheduling problem has no
benefits and raises engineering issues, due to their parallel execution
semantics and the constraint to keep them first in basic blocks.

\item Instruction scheduling must account for all the instruction issue slots
required to execute a code region. If the only ordering constraints between
instructions, besides control dependences and memory dependences, are limited to
true data dependences on operands, code motion will create interferences that
must later be resolved by inserting COPY operations in the scheduled code
region. (Except for interferences created by the overlapping of live ranges
that results from modulo scheduling, as these are resolved by modulo renaming.)
So scheduling instructions with SSA variables as operands is not effective
unless extra dependences are added to the scheduling problem to prevent such
code motion. 

\item Some machine instructions have side effects on special resources such as
the processor status register. Representing these resources as SSA variables
even though they are not operated like the processor architectural registers
requires coarsening the instruction effects to the whole resource, as discussed
in Section~\ref{sec:non-kill-target}. In turn this implies def-use variable
ordering that prevents aggressive instruction scheduling. For instance, all
'sticky' bit-field definitions can be reordered with regards to the next use,
and an instruction scheduler is expected to do so. Scheduling OR-type predicate
define operations \cite{Schlansker:1999:PLDI} raises the same issues. An
instruction scheduler is also expected to precisely track accesses to unrelated
of partially overlapping bit-fields in a processor status register.

\item Aggressive instruction scheduling relaxes some flow data dependences that
are normally implied by SSA variable def-use ordering. A first example is 'move
renaming' \cite{Young:1998:MICRO}, the dynamic switching of the definition of a
source operand defined by a COPY operation when the consumer operations ends
up being scheduled at the same cycle or earlier. Another example is 'inductive
relaxation' \cite{Dinechin:1997:PaCT}, where the dependence between additive
induction variables and their use as base in base+offset addressing modes is
relaxed to the extent permitted by the induction step and the range of the
offset. These techniques apply to acyclic scheduling and to modulo scheduling.

\end{itemize}

To summarize, trying to keep the SSA form inside the pre-pass instruction scheduling
currently appears more complex than operating on the program representation with
classic compiler temporary variables. This representation is obtained from the
SSA form after aggressive coalescing and SSA destruction. If required by the
register allocation, the SSA form should be re-constructed.


\subsection{SSA form destruction}

The destruction of the SSA form in a code generator is required before the
pre-pass instruction scheduling and software pipelining, at least with the
established techniques, and also before classic register allocation such as
George \& Appel \cite{George96}. A weaker form is the conversion of transformed
SSA form to conventional, which is required by some register allocators
operating on the SSA form \cite{Pereira08}. For all these uses cases, the main
objective besides removing the SSA form extensions from the program
representation is to ensure that the ISA and ABI operand naming constraints are
satisfied. These objectives are met with aggressive coalescing.

A summary of the SSA form destruction techniques is that correctness issues of
the original out-of-SSA translation of Cytron et al. \cite{Cytron} were
addressed first. The focus then moved to the handling of ISA and
ABI operand naming constraints. Simultaneously, the time and space constraints of
JIT compilation motivated new approaches to SSA variable interference checks.
State-of-the-art algorithms such as those presented in
Chapter~\ref{chapter:alternative_ssa_destruction_algorithm} fix the remaining
correctness issues, handle all ISA and ABI operand naming constraints, support
aggressive coalescing, and can be adapted to JIT compilation constraints.
\begin{itemize}

\item Cytron et al. \cite{Cytron} describe the process of 'translating out of
SSA' as 'naive replacement preceded by dead code elimination and followed by
coloring' (graph coloring coalescing). The naive replacement replaces
each $\phi$-function $B_0:a_0=\phi(B_1:a_1,\dots,B_n:a_n)$ by $n$ copies $a_0 = a_i$,
one per basic block $B_i$.

\item Briggs et al. \cite{bib:briggs.ea-98} identifies correctness issues in
Cytron et al. translation out of (transformed) SSA form and illustrate them by
the 'lost-copy' and the 'swap' problems. These issues are related to the fact
that critical edges must be split, and that a sequence of $\phi$-functions at
the start of a basic block has parallel assignment semantics. Two SSA form
destruction algorithms are proposed, depending on the presence of critical edges
in the confrol-flow graph.

\item Sreedhar et al. \cite{SreedharSep99} 

\end{itemize}


\begin{itemize}

\item Budimli\'c et al. ``Fast Copy Coalescing and Live-Range Identification''
[PLDI'02]

\begin{itemize}
\item lightweight SSA destruction motivated by JIT compilation
\item use the SSA form dominance of definitions over uses to avoid explicit
interference graph
\item construct SSA-webs with early pruning of interfering variables, then
partition into non-interfering classes
\item introduce the ``dominance forest'' data-structure to avoid quadratic
number of interference tests
\item critical edge splitting is required
\end{itemize}

\item Sreedhar et al. ``Translating Out of Static Single Assignment Form''
[SAS'99] (US patent 6182284):

\begin{itemize}
\item Method~1 inserts COPY for the arguments of $\Phi$-functions in the
predecessors \emph{and} for the $\Phi$-functions targets in the current block,
then applies a new SSA-based coalescing algorithm
\item Method~3 maintains liveness and interference graph to insert COPY that will not
be removed by the new SSA-based coalescing algorithm
\item the new SSA-based coalescing algorithm is more effective than register allocation
coalescing
%\item Both methods yield significantly better code than Briggs et al. [SPE 28(8) 1998]
%or Budimli\'c et al. [PLDI'02]
\end{itemize}

\item Leung \& George ``Static Single Assignment Form for Machine Code'' [PLDI'99]

\begin{itemize}
\item handles the operand constraints of machine-level SSA form
\item builds on the algorithm by Briggs et al. [SPE 28(8) 1998]
\item Errors in missing parallel copies in case of W shaped CFG, the copies in
the pointy edge need to be in parallel
\end{itemize}

\item Rastello et al. ``Optimizing Translation Out of SSA using Renaming
Constraints'' [CGO'04] (STMicroelectronics)
\begin{itemize}
\item fix bugs and generalize Leung \& George [PLDI'99]
\item generalize Sreedhar et al. [SAS'99] (and avoid patent)
\end{itemize}

\end{itemize}


\begin{center} \underline{Sreedhar et al. [SAS~1999] Liveness and Congruence Example}
\end{center}

%\input{ssa-live.latex}

\begin{itemize}

\item variables ${x_1, x_3, x_3, y_1, y_2, y_3}$ are in the same congruence class

\item in this example, several interferences inside the congruence class

\end{itemize}


\begin{center} \underline{Insights of Sreedhar et al. [SAS~1999]}
\end{center}

\begin{itemize}

\item a $\Phi$-congruence class is the closure of the $\Phi$-connected relation

\item liveness under SSA form: $\Phi$ arguments are live-out of predecessor
blocks and $\Phi$ targets are live-in of $\Phi$ block

\item SSA form is \emph{conventional} if no two members of a $\Phi$-congruence
class interfere under this liveness

\item correct SSA destruction is the removal of $\Phi$-functions from
a conventional SSA form

\item after SSA construction (without COPY propagation), the SSA form is
conventional

\item Methods 1 -- 3 restore a conventional SSA form

\item the new SSA-based coalescing is able to coalesce interfering variables, as long
as the SSA form remains conventional

\end{itemize}

% virtual swap & lost copy: need parallel copies and critical arc splitting
% critical arc splitting required when folded copy and multiple edges from same
% predecessor

\subsection{Register allocation}

% brute-force is a conservative coalescing algorithm
% for low run-times, better brute-force followed by decoalescing
% see paperby Rastello, Guillon, Pereira for register aliasing

The last major phase of code generation where SSA form has demonstrated benefits
is register allocation and its three sub-problems: variable spilling, variable
coloring, and variable coalescing (Chapter~\ref{chapter:register_allocation}).

Issue with predicated or conditional instructions.
\cite{Eichenberger:1995:MICRO} \cite{Johnson:1996:MICRO}

Issue with aliased registers.

Pre conditioning to reduce MaxLive, saturation, sufficiency

