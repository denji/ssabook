\chapter{Introduction \Author{B. Dupont de Dinechin}}
\inputprogress

In a compiler for imperative languages like C, C++, or Fortran, the code
generator covers the set of code transformations and optimizations that operate
on a program representation close to the target processor ISA, and produce an
assembly source or relocatable file with debugging information as result.

The main duties of code generation are: lowering the program intermediate
representation to the target processor instruction and calling conventions;
allocating variable live ranges to architectural registers; scheduling
instructions to exploit micro-architecture; and producing assembly source or
object code.

Precisely, in the 1986 edition of
the ``Compilers Principles, Techniques, and Tools'' Dragon Book by Aho et al.,
the tasks of code generation are listed as:
\begin{itemize}
\item Instruction selection and calling conventions lowering.
\item Control-flow (dominators, loops) and data-flow (variable liveness) analyses.
\item Register allocation and stack frame building.
\item Peephole optimizations.
\end{itemize}
Ten years later, the 1997 textbook ``Advanced Compiler Design \& Implementation''
by Muchnich extends code generation with the following: \begin{itemize}
\item Loop unrolling and basic block replication.
\item Instruction scheduling and software pipelining.
\item Branch optimizations and basic block alignment.
\end{itemize}
In recent releases of compilers such as the Open64 or GCC, code generation
techniques have significantly evolved, as they are mainly responsible for
exploiting the performance-oriented features of processor architectures and
micro-architectures. Code generation techniques implement in these compilers
include: \begin{itemize}
\item If-conversion using conditional MOVE, SELECT, or predicated, instructions.
\item Use of specialized addressing modes such as auto-modified and modulo.
\item Exploitation of hardware looping instruction or static branch prediction
hints.
\item Matching fixed-point arithmetic and SIMD idioms by special instructions.
\item Memory hierarchy optimizations, including pre-fetching and pre-loading
data.
\item Bundling of VLIW instructions, that may interact with instruction
scheduling.
\end{itemize}
To our knowledge, there is no compilation textbook that is up to date with the
art code generation techniques as implemented in modern production compilers.
\medskip

\section{Uses of the SSA form in code generation}

This increasing sophistication of code generation techniques motivates the
introduction of SSA-form analyses and optimizations in the code generator in
order to simplify analyses and optimizations. However the applicability of the SSA
form does not spawn the whole code generation process. The phases that are known
to benefit from it appear early in the code generation process:
\begin{itemize}

\item Analyzes such as induction variable classification, bit-width analysis,
variable interference, liveness sets, and liveness checking
(Chapter~\ref{chap:liveness_analysis}).

\item Instruction selection (Chapter~\ref{chapter:code_selection}). Unlike
classic techniques that match one IR tree or one DAG at a time, using the SSA
form as input extends the scope of pattern matching to more complex IR graphs,
in particular those resulting from control-flow constructs.

\item If-conversion or instruction predication
(Chapter~\ref{chap:if_conversion}). A predicated instruction behavior includes a
traditional data processing part such as arithmetic or memory access, and a
predicate evaluation part often connected to a separate operand. When the
predicate evaluates to false, the instruction has no architectural effects. The
simplest form of predicated instruction is the conditional MOVE such as
CMOV\emph{xx} on the x86. 

\item Inner loop unrolling or unwinding (Chapter~\ref{chap19:loopTree}).

\item Memory addressing and memory packing optimizations.

\item Constant propagation, copy folding, and dead code elimination.

\item Instruction re-selection.

\end{itemize} \medskip

Further down the code generator, the next major phase is pre-pass instruction
scheduling. Innermost loops whose bodies are single basic block or super
block are candidates for software pipelining techniques such as modulo
scheduling. For innermost loops that are not software pipelined, and for other
program regions, acyclic instruction scheduling techniques apply: basic block
scheduling; super-block scheduling; hyper-block scheduling; tree block
scheduling; or trace scheduling.

By definition, pre-pass instruction scheduling operates before register
allocation. On a classic code generator, instruction operands are mostly virtual
registers, except for instructions with ISA or ABI constraints that binds them
to specific architectural registers. Moreover, preparation to pre-pass
instruction scheduling include virtual register renaming, also known as register
web construction, in order to reduce the number of anti dependences and output
dependences in the instruction scheduling problem. Other reasons why it seems
there is little to gain to schedule instructions on a SSA form program
representation include: \begin{itemize}

\item Except in case of trace scheduling, the scheduling regions considered
earlier are single-entry and do not have control-flow merge. So there are no
$\phi$-functions in case of acyclic scheduling, and only $\phi$-functions in the
loop header in case of software pipelining. Keeping those $\phi$-functions in
the scheduling problem has no benefits and raises engineering issues, due to
their parallel execution semantics and the constraint to keep them first in
basic blocks.

\item Instruction scheduling must account for all the instruction issue slots
required to execute a code region. If the only ordering constraints between
instructions, besides control dependences and memory dependences, are limited to
true data dependences on operands, code motion will create interferences that
must later be resolved by inserting COPY instruction in the scheduled code
region. (Except for interferences created by the overlapping of live ranges
that results from modulo scheduling, as these are resolved by modulo renaming.)
So scheduling instructions with SSA variables as operands is not effective
unless extra dependences are added to the scheduling problem to prevent such
code motion. 

\item Machine instructions may have side effects on stateful resources such as
the processor condition codes. Representing them as SSA variables even though
these resources are not operated like the processor architectural registers
requires special casing. For instance 'sticky' flags definitions of the IEEE~754
arithmetic imply a OR with the previous value. All such definitions can be
reordered with regards to the next use and an instruction scheduler is expected
to do so. However this type of flexibility is cumbersome to represent in the
SSA form.

\end{itemize} These issues need to be addressed before any pre-pass instruction
scheduling on the SSA form demonstrates advantages over classic approaches.
\medskip

The last major phase of code generation where SSA form has demonstrated benefits
is register allocation and its three sub-problems: variable spilling, variable
coloring, and variable coalescing (Chapter~\ref{chap:register_allocation}). After
the register allocation, program variables are mapped to architectural registers
or memory locations, so the SSA form analyses and optimizations no longer apply.

\section{Issues of the SSA form on machine instructions}

Applying the SSA form to a program representation at the level of machine
instructions raises a number issues that must be correctly addressed:
\begin{itemize}

\item In compiler code generator, the sequential layout of basic blocks is a key
component of the program representation, is the target of specific
optimizations, and is constrained by several factors, including exception
handling ranges in compilers such as the Open64. A number of embedded processor
architectures such as the Tensilica Xtensa provide hardware loops, where an
implicit conditional branch back to the loop header is taken whenever the
program counter match some address. A number of SSA form optimizations require
that critical edges be split, however this is not possible with hardware loop
back edges.

\item Machine instructions may have multiple result operands, such as memory
accesses with auto-modified addressing, or memory loads into register pairs.
More generally, instructions with a result in register tuple such as long
multiplies of the ARM imply multiple results as far as the SSA form is
concerned, whether on the instruction itself, or on a pseudo-instruction
instructions that extracts the individual components of the register tuple. So
there is no longer a one-to-one mapping between variable definitions and
instructions for the SSA form on machine instructions.

\item The SSA form analyses and optimizations assumes that variable definitions
are kills. As discussed earlier, this is not the case for condition registers
with sticky bit semantics. Besides sticky bits, any containment or overlap
between bit-fields abstracted as SSA variables, such as the condition codes of
the processor status register, implies aliasing between these variables that
must be worked around so their definition can be processed as kills.

\item Some variables, such as the stack pointer, must be bound to a specific
register at all points in the program, for instance if interrupt handlers reuse
the program run-time stack. One possibility for such case is to inhibit the
promotion of the stack pointer variable to a SSA variable. As a result, all
instructions that use the stack pointer must be specialized with respect to
their semantics as far as the SSA form optimizations are concerned.

\item Machine instruction operands may be constrained to specific architectural
registers, either by the instruction set architecture (ISA constraints), or by
the run-time software conventions such as the application binary interface (ABI
constraints). One way to deal with this problem is by inserting parallel COPY
instruction that write to the constrained operands or read from the constrained
results of the instructions. The new SSA variables thus created are pre-colored
with the required architectural register. The parallel COPY instructions are
coalesced away or sequentialized when going out of SSA.

\item Machine instruction operands may be constrained to use the same
architectural register between an argument and a result, or even to use
different architectural registers between two argument operands (MUL
instructions on the ARM). Operand constraints between one argument and the
result are the general case on popular instruction set architectures such as
x68 and the ARM~Thumb. These constraints are represented by inserting a COPY
between the constrained argument operand and a new variable, then using this new
variable as the constrained argument. The COPY is a parallel copy in case of
multiple constrained arguments. Again, the COPY instructions are processed when
going out of SSA.

\item Predicated instructions present another aspect of definitions that do not
kill the result register. Extensions such as $\psi$-SSA
(Chapter~\ref{chap:psi_ssa}) specifically address the issues of handling
predicated instructions in the SSA form. A simpler solution is as follows: for
each result operand of the predicated instruction, add a corresponding argument
operand in the instruction signature; then add an operand renaming constraint
between the result operands and the corresponding argument operands. This simple
transformation enables SSA form analyses and optimizations to remain oblivious
to predicated instructions; for instance, a conditional MOVE appears as a SELECT
instruction. The drawback of this solution is that predicated definitions of a
given variable (before SSA renaming) remain in dominance order across
transformations, as opposed to $\psi$-SSA where predicate analysis may enable to
relax this order.

\end{itemize}


\section{Engineering a SSA form code generator}

Machine description system

Choices of phi function semantics

Register allocate under SSA or not

%\section{Interference graphs on the SSA form}

%that enabling information such as pointer aliasing and memory dependence be
%available. The main stand-alone SSA-based analyses useful in a code generator
%are induction variable classification, and liveness analysis. Other

%Code generation techniques have significantly evolved in the past decades, as
%they are mainly responsible for exploiting increasingly sophisticated processor
%architectures and micro-architectures. Features to be addressed
%by the code generator include: selection of SIMD instructions; selection of
%conditional moves or predicated instructions; exploitation of hardware looping
%or branch prediction hints; exploitation of DSP-like addressing modes
%such as auto-modified; explicit memory hierarchy control, including pre-fetching
%of pre-loading data; exploitation of wide instruction issuing and instruction pipelining
%through instruction scheduling and

