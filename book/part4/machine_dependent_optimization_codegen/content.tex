\chapter{SSA form and code generation \Author{B. Dupont de Dinechin}}
\inputprogress
\label{chap:ssa-codegen}

In a compiler for imperative languages like C, C++, or Fortran, the code
generator covers the set of code transformations and optimizations that operate
on a program representation close to the target processor ISA, and produce an
assembly source or relocatable file with debugging information as result.

The main duties of code generation are: lowering the program intermediate
representation to the target processor instruction and calling conventions;
allocating variable live ranges to architectural registers; scheduling
instructions to exploit micro-architecture; and producing assembly source or
object code.

Precisely, in the 1986 edition of
the ``Compilers Principles, Techniques, and Tools'' Dragon Book by Aho et al.,
the tasks of code generation are listed as:
\begin{itemize}
\item Instruction selection and calling conventions lowering.
\item Control-flow (dominators, loops) and data-flow (variable liveness) analyses.
\item Register allocation and stack frame building.
\item Peephole optimizations.
\end{itemize}
Ten years later, the 1997 textbook ``Advanced Compiler Design \& Implementation''
by Muchnich extends code generation with the following: \begin{itemize}
\item Loop unrolling and basic block replication.
\item Instruction scheduling and software pipelining.
\item Branch optimizations and basic block alignment.
\end{itemize}
In recent releases of compilers such as the Open64 or GCC, code generation
techniques have significantly evolved, as they are mainly responsible for
exploiting the performance-oriented features of processor architectures and
micro-architectures. Code generation techniques implement in these compilers
include: \begin{itemize}
\item If-conversion using conditional move, SELECT, or predicated, instructions.
\item Use of specialized addressing modes such as auto-modified and modulo.
\item Exploitation of hardware looping instruction or static branch prediction
hints.
\item Matching fixed-point arithmetic and SIMD idioms by special instructions.
\item Memory hierarchy optimizations, including pre-fetching and pre-loading
data.
\item VLIW instruction bundling, that may interfere with instruction
scheduling.
\end{itemize}

This increasing sophistication of compiler code generation motivates the
introduction of the SSA-form in order to simplify analyses and optimizations.
However, using the SSA form on machine code raises issues, a number of
which we list in Section~\ref{sec:ssa-codegen-issues}.  Some options for
engineering a code generator with the SSA form as a first class representation
are presented in Section~\ref{sec:ssa-codegen-engineering}.  The suitability of
the SSA form for main code generation phases is discussed in
Section~\ref{sec:ssa-codegen-suitability}.

\section{SSA form on machine code issues}
\label{sec:ssa-codegen-issues}

\subsection{Representation of instruction semantics}

Unlike IR operators, there is no straightforward mapping between machine
instruction and their operational semantics. For instance, a substract
instruction with operands $(a,b,c)$ may either compute $c \leftarrow a-b$ or $c
\leftarrow b-a$ or any such expression with permuted operands. Yet basic SSA
form code cleanups such as constant propagation and sign extension removal need
to know what is actually computed by machine instructions.  Machine instructions
may also have multiple target operands, such as memory accesses with
auto-modified addressing, or combined division/modulus instructions.
There are at least two ways to address this issue. \begin{itemize}
\item Add properties to the instruction operation and to its operands, a
technique used by the Open64 compiler. Typical operation properties include
'isAdd', 'isLoad', etc. Typical operand properties include 'isLeft', 'isRigh',
'isBase', 'isOffset', 'isPredicated', etc. Extended properties that involve the
instruction operation and some of its operands include 'isAssociative',
'isCommutative', etc.
\item Associate a 'semantic combinator' \cite{Leung:1999:PLDI}, that is, a tree
of IR-like operators, to each target operand of a machine instruction. This more
ambitious alternative was implemented in the SML/NJ \cite{Leung:1999:PLDI}
compiler and the LAO compiler \cite{Dinechin:2000:CASES}.  \end{itemize}

An issue related to the representation of instruction semantics is how to factor
it. Most information can be statically tabulated by instruction opcode, yet
properties such as safety for control speculation, or being equivalent to a
simple IR instruction, can be refined by the context where the instruction
appears. For instance, range propagation may ensure that an addition cannot
overflow, that a division by zero is impossible, or that a memory access is safe.
Alternate semantic combinators, or modifiers of the instruction opcode semantic
combinator, need to be associated with each machine instruction of the code
generator internal representation.

Finally, code generation for some instruction set architectures require that
pseudo-instructions with known semantics be available, besides variants of
$\phi$-functions and parallel COPY instructions. \begin{itemize}
\item Machine instructions that
operate on register pairs, such as the long multiplies on the ARM, or more
generally on register tuples, are common. In such cases there is a need for
pseudo-instructions to compose wide operands in register tuples, and to extract
independently register allocatable operands from wide operands.
\item A number of embedded processor architectures such as the Tensilica Xtensa
provide hardware loops, where an implicit conditional branch back to the loop
header is taken whenever the program counter match some address. The implied
loop-back branch is also conveniently materialized by a pseudo-instruction.
\end{itemize}

\subsection{Operand naming constraints}

Explicit instruction operands are those associated with a non-opcode bit-field
in the instruction encoding. These operands correspond to allocatable
architectural registers or to immediate values. Implicit operands do not have
encoding bits, but their access is apparent in the instruction semantics. These
operands correspond to single instance architectural resources, such as a
hardware loop counter, a procedure link register, or a processor status
register. Indirect operands are not apparent in the instruction semantics, but
need to appear as instruction operands in order to connect the flow of values.
Such operands correspond to registers masks in instructions such PUSH/POP on the
ARM, and to the list of registers used for passing arguments are returning
results at function boundaries.

Implicit operands and indirect operands are constrained to specific
architectural registers either by the instruction set architecture (ISA
constraints), or by application binary interface (ABI constraints). An effective
way to deal with these constraints in the SSA form is by inserting parallel COPY
instruction that write to the constrained source operands or read from the
constrained target operands of the instructions. The new SSA variables thus
created are pre-colored with the required architectural register. The parallel
COPY instructions are coalesced away or sequentialized when going out of SSA
\cite{Boissinot:2009:CGO}.

Explicit instruction operands may also be constrained to use the same register
between a source and a target operand, or even to use different registers
between two source operands (MUL instructions on the ARM). Operand constraints
between one source and the target operand are the general case on popular
instruction set architectures such as x68 and the ARM~Thumb. In the setting of
\cite{Boissinot:2009:CGO}, these constraints are represented by inserting a COPY
between the constrained source operand and a new variable, then using this new
variable as the constrained source operand. The COPY is a parallel copy in case
of multiple constrained source operands. Again, the COPY instructions are
processed when going out of SSA.

A difficult case of ISA or ABI operand constraint is when a variable must be
bound to a specific register at all points in the program. This is the case of
the stack pointer, as interrupt handling may reuse the program run-time stack.
One possibility is to inhibit the promotion of the stack pointer to a SSA
variable. Stack pointer definitions including memory allocations through
\verb|alloca()|, activation frame creation/destruction, are then encapsulated in
a specific pseudo-instruction. Instructions that use the stack pointer must be
treated as special cases as far as the SSA form analyses and optimizations are
concerned.

\subsection{Non-kill target operands} \label{sec:non-kill-target}

The SSA form assumes that variable definitions are kills. This is not the case
for target operands such as a processor status register that contains several
independent bit-fields. Moreover, some instruction effects on bit-field may be
'sticky', that is, with an implied OR with the previous value. Typical sticky
bits include exception flags of the IEEE~754 arithmetic, or the integer overflow
flag on DSPs with fixed-point arithmetic. When mapping a processor status
register to a SSA variable, any instruction that partially reads or modifies the
register bit-fields should appear as reading and writing the corresponding
variable.

Predicated instructions present another aspect of definitions that do not kill
the target register. Extensions of the SSA form such as the $\psi$-SSA
(Chapter~\ref{chap:psi_ssa}) specifically address the issues of handling
predicated instructions. Basic architectural support for if-conversion is
provided by conditional move instructions and SELECT instructions
\cite{Mahlke:1995:ISCA}.  The former are predicated, while the latter always
write to their destination register. Observe that under the SSA form, a
conditional move instruction is equivalent to a SELECT instruction with a 'must
be same register' naming constraint between one source and the target operand.
Generalizing this observation provides a simple way to handle predicated
instructions: \begin{itemize}
\item For each target operand of the predicated instruction, add a corresponding
source operand in the instruction signature.
\item For each added source operand, add a 'must be same same register' renaming
constraint with the corresponding target operand.
\end{itemize}
This simple transformation enables SSA form analyses and optimizations to remain
oblivious to predicated instructions. The drawback of this solution is that
predicated definitions of a given variable (before SSA renaming) remain in
dominance order across transformations, as opposed to $\psi$-SSA where predicate
value analysis may enable to relax this order.

%\subsection{Un-splittable critical edges}
%
%In a compiler code generator, the sequential layout of basic blocks is a
%structural component of the program representation, is the target of specific
%optimizations, and may constrained by several factors, including exception
%handling ranges in compilers such as the Open64.  A number of SSA form
%optimizations require that critical edges be split, however this is not possible
%with hardware loop back edges.


\section{Engineering a SSA form code generator}
\label{sec:ssa-codegen-engineering}

%\subsection{Machine description system}
%
%\begin{itemize}
%
%\item Operand constraints
%
%\item Expose implicit operands
%
%\item Instruction semantics, also include speculability
%
%\item Hardware loop pseudo instructions
%
%\end{itemize}

\subsection{Control tree flavor}

\begin{itemize}

\item Control tree and memory dependencies and user hints

\item Control structures Bourdoncle Havlak.

\item Choice to use DJ graph or not

\item Loop nesting forest for abstract interpretation (under SSA)

\item Mismatch with the IR control tree

\end{itemize}

\subsection{SSA form variant}

\begin{itemize}

\item Mixes with non-SSA form or not

\item Choice to use multiplex or not

\item Choices of phi function semantics

\item Choice to keep conventional or not.

\item SSI flavor or not

\item SSA on stack slots for register allocation

\end{itemize}

%More generally, the code generator intermediate representation should allow
%that machine instruction operands mix SSA variables, ordinary compiler
%temporaries, and possibly architectural registers.

\subsection{SSA form construction}

\begin{itemize}

\item When is SSA form constructed

\item Copy folding or not

\end{itemize}

\subsection{SSA form destruction}

\begin{itemize}

\item Before, during, after register allocation

\item Sreedhar, Boissinot

\item Parallel copy sequentialization

\end{itemize}

\begin{itemize}

\item Region-based or not

\item Register allocate under SSA or not

\item Reassociation under SSA

\item Coalescing aggressiveness before instruction scheduling (dedicated etc.)

\end{itemize}


\section{Suitability of the SSA to code generation phases}
\label{sec:ssa-codegen-suitability}

The applicability of the SSA form only spans the phases that appear
early in the code generation process: from instruction selection, down to
register allocation. After register allocation, program variables are mapped to
architectural registers or to memory locations, so the SSA form analyses and
optimizations no longer apply.

\subsection{Classic analyses and optimizations}

\begin{itemize}

\item Induction variable classification (Chapter~\ref{chap:}),

\item Bit-width analysis,

\item liveness sets, liveness checking
(Chapter~\ref{chap:liveness_analysis}).

\item Variable interference,

\item GVN or LICM of PRE

\end{itemize}

\subsection{Instruction selection}

Instruction selection (Chapter~\ref{chapter:code_selection}). Unlike
classic techniques that match one IR tree or one DAG at a time, using the SSA
form as input extends the scope of pattern matching to more complex IR graphs,
in particular those resulting from control-flow constructs.

\subsection{If-conversion}

If-conversion refers to optimization techniques that remove conditional branches
from a program region. The basic idea is to replace conditional branches by
straight-line computations that directly use the condition as a source operand.
The scope and effectiveness of if-conversion depends on ISA support for predicated
execution \cite{Mahlke:1995:ISCA}: \begin{description}
\item[\bf No predicated execution support] The condition source operand is converted
to $\{0, -1, 1\}$ integer values and used in AND or MUL instructions.
\item[\bf Partial predicated execution support] The condition source operand is used
as selector in conditional move (CMOV) or SELECT instructions. The CMOVxx
instructions were introduced in the Alpha AXP architecture
\cite{Blickstein:1992:DTJ} and are available in the ia32 ISA since the Pentium Pro.
The SELECT instructions were introduced in the Multiflow TRACE 300 architecture
family.
\item[\bf Full predicated execution support] Most data processing instructions
accept a Boolean predicate operand which nullifies the instruction effects if the
predicate evaluates to false. Predicate define instructions (PDI) are also
provided to efficiently evaluate the predicate of nested conditions. Classic PDI
variants include Unconditional, OR-style, AND-style \cite{Mahlke:1995:ISCA}.
The ARM architecture can also be considered as fully predicated, since most user
instructions are conditionally executed depending on the evaluation of a 4-bit
source operand.
\end{description}
Besides predicated execution, architectural support for if-conversion can be
improved by supporting speculative execution. Speculative execution refers to
the execution of instruction under a more general condition than prior to
optimization, which occurs as a result of speculative code motion
\cite{Lowney:1992:JS} or predicate promotion \cite{Mahlke:1995:ISCA}.
Speculative execution is problematic with instructions with non-reversible side
effects such as exceptions. On the Multiflow TRACE 300 architecture and later on
the Lx VLIW architecture \cite{Faraboschi:2000:ISCA}, non-trapping memory loads
known as 'dismissable' are provided. Further architectural support is provided
on the IMPACT EPIC architecture \cite{August:1998:ISCA} and more generally,
effective if-conversion techniques rely on predicated execution and speculative
execution.

If-conversion is primarily motivated by unlocking instruction scheduling on
instruction-level parallel processors \cite{Mahlke:1995:ISCA}, as removing
conditional branches: \begin{itemize} \item eliminates branch resolution stalls
in the instruction pipeline, \item reduces uses of the branch unit, which is
often single-issue, \item increases the size of the instruction scheduling
regions.  \end{itemize} In case of inner loop bodies, if-conversion further
enables vectorization \cite{Allen:1993:POPL} and software pipelining (modulo
scheduling) \cite{Park:1991:HPL58}. Consequently, control-flow regions selected
for if-conversion are acyclic, even though seminal if-conversion papers
\cite{Allen:1993:POPL, Park:1991:HPL58} consider more general control-flow.
The result of an if-converted region is typically a hyperblock, that is, a set
of predicated basic blocks in which control may only enter from the top, but may
exit from one or more locations \cite{Mahlke:1992:MICRO} (loop back edges are
considered exits).
\medskip

The generic steps of if-conversion are \cite{Fang:1996:LCPC}: region selection;
code hoisting; assignment of predicates to basic blocks; insertion of
instructions to compute basic block predicates; predication or speculation of
instructions and branch removal.

If-conversion outside the SSA Form:
\begin{itemize}

\item Park \& Schlansker ``On Predicated Execution'' [HPL-91-58 1991]
\begin{itemize}
\item 'R-K algorithm' generalizes the Cydrome if-conversion
\item operates on the control dependence graph
\end{itemize}

\item Fang ``Compiler Algorithms on If-Conversion, Speculative Predicates
Assignment and Predicated Code Optimizations'' [LCPC 1996]
\begin{itemize}
\item simple and effective if-conversion using (post) dominance
\item operates on acyclic SEME code regions
\end{itemize}

\item Chuang et al. ``Phi-Predication for Light-Weight If-Conversion'' [CGO
2003]
\begin{itemize}
\item generates SELECT operations (not SSA form if-conversion)
\end{itemize}

\end{itemize}

If-conversion Under SSA Form:
\begin{itemize}

\item Stoutchinin \& Gao ``If-Conversion in SSA Form'' [Euro-Par 2004]
\begin{itemize}
\item prove it is correct to replace $\Phi$-functions by $\Psi$-functions
in conventional SSA form
\item apply to Fang [LCPC 1996] for if-conversion under SSA form
\item implement in Open64 for IA64
\end{itemize}

\item Bruel ``If-Conversion SSA Framework for Partially Predicated VLIW
Architectures'' [ODES-4 2006]
\begin{itemize}
\item rework Muliflow-like if-conversion algorithm for SSA form
\item locally generate $\Psi$-functions for predicated LOAD \& STORE
\item production use in ST200 compilers (ST220, ST231, ST240)
\end{itemize}

\item Ferriere ``Improvements to the Psi-SSA Representation" [SCOPES 2007]

\end{itemize}


%\paragraph{Schlansker:1999:PLDI} Fully resolved predicates (FPRs)

%\paragraph{Jacome:2001:DAC} The idea of Static Single Assignment -- Predicated
%Switching transformation (SSA-PS) is to realize conditional assigmnents
%corresponding to $\phi$-functions by so-called predicated switching operations.
%This technique is aimed at clustered VLIW architectures, with predicated move
%instructions that operate inside clusters (internal moves) or between clusters
%(external moves).

%\paragraph{Chuang:2003:CGO} The phi-predication is motivated by the removal of
%conditional register writes in the architecture. The proposed instruction set
%includes select instructions called phi-op, predicated load and stores with
%loads that always update destination register, wide OR between predicates,
%unconditional compares. The floating-point instructions propagates NaN.
%Arithmetic instructions are not predicated. Start from a non-SSA from program
%representation. Use a modified R-K algorithm on acyclic single-entry single-exit
%regions. The modification is for using unconditional compares and the ORP. Use
%the phi-version of memory operation on basic block with control dependences, and
%convert all compares to unconditional.  Other instructions, such as arithmetic
%or prefetch, are left unchanged. The insertion point of PHI operations is
%computed like the placement of the $\phi$-functions. However the source operands
%of $\phi$-functions are replaced by so-called $\phi$-lists, where each operand
%is associated with the predicate of its source basic block. The $\phi$-lists are
%processed by topological order of the operand predicates to generate the PHI
%operations. The phi-predication algorithm is not an SSA algorithm, however it is
%straightforward to adapt to convert conventional SSA form to $\psi$-SSA form.


% Weak control regions, strong control regions

% Full predication support, partial predication support

% If-conversion includes predication and speculation

% Park \& Schlansker RK algorithm:
% - Minimal in predicates and predicate defines
% - Compute control dependences
% - assign predicate to each unique value
% - insert predicate defines
% - remove control-flow

% Open64
%   Algorithm description:
% 
%   The if-conversion algorithm that we will use will essentially be the
%   same as that used by Mahlke, et. al. in the IMPACT compiler.  This is
%   a variant of the RK algorithm from Park and Schlansker.  The algorithm
%   groups blocks into equivalence classes based on control dependence.
%   The blocks that are placed in the same equivalence class will be
%   controlled by the same predicate.  The predicates are set in those
%   predecessor blocks that are the source of the control dependences.
%   The algorithm, then, is roughly as follows:
%   
%       Foreach block in the region being if-converted
%          Determine predicate assignment for the block
%          Insert predicate calculations for the block
%   
%   Each of these actions will be described below.
%   
%   
%   Predicate Assignment
%   --------------------
%   
%   As was mentioned above, predicates are assigned to blocks based on
%   control dependence equivalence classes.  That is, any two blocks that
%   share precisely the same control dependences (same blocks, same edges)
%   can share the same predicate register.  The IMPACT compiler ignores
%   edges that exit the hyperblock when calculating control dependence
%   (this is the primary variation from Park and Schlansker).  Beyond
%   compile time, the benefits of this are unclear to me.  For the moment,
%   we will include the exiting arcs.  The algorithm for predicate assignment
%   for block X would be as follows:
%   
%       Calculate control dependences for X
%       If this set of control dependences has been seen before then
%         assign the associated predicate to X
%       else
%         get next predicate and associate it with X's control dependence set
%       endif
%       change ops in X to predicated form
%       remove branch if not an exit
%   
%   
%   Insert Predicate Calculations
%   -----------------------------
%   
%   Predicate calculations will have to be placed in each of the blocks that 
%   is a source of a control dependence in the equivalence class to which the
%   predicate is assigned.
% 
% 
%   Externally Visible routines:
% 
%       void HB_If_Convert(HB* hb, list<HB_CAND_TREE*>& candidate_regions)
%         Removes non-exit branches and predicates the instructions in
%         the blocks selected for the hyperblock. <candidate_regions> is
%         passed to incorporate any dynamic updates to original CFG.
% 
%       BOOL HB_Safe_For_If_Conversion(HB *hb)
%         Return TRUE if the hyperblock contains no side entrances. This
%         is used to screen out hyperblock candidates during the simple if-conversion
%         phase of the hyperblock formation.

\subsection{Inner loop optimizations}

Non-inner loop transformations, such as
unroll-and-jam, are usually left to the IR loop optimizer. Loop unrolling
replicates the loop body and removes all but one of the replicated loop exit
branches. Loop unwinding replicates the loop body while keeping the loop exit
branches. This style of loop unrolling necessarily applies to counted loops, and
requires that pre-conditioning or post-conditioning code be inserted
\cite{Lowney:1992:JS}. Inner loop unrolling or unwinding is facilitated by using
the loop-closed SSA form (Chapter~\ref{chap19:loopTree}), the SSA version of the
self assignment technique also pioneered by the Multiflow Trace Scheduling
compiler \cite{Lowney:1992:JS}. A main motivation for inner loop unrolling is to
unlock modulo scheduling benefits \cite{Lavery:1995:MICRO}.

%SIMD instruction reselection.

%Hardware loop mapping.

\subsection{Memory addressing and memory packing}

Memory addressing optimizations include selection of auto-modified addressing
modes. Memory packing optimizations select wider memory access instructions
whenever the effective addresses are provably adjacent, and no side effects such
as possible address misalignment traps are introduced.

\subsection{Code cleanups}

Constant propagation, copy folding, and dead code elimination.

\subsection{Instruction re-selection}

Instruction re-selection. Uses bit width analysis. SIMD idioms.
Required because analyses and code specialization

\subsection{Pre-pass instruction scheduling}

Further down the code generator, the next major phase is pre-pass instruction
scheduling. Innermost loops with a single basic block, super-block or
hyper-block body are candidates for software pipelining techniques such as
modulo scheduling. For innermost loops that are not software pipelined, and for
other program regions, acyclic instruction scheduling techniques apply: basic
block scheduling \cite{Goodman:1988:ICS}; super-block scheduling
\cite{Hwu:1993:JS}; hyper-block scheduling \cite{Mahlke:1992:MICRO}; tree region
scheduling \cite{Havanki:1998:HPCA}; or trace scheduling \cite{Lowney:1992:JS}.

By definition, pre-pass instruction scheduling operates before register
allocation. On a classic code generator, instruction operands are mostly virtual
registers, except for instructions with ISA or ABI constraints that binds them
to specific architectural registers. Moreover, preparation to pre-pass
instruction scheduling include virtual register renaming, also known as register
web construction, in order to reduce the number of anti dependences and output
dependences in the instruction scheduling problem. Other reasons why it seems
there is little to gain to schedule instructions on a SSA form program
representation include: \begin{itemize}

\item Except in case of trace scheduling which pre-dates the use of SSA form in
production compilers, the classic scheduling regions are single-entry and do not
have control-flow merge. So there are no $\phi$-functions in case of acyclic
scheduling, and only $\phi$-functions in the loop header in case of software
pipelining. Keeping those $\phi$-functions in the scheduling problem has no
benefits and raises engineering issues, due to their parallel execution
semantics and the constraint to keep them first in basic blocks.

\item Instruction scheduling must account for all the instruction issue slots
required to execute a code region. If the only ordering constraints between
instructions, besides control dependences and memory dependences, are limited to
true data dependences on operands, code motion will create interferences that
must later be resolved by inserting COPY instruction in the scheduled code
region. (Except for interferences created by the overlapping of live ranges
that results from modulo scheduling, as these are resolved by modulo renaming.)
So scheduling instructions with SSA variables as operands is not effective
unless extra dependences are added to the scheduling problem to prevent such
code motion. 

\item Some machine instructions have side effects on special resources such as
the processor status register. Representing these resources as SSA variables
even though they are not operated like the processor architectural registers
requires coarsening the instruction effects to the whole resource, as discussed
in Section~\ref{sec:non-kill-target}. In turn this implies def-use variable
ordering that prevents aggressive instruction scheduling. For instance, all
'sticky' bit-field definitions can be reordered with regards to the next use,
and an instruction scheduler is expected to do so. Scheduling OR-type predicate
define instructions \cite{Schlansker_1999_PLDI} raises the same issues. An
instruction scheduler is also expected to precisely track accesses to unrelated
of partially overlapping bit-fields in a processor status register.

\item Aggressive instruction scheduling relaxes some flow data dependencies that
are normally implied by SSA variable def-use ordering. A first example is 'move
renaming' \cite{Young:1998:MICRO}, the dynamic switching of the definition of a
source operand defined by a COPY instruction when the consumer instructions ends
up being scheduled at the same cycle or earlier. Another example is 'inductive
relaxation' \cite{Dinechin:1997:PaCT}, where the dependence between additive
induction variables and their use as base in base+offset addressing modes is
relaxed to the extent permitted by the induction step and the range of the
offset. These techniques apply to acyclic scheduling and to modulo scheduling.

\end{itemize}

To summarize, trying to keep the SSA form inside the pre-pass instruction scheduling
currently appears more complex than operating on the program representation with
classic compiler temporary variables. This representation is obtained from the
SSA form after aggressive coalescing and SSA destruction. If required by the
register allocation, the SSA form should be re-constructed.

\subsection{Register allocation}

The last major phase of code generation where SSA form has demonstrated benefits
is register allocation and its three sub-problems: variable spilling, variable
coloring, and variable coalescing (Chapter~\ref{chap:register_allocation}).

