\chapter{Introduction \Author{B. Dupont de Dinechin}}
\inputprogress
\label{chap:ssa-codegen}

In a compiler for imperative languages like C, C++, or Fortran, the code
generator covers the set of code transformations and optimizations that operate
on a program representation close to the target processor ISA, and produce an
assembly source or relocatable file with debugging information as result.

The main duties of code generation are: lowering the program intermediate
representation to the target processor instruction and calling conventions;
allocating variable live ranges to architectural registers; scheduling
instructions to exploit micro-architecture; and producing assembly source or
object code.

Precisely, in the 1986 edition of
the ``Compilers Principles, Techniques, and Tools'' Dragon Book by Aho et al.,
the tasks of code generation are listed as:
\begin{itemize}
\item Instruction selection and calling conventions lowering.
\item Control-flow (dominators, loops) and data-flow (variable liveness) analyses.
\item Register allocation and stack frame building.
\item Peephole optimizations.
\end{itemize}
Ten years later, the 1997 textbook ``Advanced Compiler Design \& Implementation''
by Muchnich extends code generation with the following: \begin{itemize}
\item Loop unrolling and basic block replication.
\item Instruction scheduling and software pipelining.
\item Branch optimizations and basic block alignment.
\end{itemize}
In recent releases of compilers such as the Open64 or GCC, code generation
techniques have significantly evolved, as they are mainly responsible for
exploiting the performance-oriented features of processor architectures and
micro-architectures. Code generation techniques implement in these compilers
include: \begin{itemize}
\item If-conversion using conditional MOVE, SELECT, or predicated, instructions.
\item Use of specialized addressing modes such as auto-modified and modulo.
\item Exploitation of hardware looping instruction or static branch prediction
hints.
\item Matching fixed-point arithmetic and SIMD idioms by special instructions.
\item Memory hierarchy optimizations, including pre-fetching and pre-loading
data.
\item VLIW instruction bundling, that may interfere with instruction
scheduling.
\end{itemize}

This increasing sophistication of compiler code generation motivates the
introduction of the SSA-form in order to simplify analyses and optimizations.
However, using the SSA form on machine instructions raises issues, a number of
wich we list in Section~\ref{sec:ssa-codegen-issues}.  Some options for
engineering a code generator with the SSA form as a first class representation
are presented in Section~\ref{sec:ssa-codegen-engineering}.  The suitability of
the SSA form for main code generation phases is discussed in
Section~\ref{sec:ssa-codegen-suitability}.

\section{Issues of the SSA form on machine instructions}
\label{sec:ssa-codegen-issues}

\subsection{Un-splittable critical edges}

In compiler code generator, the sequential layout of basic blocks is a
structural component of the program representation, is the target of specific
optimizations, and is constrained by several factors, including exception
handling ranges in compilers such as the Open64. A number of embedded processor
architectures such as the Tensilica Xtensa provide hardware loops, where an
implicit conditional branch back to the loop header is taken whenever the
program counter match some address. A number of SSA form optimizations require
that critical edges be split, however this is not possible with hardware loop
back edges.

\subsection{Instructions with multiple target operands}

Machine instructions may have multiple target operands, such as memory
accesses with auto-modified addressing, or memory loads into register pairs.
More generally, instructions with a register tuple target operand such as long
multiplies of the ARM imply multiple results as far as the SSA form is
concerned, whether on the instruction itself, or on a pseudo-instruction
instructions that extracts the individual components of the register tuple. So
there is no longer a one-to-one mapping between variable definitions and
instructions for the SSA form on machine instructions.

\subsection{Non-kill target operands}

The SSA form assumes that variable definitions are kills. This is not the case
for target operands such as a processor status register that contains several
independent bit-fields. Moreover, some instruction effects on bit-field may be
'sticky', that is, with an implied OR with the previous value. When mapping a
processor status register to a SSA variable, any instruction that partially
reads or modifies the register bit-fields should appear as reading and writing
the corresponding variable.

Predicated instructions present another aspect of definitions that do not kill
the target register. Extensions of the SSA form such as the $\psi$-SSA
(Chapter~\ref{chap:psi_ssa}) specifically address the issues of handling fully
predicated instructions. A simpler solution is as follows: for each target
operand of the predicated instruction, add a corresponding source operand in
the instruction signature; then add an operand renaming constraint between the
target operands and the corresponding source operands. This simple
transformation enables SSA form analyses and optimizations to remain oblivious
to predicated instructions; for instance, a conditional move appears as a
SELECT instruction \cite{Mahlke:1995:ISCA}. The drawback of this solution is
that predicated definitions of a given variable (before SSA renaming) remain in
dominance order across transformations, as opposed to $\psi$-SSA where
predicate analysis may enable to relax this order.

\subsection{Stack pointer and non-SSA operands}

Some variables must be bound to a specific register at all points in the
program. In particular this is the case of the stack pointer if interrupt
handlers reuse the program run-time stack. One possibility is to inhibit the
promotion of the stack pointer variable to a SSA variable. Stack pointer
definitions including memory allocations through \verb|alloca()|, activation
frame creation/destruction, are then encapsulated in a specific
pseudo-instruction. Instructions that use the stack pointer must be treated as
special cases as far as the SSA form analyses and optimizations are concerned.

More generally, the code generator intermediate representation should allow
that machine instruction operands mix SSA variables, ordinary compiler
temporaries, and possibly architectural registers.

\subsection{Operand naming constraints}

Explicit instruction operands are those associated with a non-opcode bit-field
in the instruction encoding. These operands correspond to architectural
registers or to immediate values. Implicit operands do not have encoding bits,
but their access is apparent in the instruction semantics. These operands
correspond to architectural resources with a single instance, such as a
hardware loop counter, a procedure link register, or a processor status
register. Indirect operands are not apparent in the instruction semantics, but
need to appear as instruction operands in order to connect the flow of values.
Such operands correspond to registers masks in instructions such PUSH/POP on
the ARM, and to the list of registers used for passing arguments are returning
results at function boundaries.

Implicit operands and indirect operands are constrained to specific
architectural registers either by the instruction set architecture (ISA
constraints), or by application binary interface (ABI constraints). One way to
deal with these constraints in the SSA form is by inserting parallel COPY
instruction that write to the constrained source operands or read from the
constrained target operands of the instructions. The new SSA variables thus
created are pre-colored with the required architectural register. The parallel
COPY instructions are coalesced away or sequentialized when going out of SSA.

Explicit instruction operands may also be constrained to use the same
architectural register between a source and a target operand, or even to use
different architectural registers between two source operands (MUL instructions
on the ARM). Operand constraints between one source and the target operand are
the general case on popular instruction set architectures such as x68 and the
ARM~Thumb. These constraints are represented by inserting a COPY between the
constrained source operand and a new variable, then using this new variable as
the constrained source operand. The COPY is a parallel copy in case of multiple
constrained source operands. Again, the COPY instructions are processed when
going out of SSA.

\subsection{Instruction semantics representation}

Unlike IR operators, there is no straightforward mapping between machine
instruction and their operational semantics. For instance, a substract
instruction with operands $(a,b,c)$ may either compute $c \leftarrow a-b$ or $c
\leftarrow b-a$ or any such expression with permuted operands. Yet basic SSA
form code cleanups such as constant propagation and sign extension removal need
to know what is actually computed by machine instructions. There are at least
two ways to address this issue. One is to add properties to the instruction
operation and to its operands, a technique used by the Open64 compiler. Typical
operation properties include 'isAdd', 'isLoad', etc. Typical operand properties
include 'isLeft', 'isRigh', 'isBase', 'isOffset', etc. A more ambitious
alternative implemented in the SML/NJ \cite{Leung:1999:PLDI} compiler and the
LAO compiler \cite{Dinechin:2000:CASES} is to associate a tree of IR-like
operators to each target operand of a machine instruction.

%Static vs run-time properties, speculability

% reassociation, induction variables

\section{Engineering a SSA form code generator}
\label{sec:ssa-codegen-engineering}

\subsection{Machine description system}

\begin{itemize}

\item Operand constraints

\item Expose implicit operands

\item Instruction semantics, also include speculability

\item Hardware loop pseudo instructions

\end{itemize}

\subsection{Control tree flavor}

\begin{itemize}

\item Control tree and memory dependencies and user hints

\item Control structures Bourdoncle Havlak.

\item Choice to use DJ graph or not

\item Loop nesting forest for abstract interpretation (under SSA)

\item Mismatch with the IR control tree

\end{itemize}

\subsection{SSA form variant}

\begin{itemize}

\item Mixes with non-SSA form or not

\item Choice to use multiplex or not

\item Choices of phi function semantics

\item Choice to keep conventional or not.

\item SSI flavor or not

\item SSA on stack slots for register allocation

\end{itemize}

\subsection{SSA form construction}

\begin{itemize}

\item When is SSA form constructed

\item Copy folding or not

\end{itemize}

\subsection{SSA form destruction}

\begin{itemize}

\item Before, during, after register allocation

\item Sreedhar, Boissinot

\item Parallel copy sequentialization

\end{itemize}

\begin{itemize}

\item Region-based or not

\item Register allocate under SSA or not

\item Reassociation under SSA

\item Coalescing aggressiveness before instruction scheduling (dedicated etc.)

\end{itemize}


\section{Suitability of the SSA to code generation phases}
\label{sec:ssa-codegen-suitability}

The applicability of the SSA form only spans the phases that appear
early in the code generation process: from instruction selection, down to
register allocation. After register allocation, program variables are mapped to
architectural registers or to memory locations, so the SSA form analyses and
optimizations no longer apply.

\subsection{Classic analyses and optimizations}

\begin{itemize}

\item Induction variable classification (Chapter~\ref{chap:}),

\item Bit-width analysis,

\item liveness sets, liveness checking
(Chapter~\ref{chap:liveness_analysis}).

\item Variable interference,

\item GVN or LICM of PRE

\end{itemize}

\subsection{Instruction selection}

Instruction selection (Chapter~\ref{chapter:code_selection}). Unlike
classic techniques that match one IR tree or one DAG at a time, using the SSA
form as input extends the scope of pattern matching to more complex IR graphs,
in particular those resulting from control-flow constructs.

\subsection{If-conversion or instruction predication}

(Chapter~\ref{chap:if_conversion}). A predicated instruction behavior includes a
traditional data processing part such as arithmetic or memory access, and a
predicate evaluation part often connected to a separate operand. When the
predicate evaluates to false, the instruction has no architectural effects. The
simplest form of predicated instruction is the conditional MOVE such as
CMOV\emph{xx} instructions on the x86. 

% Weak control regions, strong control regions

% Full predication support, partial predication support

% If-conversion includes predication and speculation

% Park \& Schlansker RK algorithm:
% - Minimal in predicates and predicate defines
% - Compute control dependences
% - assign predicate to each unique value
% - insert predicate defines
% - remove control-flow

% Open64
%   Algorithm description:
% 
%   The if-conversion algorithm that we will use will essentially be the
%   same as that used by Mahlke, et. al. in the IMPACT compiler.  This is
%   a variant of the RK algorithm from Park and Schlansker.  The algorithm
%   groups blocks into equivalence classes based on control dependence.
%   The blocks that are placed in the same equivalence class will be
%   controlled by the same predicate.  The predicates are set in those
%   predecessor blocks that are the source of the control dependences.
%   The algorithm, then, is roughly as follows:
%   
%       Foreach block in the region being if-converted
%          Determine predicate assignment for the block
%          Insert predicate calculations for the block
%   
%   Each of these actions will be described below.
%   
%   
%   Predicate Assignment
%   --------------------
%   
%   As was mentioned above, predicates are assigned to blocks based on
%   control dependence equivalence classes.  That is, any two blocks that
%   share precisely the same control dependences (same blocks, same edges)
%   can share the same predicate register.  The IMPACT compiler ignores
%   edges that exit the hyperblock when calculating control dependence
%   (this is the primary variation from Park and Schlansker).  Beyond
%   compile time, the benefits of this are unclear to me.  For the moment,
%   we will include the exiting arcs.  The algorithm for predicate assignment
%   for block X would be as follows:
%   
%       Calculate control dependences for X
%       If this set of control dependences has been seen before then
%         assign the associated predicate to X
%       else
%         get next predicate and associate it with X's control dependence set
%       endif
%       change ops in X to predicated form
%       remove branch if not an exit
%   
%   
%   Insert Predicate Calculations
%   -----------------------------
%   
%   Predicate calculations will have to be placed in each of the blocks that 
%   is a source of a control dependence in the equivalence class to which the
%   predicate is assigned.
% 
% 
%   Externally Visible routines:
% 
%       void HB_If_Convert(HB* hb, list<HB_CAND_TREE*>& candidate_regions)
%         Removes non-exit branches and predicates the instructions in
%         the blocks selected for the hyperblock. <candidate_regions> is
%         passed to incorporate any dynamic updates to original CFG.
% 
%       BOOL HB_Safe_For_If_Conversion(HB *hb)
%         Return TRUE if the hyperblock contains no side entrances. This
%         is used to screen out hyperblock candidates during the simple if-conversion
%         phase of the hyperblock formation.

\subsection{Inner loop optimizations}

Non-inner loop transformations, such as
unroll-and-jam, are usually left to the IR loop optimizer. Loop unrolling
replicates the loop body and removes all but one of the replicated loop exit
branches. Loop unwinding replicates the loop body while keeping the loop exit
branches. This style of loop unrolling necessarily applies to counted loops, and
requires that pre-conditioning or post-conditioning code be inserted
\cite{Lownet:1992:JS}. Inner loop unrolling or unwinding is facilitated by using
the loop-closed SSA form (Chapter~\ref{chap19:loopTree}), the SSA version of the
self assignment technique also pioneered by the Multiflow Trace Scheduling
compiler \cite{Lownet:1992:JS}.

Hardware loop mapping.

\subsection{Memory addressing and memory packing}

Memory addressing optimizations include selection of auto-modified addressing
modes. Memory packing optimizations select wider memory access instructions
whenever the effective addresses are provably adjacent, and no side effects such
as possible address misalignment traps are introduced.

\subsection{Code cleanups}

Constant propagation, copy folding, and dead code elimination.

\subsection{Instruction re-selection}

Instruction re-selection. Uses bit width analysis. SIMD idioms.
Required because analyses and code specialization

\subsection{Pre-pass instruction scheduling}

Further down the code generator, the next major phase is pre-pass instruction
scheduling. Innermost loops with a single basic block or super block body are
candidates for software pipelining techniques such as modulo scheduling. For
innermost loops that are not software pipelined, and for other program regions,
acyclic instruction scheduling techniques apply: basic block scheduling;
super-block scheduling; hyper-block scheduling; tree block scheduling; or trace
scheduling.

By definition, pre-pass instruction scheduling operates before register
allocation. On a classic code generator, instruction operands are mostly virtual
registers, except for instructions with ISA or ABI constraints that binds them
to specific architectural registers. Moreover, preparation to pre-pass
instruction scheduling include virtual register renaming, also known as register
web construction, in order to reduce the number of anti dependences and output
dependences in the instruction scheduling problem. Other reasons why it seems
there is little to gain to schedule instructions on a SSA form program
representation include: \begin{itemize}

\item Except in case of trace scheduling, the scheduling regions considered
earlier are single-entry and do not have control-flow merge. So there are no
$\phi$-functions in case of acyclic scheduling, and only $\phi$-functions in the
loop header in case of software pipelining. Keeping those $\phi$-functions in
the scheduling problem has no benefits and raises engineering issues, due to
their parallel execution semantics and the constraint to keep them first in
basic blocks.

\item Instruction scheduling must account for all the instruction issue slots
required to execute a code region. If the only ordering constraints between
instructions, besides control dependences and memory dependences, are limited to
true data dependences on operands, code motion will create interferences that
must later be resolved by inserting COPY instruction in the scheduled code
region. (Except for interferences created by the overlapping of live ranges
that results from modulo scheduling, as these are resolved by modulo renaming.)
So scheduling instructions with SSA variables as operands is not effective
unless extra dependences are added to the scheduling problem to prevent such
code motion. 

\item Machine instructions may have side effects on stateful resources such as
the processor condition codes. Representing them as SSA variables even though
they are not operated like the processor architectural registers requires
special casing. For instance, 'sticky' exception flags of the IEEE~754
arithmetic imply a OR with the previous value. All such definitions can be
reordered with regards to the next use, and an instruction scheduler is expected
to do so. The SSA form on machine instructions ordered by def-use on variables
do not expose this flexibility.

\item Move effects around COPY(ies) 

\end{itemize} These issues need to be addressed before any pre-pass instruction
scheduling on the SSA form demonstrates advantages over classic approaches.

\subsection{Register allocation}

The last major phase of code generation where SSA form has demonstrated benefits
is register allocation and its three sub-problems: variable spilling, variable
coloring, and variable coalescing (Chapter~\ref{chap:register_allocation}).

