\chapter{Introduction \Author{B. Dupont de Dinechin}}
\inputprogress

In a compiler for imperative languages like C, C++, or Fortran, the code
generator covers the set of code transformations and optimizations that operate
on a program representation close to the target processor ISA, and produce an
assembly source or relocatable file with debugging information as result.

The main duties of code generation are: lowering the program intermediate
representation to the target processor instruction and calling conventions;
allocating variable live ranges to architectural registers; scheduling
instructions to exploit micro-architecture; and producing assembly source or
object code.

Precisely, in the 1986 edition of
the ``Compilers Principles, Techniques, and Tools'' Dragon Book by Aho et al.,
the tasks of code generation are listed as:
\begin{itemize}
\item Instruction selection and calling conventions lowering.
\item Control-flow (dominators, loops) and data-flow (variable liveness) analyses.
\item Register allocation and stack frame building.
\item Peephole optimizations.
\end{itemize}
Ten years later, the 1997 textbook ``Advanced Compiler Design \& Implementation''
by Muchnich extends code generation with the following: \begin{itemize}
\item Loop unrolling and basic block replication.
\item Instruction scheduling and software pipelining.
\item Branch optimizations and basic block alignment.
\end{itemize}
In recent releases of compilers such as the Open64 or GCC, code generation
techniques have significantly evolved, as they are mainly responsible for
exploiting the performance-oriented features of processor architectures and
micro-architectures. Code generation techniques implement in these compilers
include: \begin{itemize}
\item If-conversion using conditional MOVE, SELECT, or predicated, instructions.
\item Use of specialized addressing modes such as auto-modified and modulo.
\item Exploitation of hardware looping instruction or static branch prediction
hints.
\item Matching fixed-point arithmetic and SIMD idioms by special instructions.
\item Memory hierarchy optimizations, including pre-fetching and pre-loading
data.
\item Bundling of VLIW instructions, that may interact with instruction
scheduling.
\end{itemize}
To our knowledge, there is no compilation textbook that is up to date with the
art code generation techniques as implemented in modern production compilers.
\medskip

\section{Uses of the SSA form in code generation}

This increasing sophistication of code generation techniques motivates the
introduction of SSA-form analyses and optimizations in the code generator in
order to simplify analyses and optimizations. However the applicability of the SSA
form does not spawn the whole code generation process. The phases that are known
to benefit from it appear early in the code generation process:
\begin{itemize}

\item Analyzes such as induction variable classification, bit-width analysis,
liveness sets, and liveness checking (Chapter~\ref{chap:liveness_analysis}).

\item Instruction selection (Chapter~\ref{chapter:code_selection}). Unlike
classic techniques that match one IR tree or one DAG at a time, using the SSA
form as input extends the scope of pattern matching to more complex IR graphs,
in particular those resulting from control-flow constructs.

\item If-conversion or instruction predication (Chapter~\ref{chap:if_conversion})

\item Inner loop unrolling or unwinding (Chapter~\ref{chap19:loopTree}).

\item Memory addressing and memory packing optimizations.

\item Constant propagation, copy folding, and dead code elimination.

\item Instruction re-selection.

\end{itemize}

Further down the code generator, the next major phase is pre-pass instruction
scheduling. Innermost loops whose bodies are single basic block or super
block are candidates for software pipelining techniques such as modulo
scheduling. For innermost loops that are not software pipelined, and for other
program regions, acyclic instruction scheduling techniques apply: basic block
scheduling; super-block scheduling; hyper-block scheduling; tree block
scheduling; or trace scheduling.

By definition, pre-pass instruction scheduling operates before register
allocation. On a classic code generator, instruction operands are mostly virtual
registers, except for instructions with ISA or ABI constraints that binds them
to specific architectural registers. Moreover, preparation to pre-pass
instruction scheduling include virtual register renaming, also known as register
web construction, in order to reduce the number of anti dependences and output
dependences in the instruction scheduling problem. As a result, there is little
to gain to schedule instructions on a SSA form program representation:
\begin{itemize}

\item Except in case of trace scheduling, the scheduling regions considered are
single-entry and do not have control-flow merge. So there are no
$\phi$-functions in case of acyclic scheduling, and only $\phi$-functions in the
loop header in case of software pipelining. Keeping those $\phi$-functions in
the scheduling problem has no benefits and raises engineering issues, due to
their peculiar semantics (parallel execution, must be first in basic block).

\item Instruction scheduling must account for all the instruction issue slots
required to execute a code region. If the only ordering constraints between
instructions, besides control dependences and memory dependences, are limited to
true data dependences on operands, code motion will create interferences that
must later be resolved by inserting COPY instruction in the scheduled code
region. (Except for interferences created by the overlapping of live ranges
that results from modulo scheduling, as these are resolved by modulo renaming.)
So scheduling instructions with SSA variables as operands is not effective
unless extra dependences are added to the scheduling problem to prevent such
code motion. 

\end{itemize} These issues need to be addressed before any pre-pass instruction
scheduling on the SSA form demonstrates advantages over classic approaches.

The last major phase of code generation where SSA form has demonstrated benefits
is register allocation and its three sub-problems: variable spilling, variable
coloring, and variable coalescing (Chapter~\ref{chap:register_allocation}). After
the register allocation, program variables are mapped to architectural registers
or memory locations, so the SSA form analyses and optimizations no longer apply.

\section{Pragmatics of the SSA form on machine code}

2-operands, predicated, phi-sets, multiple outputs, operand pinning, variable
pinning

\section{Interference graphs on the SSA form}

%that enabling information such as pointer aliasing and memory dependence be
%available. The main stand-alone SSA-based analyses useful in a code generator
%are induction variable classification, and liveness analysis. Other

%Code generation techniques have significantly evolved in the past decades, as
%they are mainly responsible for exploiting increasingly sophisticated processor
%architectures and micro-architectures. Features to be addressed
%by the code generator include: selection of SIMD instructions; selection of
%conditional moves or predicated instructions; exploitation of hardware looping
%or branch prediction hints; exploitation of DSP-like addressing modes
%such as auto-modified; explicit memory hierarchy control, including pre-fetching
%of pre-loading data; exploitation of wide instruction issuing and instruction pipelining
%through instruction scheduling and

