 \chapter{If-Conversion \Author{C. Bruel}}\label{chapter:if_conversion}
\inputprogress
\label{chapter:if_conversion}
\graphicspath{{img/}{if_conversion/img/}{part4/if_conversion/img/}}

\newcommand{\annotation}[1]{%
  \marginpar{\small\itshape\color{red}#1}}

%\setcounter{tocdepth}{3} 
%\tableofcontents

% remember
% Check entry vs head in the examples
% example 21.9(b) x=psi (T ? x1, p bar ? x2)
% example 21.9(c) x=psi (T ? x1, p bar ? x2)

\section{Introduction}

Very Large Instruction Word (VLIW) or Explicitly Parallel Instruction Computing (EPIC) architectures make Instruction Level Parallelism (ILP) visible within the Instruction Set Architecture (ISA), relying on static schedulers to organize the compiler output such that multiple instructions can be issued in each cycle.
The imperative control flow representation of the program, inherently imposes an order of execution between instructions of different basic blocks. Yet, two different instructions, even ``connected'' by some control flow edges, could be executed in parallel. As explained in Chapter~\ref{chapter:vsdg}, this is the case whenever they are connected neither by a data nor a control dependence. 

If-conversion is the process of transforming a control flow region with conditional branches, into an equivalent predicated sequence of instructions (in a single basic-block). As for gated-SSA form (see Chapter~\ref{chapter:vsdg}), if-converted code replaces control dependencies by data dependencies, and thus exposes parallelism very naturally within the new merging basic-block. 
Removing branches improves performance in several ways: By removing the misprediction penalty, the instruction fetch throughput is increased and the instruction cache miss penalty reduced. Enlarging the size of basic blocks allows earlier execution of long latency operations and the merging of multiple control flow paths into a single flow of execution, than can later be exploited by scheduling frameworks such as VLIW scheduling, hyperblock scheduling or modulo scheduling.

Consider the simple example given in figure \ref{fig:example1}, that represents the execution of an \texttt{if-then-else-end} statement on a 4-issue processor. In this figure, $r=q\cond r_1:r_2$ stands for a select instruction where $r$ is assigned $r_1$ if $q$ is true, and $r_2$ otherwise. With standard basic-block ordering, assuming that all instructions have a one cycle latency, the schedule height goes from six cycles in the most optimistic case, to seven cycles. After if-conversion the execution path is reduced to six cycles with no branches, regardless of the test outcome, and assuming a very optimistic one cycle branch penalty. But the main benefit here is that it can be executed without branch disruption. 

\begin{figure}
  \subfloat[Control Flow] {
    \includegraphics[scale=0.9]{specul}
    \label{fig:orig}
  }
  \subfloat[with basic block ordering] {
    \includegraphics[scale=0.9]{specul-linear}
  }

  \subfloat[After if-conversion] {
    \includegraphics[scale=0.9]{specul-ifconverted}
  }
\caption{Static schedule of a simple region on a 4 issue processor. }
\label{fig:example1}
\end{figure}


From this introductory example, we can observe that:
\begin{itemize}
\item the two possible execution paths have been merged into a single execution path, implying a  better exploitation of the available resources;  
\item the schedule height have been reduced, because instructions can be control speculated before the branch;
\item the variables have been renamed, and a \textit{merge} pseudo-instruction have been introduced.
\end{itemize}

% \annotation{Reference join sets; phi-congruence already here?}
Thanks to SSA, the merging point is already materialized in the original control flow as a $\phi$ pseudo-instruction, and register renaming was performed by SSA construction. Given this, the transformation to generate if-converted code seems natural locally. Still exploiting those properties on larger scale control flow regions requires a framework that we will develop further.

\subsection{Architectural requirements}

The \textit{merge} pseudo operations needs to be mapped to a conditional form of execution in the target's architecture. As illustrated by Figure~\ref{fig:pred} we differentiate the three following models of conditional execution:
\begin{itemize}
\item \emph{Fully predicated execution}: Any instruction can be executed conditional on the value of a predicate operand.
\item \emph{(Control) speculative execution}: The instruction is executed unconditionally, and then committed using conditional moves (\texttt{cmov}) or \texttt{select} instructions. 
\item \emph{partially predicated execution}: Only a subset of the ISA is predicated, usually memory operations that are not easily speculated; other instructions are speculated.
\end{itemize}
In this figure, we use the notation $r = c \cond :r_1:r_2$, to represent a \texttt{select} like operation. Its semantic is identical to the gated $\phiif$-function presented in Chapter~\ref{chapter:vsdg}: $r$ takes the value of $r_1$ if $c$ is true, $r_2$ otherwise. Similarly, we also use the notation $r = c \cond op$ to represent the predicated execution of $op$ if the predicate $c$ is true; $r = \overline{p} \cond op$ if the predicate $c$ is false.

\begin{figure}[h]
\footnotesize
\subfloat[fully predicated]{
\begin{tabular}[t]{p{0.20\textwidth}}
$p \cond x = a + b $ \\
$\overline{p} \cond x = a * b $\\
~ 
\end{tabular}
}
\hfill
\subfloat[speculative using \emph{select}]{
\begin{tabular}[t]{p{0.27\textwidth}}
$t_1 = a + b $ \\
$t_2 = a * b $ \\
$x= p \cond t_1 : t_2 $ 
\end{tabular}
} \hfill
\subfloat[speculative using \emph{cmov}]{
\begin{tabular}[t]{p{0.27\textwidth}}
$x = a + b $ \\
$t = a * b $ \\
$x = \texttt{cmov}\ p,t$ 
\end{tabular}
}
\caption{Conditional execution using different models}
\label{fig:pred}
\end{figure}

To be speculated, an instruction must not have any possible side effects, or hazards. For instance a memory load must not trap because of an invalid address. 
Memory operations are  a major impediment to if-conversion. This is regrettable, because as any other long latency instructions, speculative loads can be very effective to fetching data earlier in the instruction stream, reducing stalls. Modern architectures provide architectural support to dismiss invalid address exceptions. Examples are the \texttt{ldw.d} dismissible load operation in the Multiflow Trace series of computers, or in the STMicroelectronics \textit{ST231} processor, but also the speculative load of the Intel IA64. The main difference is that with a dismissible model, invalid memory access exceptions are not delivered, which can be problematic in embedded or kernel environment that relies on memory exception for correct behavior. A speculative model allows to catch the exception thanks to the token bit check instruction. Some architectures, such as the \textit{IA64}, offer both speculative and predicated memory operations.
%
Stores can also be executed conditionally by speculating part of their address value, with additional constraints on the ordering on the memory operations due to possible alias between the two paths. Figure \ref{fig:spec} shows examples of various form of speculative memory operations.

Note that the \texttt{select} instruction is an architecture instruction that does not need to be replaced during the SSA destruction phase. If the target architecture does not provide such an gated instruction, it can be emulated using two conditional moves. This translation can be done afterward, and the \texttt{select} instruction still be used as an intermediate form. It allows the program to stay in full SSA form where all the data dependencies are made explicit, and can thus be feed to all SSA optimizers. 

\begin{figure}[h]
\subfloat[IA64 speculative load]{
\begin{minipage}[b]{0.30\textwidth}
$t = \texttt{ld.s(\textit{addr})} $ \\
$\texttt{chk.s} $\\
$p \cond x = t$ 
\end{minipage}
}\hfill
\subfloat[ST231 dismissible load]{
\begin{minipage}[b]{0.32\textwidth}
$t = \texttt{ldw.d(\textit{addr})} $ \\
$x = \texttt{select}\:p \cond t : x $ \\
~
\end{minipage}
    }\\
\subfloat[base store hoisting]{
\begin{minipage}[b]{0.33\textwidth}
$x=\texttt{select}\:p \cond \textit{addr} : \textit{dummy} $ \\
$\texttt{stw} (x, \textit{value}) $ 
\end{minipage}
}\hfill
\subfloat[index store hoisting]{
\begin{minipage}[b]{0.32\textwidth}
$\textit{index}=\texttt{select}\:p \cond i : j $ \\
$\texttt{stw} (x[\textit{index}], \textit{value}) $ 
\end{minipage}
}
\caption{Examples of speculated memory operations}
\label{fig:spec}
\end{figure}




This chapter is organized as follow: we start to describe the SSA techniques to transform a CFG region in SSA form to produce an if-converted SSA representation using speculation. We then describe how this framework is extended to use predicated instructions, using the $\psi$-SSA form presented in Chapter~\ref{chapter:psi_ssa}. Finally we propose a global framework to pull together those techniques, incrementally enlarging the scope of the if-converted region to its maximum beneficial size.

\section{Basic Transformations}
\label{sec:basic}

Global if-conversion approaches, identify a control-flow region and if-convert it in one shot. As opposed to global approaches, the technique described in this chapter is based on incremental reductions. To this end, we consider basic SSA transformations which goal is to isolate a simple diamond-DAG structure (informally an \texttt{if-then-else-end}) that can be easily if-convert. The complete framework, that identifies and incrementally performs the transformation will be described in Section~\ref{sec:if_conversion:hyperblock}. 

\subsection{SSA operations on Basic Blocks}



The basic transformation that actually if-converts the code is the \emph{$\phi$~removal} that takes a simple diamond-DAG  as an input, i.e. a single-entry-node/single-exit-node (SESE) DAG with only two distinct forward paths from its entry-node to its exit-node. The $\phi$-removal consists in (1) speculating the code of both branches in the entry basic-block; (2) then replacing the \phifun by a select; (3) finally simplifying the control-flow to a single basic-block. This transformation is illustrated by the example of Figure~\ref{fig:phi_rem}. 
\begin{figure}[h]
  \includegraphics[scale=0.9]{phi_removal}
  \caption{$\phi$~removal\label{fig:phi_rem}}
\end{figure}


The goal of the \emph{$\phi$ reduction} transformation is to isolate a diamond-DAG from a structure that resemble a diamond-DAG but has side entries to its exit block. This diamond-DAG can then be reduced using the $\phi$~removal transformation. Nested \texttt{if-then-else-end} in the original code can create such control flow. One can notice the similarity with the nested arity-two $\phiif$-functions used for gated-SSA (see Chapter~\ref{chapter:vsdg}). In the most general case, the join node of the considered region has $n$ predecessors with \phifuns of the form $B_0:r=\phi(B_1:r_1,B_2:r_2,\dots,B_n:r_n)$, and is such that removing edges from $B_3,\dots, B_n$ would give a diamond-DAG. After the transformation, $B_1$ and $B_2$ point to a freshly created basic-block, say $B_{12}$, that itself points to $B_0$; a new variable $B_{12}:r_{12}=\phi(B_1:r_1,B_2:r_2)$ is created in this new basic-block; the \phifun in $B_0$ is replaced by $B_0:r=\phi(B_{12}:r_{12},\dots,B_n:r_n)$. This is illustrated through the example of Figure~\ref{fig:phi_red}.
\begin{figure}[h]
  \includegraphics[scale=0.9]{phi_reduction}
  \caption{\label{fig:phi_red}$\phi$ reduction}
\end{figure}

The objective of \emph{path duplication} is to get rid of all side entry edges that avoid a single-exit-node region to be a diamond-DAG. Through path duplication, all edges that point to a node different than the exit node or to the willing entry node, are ``redirected'' to the exit node. $\phi$~reduction can then be applied to the obtained region. More formally, consider two distinguished nodes, named \textit{entry} and the single exit node of the region \textit{exit}, such that there are exactly two different control flow paths from \textit{entry} to \textit{exit}; consider (if exists), the first node $\textit{side}_i$ on one of the forward path $\textit{entry}\rightarrow \textit{side}_0\rightarrow\dots\textit{side}_p\rightarrow\textit{exit}$ that has at least two predecessors. The transformation duplicates the path $P=\textit{side}_i\rightarrow\dots\rightarrow\textit{side}_p\rightarrow\textit{exit}$ into $P'=\textit{side'}_i\rightarrow\dots\rightarrow\textit{side'}_p\rightarrow\textit{exit}$ and redirects $\textit{side}_{i-1}$ (or $\textit{entry}$ if $i=0$) to $\textit{side'}_i$. All the \phifuns that are along $P$ and $P'$  for which the number of predecessors have changed have to be updated accordingly. Hence, a $r=\phi(\textit{side}_p:r_1,B_2:r_2,\dots,B_n:r_n)$ in $\textit{exit}$  will be updated into $r=\phi(\textit{side}'_p:r_1,B_2:r_2,\dots,B_n:r_n,\textit{side}_p:r_1)$; a $r=\phi(\textit{side}_{i-1}:r_0, r_1, \dots, r_m)$ originally in $\textit{side}_i$ will be updated into $r=\phi(r_1, \dots, r_m)$ in $\textit{side}_i$ and into $r=\phi(r_0)$ i.e. $r=r_0$ in $\textit{side'}_i$. Variables renaming (see Chapter~\ref{chapter:repair_maintain_ssa_after_optimization}) along with copy-folding can then be performed on $P$ and $P'$. All steps are illustrated  through the example of Figure~\ref{fig:phi_aug}.


\begin{figure}[h]
  \subfloat[almost diamond-DAG]{
    \includegraphics[scale=0.9]{phi_augmentation_a}
  }
  \subfloat[after path-duplication]{
    \includegraphics[scale=0.9]{phi_augmentation_b}
  }

  \subfloat[after renaming/copy-folding]{
    \includegraphics[scale=0.9]{phi_augmentation_c}
  }
  \subfloat[after $\phi$ reduction]{
    \includegraphics[scale=0.9]{phi_augmentation_d}
  }
  \caption{\label{fig:phi_aug}Path duplication}
\end{figure}


The last transformation, namely the \emph{Conjunctive predicate merging}, concerns the if-conversion of a control flow pattern that sometimes appears on codes to represent logical \texttt{and} or \texttt{or} conditional operations. As illustrated by Figure~\ref{fig:phi_merge} the goal is to get rid of side exit edges that avoid a single-entry-node region to be a diamond-DAG. As opposed to path duplication, the transformation is actually restricted to a very simple pattern highlighted in Figure~\ref{fig:phi_merge} made up of three distinct basic-block, \textit{entry}, that branches with predicate $p$ to \textit{side}, or \textit{exit}. \textit{side}, which is empty, branches itself with predicate $q$ to another basic-block outside of the region or to \textit{exit}. Conceptually the transformation can be understood has first isolating the outgoing path $p\rightarrow q$ and then if-converting the obtained diamond-DAG. Note that if $s=p\wedge q$ is false, $p\wedge \overline{q}$ simplifies to $\overline{q}$. Hence, $r_2$ can be written either way $p\cond t_1:t_2$ or  $\overline{q}\cond t_1:t_2$. 

\begin{figure}[h]
  \includegraphics[scale=0.9]{phi_merge}
  \caption{\label{fig:phi_merge}convergent conjunctive merge} 
\end{figure}

Implementing the same framework on a non-SSA form program, would require more efforts: The $\phi$~reduction would require variable renaming, involving either a global data-flow analysis or the insertion of copies at the \textit{exit} node of the diamond-DAG; inferring the minimum amount of {select} operations would require having and updating liveness information. SSA form solves the renaming issue for free, and as illustrated by Figure~\ref{fig:pred_min} the minimality and the pruned flavor of the SSA form allows to avoid inserting useless {select} operations.  

\begin{figure}[h]
\centering
  \subfloat[if-conversion on minimal SSA] {
   \includegraphics[scale=0.9]{phi_min}
   \label{fig:phi_minimal}}\hfill
  \subfloat[if-conversion on pruned SSA] {
  \includegraphics[scale=0.9]{phi_pru}
  \label{fig:phi_pruned}}
\caption{\label{fig:pred_min} SSA predicate minimality}
\label{fig:minimality}
\end{figure}

\subsection{Handling of predicated execution model}

The $\phi$ removal transformation described above considered a speculative execution model. As we will illustrate hereafter, in the context of a predicated execution model, the choice of speculation versus predication is an optimization decision that should not be imposed by the intermediate representation. Also, transforming speculated code into predicated code can be viewed as a coalescing problem. The use of $\psi$-SSA (see Chapter~\ref{chapter:psi_ssa}), as the intermediate form of if-conversion, allows to postpone the decision of speculating some code, while the coalescing problem is naturally handled by the $\psi$-SSA destruction phase. 

Just as (control) speculating an operation on a control flow graph corresponds to ignore the control dependence with the conditional branch, speculating an operation on an if-converted code corresponds to remove the data dependence with the corresponding predicate. On the other-hand on register allocated code, speculation adds anti-dependencies. This trade-off can be illustrated through the example of Figure~\ref{fig:pred_versus_spec}: For the fully predicated version of the code, the computation of $p$ has to be done before the computations of $x_1$ and $x_2$; speculating the computation of $x_1$ removes the dependence with $p$ and allows to execute it in parallel with the test $(a<\hspace{-.6em}?\ b)$; if both the computation of $x_1$ and $x_2$ are speculated, they cannot be coalesced and when destruction $\psi$-SSA, the $\psi$-function will give rise to some select instruction; if only the computation of $x_1$ is speculated, then $x_1$ and $x_2$ can be coalesced to $x$, but then an anti-dependence from $x=a+b$ and $p\cond x = c $ appears that forbid its  execution in parallel.

\begin{figure}[h]
\subfloat[predicated code]{
\hspace{0.5cm}\begin{minipage}[b]{2.4cm}\label{subfig:pred_versus_spec:a}
\footnotesize
$ p = (a<\hspace{-.6em}?\ b) $ \\
$ p \cond x_1 = a + b $ \\
$ \overline{p} \cond x_2 = c $ \\
$ x = \psi(p \cond x_1, \overline{p} \cond x_2) $
\end{minipage}
}\hfill
\subfloat[fully speculated]{
\begin{minipage}[b]{2.4cm}\label{subfig:pred_versus_spec:b}
\footnotesize
$ p = (a<\hspace{-0.6em}?\ b) $ \\
$ x_1 = a + b $ \\
$ x_2 = c $ \\
$ x = \psi(p \cond x_1, \overline{p} \cond x_2) $ 
\end{minipage}
}\hfill
\subfloat[partially speculated]{
\begin{minipage}[b]{2.7cm}\label{subfig:pred_versus_spec:b}
\footnotesize
$ p = (a<\hspace{-0.6em}?\ b) $ \\
$ x_1 = a + b $ \\
$ \overline{p} \cond x_2 = c $ \\
$ x = \psi(x_1, \overline{p} \cond x_2) $ 
\end{minipage}
}\hfill
\subfloat[after coalescing]{
\begin{minipage}[b]{2.3cm}\label{subfig:pred_versus_spec:c}
\footnotesize
$ p = (a<\hspace{-0.6em}?\ b) $ \\
$ x = a + b $ \\
$ \overline{p} \cond x = c $ \\
~
\end{minipage}
}
\caption{\label{fig:pred_versus_spec}Speculation removes the dependency with the predicate but adds anti-dependencies between concurrent computations.}
\end{figure}

In practice, speculation is performed during the $\phi$~removal transformation, whenever it is possible (operations with side effect cannot be speculated) and considered as beneficial. As illustrated by Figure~\ref{nested_psi_speculated}, only the operations part of \emph{one} of the diamond-DAG branch are actually speculated. This partial speculation leads to manipulating code made up of a mixed of predication and speculation. 


\begin{figure}
\footnotesize
\subfloat[nested if] {
\begin{minipage}[b]{2cm}
\begin{equation*}
\begin{array}{l}
\iftt (q)\\
\thentt\\
\quad p = (a<\hspace{-0.6em}?\ b)\\~\\
\quad x_1 = a + b\\
\quad \overline{p} \cond x_2 = c\\
\quad x = \psi(x_1, \overline{p} \cond x_2) \\
\quad d_1 = \texttt{f} (x) \\
\elsett\\
\quad d_2 = 3\\
\endtt\\
d = \phi(d_1,d_2)
\end{array}
\end{equation*}\\\vspace{-2.8em}
\end{minipage}
\label{nested_psi}}
\hfill
\subfloat[speculating the \texttt{then} branch] {
\hspace{1cm}
\begin{minipage}[b]{3cm}
~\\\\
$ p = (a<\hspace{-0.6em}?\ b) $ \\~\\
$ x_1 = a + b $ \\
$ \overline{p} \cond x_2 = c $ \\
$ x = \psi(x_1, \overline{p} \cond x_2) $ \\
$ d_1 = \texttt{f} (x) $ \\
~\\
$ \overline{q} \cond d_2 = 3 $ \\
~\\
$ d = \psi(d_1, \overline{q} \cond d_2) $ 
\end{minipage}
\label{nested_psi_speculated}}
\hfill
\subfloat[predicating both branches] {
\hspace{0.5cm}
\begin{minipage}[b]{3cm}
~\\
$ p = (a<\hspace{-0.6em}?\ b) $ \\
$ s = q\wedge \overline{p} $ \\
$ q \cond x_1 = a + b $ \\
$ s \cond x_2 = c $ \\
$ x = \psi(q \cond x_1, s \cond x_2) $ \\
$ q \cond d_1 = \texttt{f} (x) $ \\~\\
$ \overline{q} \cond d_2 = 3 $ \\~\\
$ d = \psi(q \cond d_1, \overline{q} \cond d_2) $ 
\end{minipage}
\label{nested_psi_predicated}}
\caption{Inner region $\psi$}
\end{figure}

Speculating code is the easiest part as it could be done prior to the actual if-conversion by simply hoisting the code above the conditional branch. Still we would like to outline that since $\psi$-functions are part of the intermediate representation, they can be considered for inclusion in a candidate region for if-conversion, and in particular for speculation. However, the strength of $\psi$-SSA allows to treat $\psi$-functions just as any other operation. Consider the code of Figure~\ref{nested_psi} containing a sub region already processed. To speculate the operation $d_1=\texttt{f}(x)$, the operation defining $x$, i.e. the $\psi$-function, also has to be speculated. Similarly, all the operations defining the operands $x_1$ and $x_2$ should also be speculated. If one of them can produce hazardous execution, then the $\psi$-function cannot be speculated, which forbids in turn the operation $d_1=\texttt{f}(x)$ to be speculated. Marking operations that cannot be speculated can be done easily using a forward propagation along def-use chains.

All operations that cannot be speculated, including possibly some $\psi$-functions, must be predicated. Suppose we are considering a none-speculated operation we aim at if-converting, that is part of the \texttt{then} branch on predicate $q$. Just as for $x_2=c$ in Figure~\ref{nested_psi}, this operation can already be predicated (on $\overline{p}$ here) prior to the if-conversion. In that case, a \textit{projection} on $q$ is performed, meaning that instead of predicating $x_2=c$ by $\overline{p}$ it gets predicated by $q\wedge \overline{p}$. A $\psi$-function can also be projected on a predicate $q$ as described in Chapter~\ref{chapter:psi_ssa}: All gates of each operand are individually projected on $q$. As an example, originally non-gated operand $x_1$ gets gated by $q$, while the $\overline{p}$ gated operand $x_2$ gets gated by $s=q\wedge\overline{p}$. Note that as opposed to speculating it, predicating a $\psi$-function does not impose to predicate the operation that define its operands. The only subtlety related to projection is related to generating the new predicate as the logical conjunction of the original guard (e.g. $\overline{p}$) and the current branch predicate (e.g. $q$). Here $s$ needs to be computed at some point. Our heuristic consists in first listing the set of all necessary predicates and then emitting the corresponding code at the earlier place. Here, used predicates are $q$, $\overline{q}$, and $q\wedge\overline{p}$. $q$ and $\overline{q}$ are already available. The earlier place where $q\wedge\overline{p}$ can be computed is just after calculating $p$.

Once, operations have been speculated or projected (on $q$ for the \texttt{then} branch, on $\overline{q}$ for the \texttt{else}), each \phifuns at the merge point is replaced by a $\psi$-function: operands of speculated operations are placed first and guarded by true; operands of projected operations follow, guarded by the predicate of the corresponding branch.

\section{Global Analysis and Transformations}
\label{sec:if_conversion:hyperblock}
Hot regions are rarely just composed of simple \texttt{if-then-else-end} control flow regions but processors have limited resources: the number of registers will determine the acceptable level of data dependencies to minimize register pressure; the number of predicate registers will determine the depth of the if-conversion so that the number of conditions does not exceed the number of available predicates; the number of processing units will determine the number of instructions that can be executed simultaneously. The inner-outer incremental process advocated in this chapter allows to evaluate finely the profitability of if-conversion. 

\subsection{SSA Incremental if-conversion algorithm}

%% For each basic block considered within the region, a predicate must be computed and assigned to the corresponding instructions. Those predicate computations introduce new instructions and new data dependencies, that need to be controlled while the region is been if-converted.

The algorithm takes as input a CFG in SSA form and applies incremental reductions on it using a list of candidate conditional basic-blocks sorted in post-order. 
Each basic-block in the list designates the head of a sub-graph that can be if-converted using the transformations described in Section~\ref{sec:basic}. Post-order traversal allows to process the regions from inner to outer. When the if-converted region cannot grow anymore because of resources, or because a basic block cannot be if-converted, then another candidate is considered in the post-order list until all the CFG is explored.
%
%% During this incremental process, since nested regions are already predicated when evaluating the if-conversion of a branch, all the side effects, such as new predicate merging instructions, new conditional moves merging flow or new data dependencies, will be accounted to evaluate the profitability.
%
Note that as the reduction proceeds, maintaining SSA can be done using the general technique described in Chapter~\ref{chapter:repair_maintain_ssa_after_optimization}. However, basic ad-hoc updates can be implemented instead. 

Consider for example the CFG from the gnu \texttt{wc} (word count) program reported in Figure~\ref{fig:wc1}. The exit node $\textit{BB}7$, and basic-block $\textit{BB}3$ that contains a function call cannot be if-converted (represented in gray). 
 The post-order list of conditional blocks (represented in bold) is [$\textit{BB}11$, $\textit{BB}17$, $\textit{BB}16$, $\textit{BB}14$, $\textit{BB}10$, $\textit{BB}9$, $\textit{BB}6$, $\textit{BB}2$]. 
The first candidate region is composed of $\{\textit{BB11}, \textit{BB}2, \textit{BB}12\}$. $\phi$ reduction can be applied, promoting the instructions of $\textit{BB}12$ in $\textit{BB}11$. $\textit{BB}2$ becomes the single successor of $\textit{BB}11$. 
%
The region headed by $\textit{BB}17$ is then considered. $\textit{BB}19$ cannot yet be promoted because of the side entries coming both from $\textit{BB}15$ and $\textit{BB}16$. $\textit{BB}19$ is duplicated into a $\textit{BB}19'$ with $\textit{BB}2$ as successor. $\textit{BB}19'$ can then be promoted into $\textit{BB}17$.
%
The region headed by $\textit{BB}16$ that have now $\textit{BB}17$ and $\textit{BB}19$ as successors is considered. $\textit{BB}19$ is duplicated into $\textit{BB}19''$, so as to promote $\textit{BB}17$ and $\textit{BB}19'$ into $\textit{BB}16$ through $\phi$ reduction. $\textit{BB}19'$ already contains predicated operations from the previous transformation. So, a new merging predicate is computed and inserted. After the $\phi$ removal is fully completed $\textit{BB}16$ has a unique successor, $\textit{BB}2$.
%
$\textit{BB}14$ is the head of the new candidate region where $\textit{BB}15$, $\textit{BB}16$ can be promoted. Again since $\textit{BB}16$ contains predicated and predicate setting operations, a newly predicate must be created to hold the merged conditions.
$\textit{BB}10$ is then considered; $\textit{BB}14$ needs to be duplicated to $\textit{BB}14'$. The process finished with the region head by $\textit{BB}9$.

\begin{figure}
  \subfloat[Initial] {
    \includegraphics[scale=0.7]{graph_a}
    \label{fig:wc1}}
  \subfloat[If-conversion] {
    \hspace{1em}\includegraphics[scale=0.7]{graph_b}\hspace{1em}
    \label{fig:wc2}}
  \subfloat[Tail duplication] {
    \includegraphics[scale=0.7]{graph_c}
    \label{fig:wc3}}
\caption{\label{fig:wc example}If-conversion of \texttt{wc} (word count program). Basic-blocks in gray cannot be if-converted. Tail duplication can be used to exclude \textit{BB3} from the to-be-if-converted region.}
\end{figure}


\subsection{Tail Duplication}

Just as for the example of Figure~\ref{fig:wc example}, some basic-blocks (such as \textit{BB3}) may have to be excluded from the region to if-convert. \emph{Tail duplication} can be used for this purpose. Similar to path duplication described in Section~\ref{sec:basic}, the goal of tail duplication is to get rid of incoming edges of a region to if-convert. This is usually done in the context of hyper-block formation, which technique consists in, as opposed to the inner-outer incremental technique described in this chapter, to if-convert a region in ``one shot''. Consider again the example of Figure~\ref{fig:wc1}, and suppose the set of selected basic-blocks defining the region to if-convert consists of all basic-blocks from \textit{BB2} to \textit{BB19} excluding \textit{BB3}, \textit{BB4}, and \textit{BB7}. Getting rid of the incoming edge from \textit{BB4} to \textit{BB6} is possible by duplicating all basic-blocks of the region reachable from \textit{BB6} as shown in Figure~\ref{fig:wc2}. 

Formally, consider a region $R$ made up of a set of basic-blocks, a distinguished one $\textit{entry}$ and the others denoted $\left(B_i\right)_{2\leq i\leq n}$, such that any $B_i$ is reachable from \textit{entry} in $R$. Suppose a basic-block $B_s$ has some predecessors $\textit{out}_1,\ \dots,\ \textit{out}_m$ that are not in $R$. Tail duplication consists in: (1) for all $B_j$ (including $B_s$) reachable from $B_s$ in $R$, create a basic-block $B'_j$ as a copy of $B_j$; (2) any branch from $B'_j$ that points to a basic-block $B_k$ of the region is rerouted to its duplicate $B'_k$; (3) any branch from a basic-block $\textit{out}_k$ to $B_s$ is rerouted to $B'_s$. In our example, we would have $\textit{entry}=\textit{BB2}$, $B_s=\textit{BB6}$, and $\textit{out}=\textit{BB4}$.

A global approach would just do as in Figure~\ref{fig:wc3}: first select the region; second, get-rid of side incoming edges using tail duplication; finally perform if-conversion of the whole region in one shot. We would like to point out that there is no phasing issue with tail duplication. To illustrate this point, consider the example of Figure~\ref{fig:hyper1} where \textit{BB2} cannot be if-converted. The selected region is made up of all other basic-blocks. Using a global approach as in standard hyper-block formation, tail duplication would be performed prior to any if-conversion. This would lead to the CFG of Figure~\ref{fig:hyper2}. Note that a new node, \textit{BB}7, has been added here after the tail duplication by a process called branch coalescing. Applying if-conversion on the two disjoint regions respectively head by \textit{BB4} and \textit{BB4'} would lead to the final code shown if Figure~\ref{fig:hyper3}. Our incremental scheme would first perform if-conversion of the region head by \textit{BB4}, leading to the code depicted in Figure~\ref{fig:hyper4}. Applying tail duplication to get rid of side entry from \textit{BB2} would lead to exactly the same final code as shown in Figure~\ref{fig:hyper5}.

\begin{figure}[h]
  \subfloat[initial] {
    \hspace{2em}\includegraphics[scale=0.7]{hyper1}\hspace{2em}
    \label{fig:hyper1}}\hfill
  \subfloat[tail duplication] {
    \hspace{2em}\includegraphics[scale=0.7]{hyper2}\hspace{2em}
    \label{fig:hyper2}}\hfill
  \subfloat[if-convertion] {
    \hspace{2em}\includegraphics[scale=0.7]{hyper3}\hspace{2em}
    \label{fig:hyper3}}\\
  \subfloat[initial] {
    \hspace{2em}\includegraphics[scale=0.7]{hyper1}\hspace{2em}
    }\hfill
  \subfloat[if-conversion] {
    \hspace{2em}\includegraphics[scale=0.7]{hyper4}\hspace{2em}
    \label{fig:hyper4}}\hfill
  \subfloat[tail duplication] {
    \hspace{2em}\includegraphics[scale=0.7]{hyper3}\hspace{2em}
    \label{fig:hyper5}}
 \caption{Absence of phasing issue for tail duplication\label{fig:hyper}}
\end{figure}


\subsection{Profitability}

Fusing execution paths can over commit the architectural ability to execute in parallel the multiple instructions: Data dependencies and register renaming introduce new register constraints. Moving operations earlier in the instruction stream increases live-ranges. 
Aggressive if-conversion can easily exceed the processor resources, leading to excessive register pressure or moving infrequently used long latencies instructions into the critical path. 

The prevalent idea is that a region can be if-converted if the cost of the resulting if-converted basic block is smaller than the cost of each basic block of the region taken separately weighted by its frequency. To evaluate the cost of the region before if-conversion, we consider all possible paths from the \textit{entry} block to the \textit{exit} block. For all transformations but the conjunctive predicate merging, there are only two disjoint paths.

We compare the cost of the region before if-conversion. So the cost of a path is the schedule estimation of all the blocks in the path pondered by the execution profile:
\begin{align*}
Cost(path)=Freq(path)*\sum_{k=1}^n(Cost(bb_{k}))
\end{align*}
The cost of the region starting at basic block $head$ before if-conversion is therefore the cost of all the basic blocks in the considered region, on each path.
\begin{align*}
Cost(BBs_{before:\ ifc})=Cost(bb_{head}) + branchlat + Cost(bbs_{taken:\ path}) + Cost(bbs_{fall-though:\ path})
\end{align*}
The Cost after if-conversion is estimated with:
\begin{align*}
Cost(BB_{after:\ ifc})=Cost(bb_{head} \circ bb_{taken:\ path} \circ bb_{fall-though:\ path})
\end{align*}
Where $\circ$ is the composition function that merges basic blocks together, removes associated branches and creates the predicate operations. The resulting $Cost$ applied to the new basic block represents the estimated schedule after if-conversion, that will be effective only if
\begin{align*}
Cost(BBs_{before:\ ifc}) > Cost(BB_{after:\ ifc})
\end{align*}

The estimated cost of the if-converted region is the schedule height estimation of the instructions without the branches. When the schedule height of the if-converted region is smaller than the profiled estimation, then it is profitable.

The profitability of logical disjunctive and conjunctive merge, convergent or not, can be considered as a simple merge of two paths. Consider the example from Figure~\ref{fig:phi_merge}. Since we are aiming at merging $side$ into $head$, we have:
\begin{align*}
Cost(BBs_{before:\ ifc})=Cost(bb_{head}) + branchlat + Cost(bbs_{head-side:\ path})
\end{align*}
which is after if-conversion
\begin{align*}
Cost(BB_{after:\ ifc})=Cost(bb_{head} \circ bb_{head-side:\ path} +1)
\end{align*}

Note that since the frequency for the $head-exit$ path, for $(p\wedge \overline{q})\vee \overline{p}$, and the frequency for the $head->B1$ path, for $p\wedge q$, are unchanged after if-conversion. Those paths are not included into the profitability function.
However, it is interesting to observe that when the $head-side$ path Cost is higher than the if-conversion budget, emitting a conjunctive merge might not be beneficial, and another strategy such as block duplication of the $exit$ block will be evaluated.

The objective function needs the target machine description to derive the instruction latencies, resource usage and scheduling constraints. The local dependencies computed between instructions are used to compute the dependence height. The branch frequency is obtained either from static branch prediction heuristics, profile information or user inserted directives. Naturally, this heuristic can be either pessimistic, because it does not take into account new optimization opportunities introduced by the branch removals or explicit new dependencies, or optimistic because of bad register pressure estimation leading to register spilling on the critical path, or uncertainty in the branch prediction. But since the SSA incremental if-conversion framework reduces the scope for the decision function to a localized part of the CFG, the size and complexity of the inner region under consideration makes the profitability a comprehensive process. Consequently, this cost function must be fast enough to be reapplied to each hammock during the incremental processing, with the advantage that all the instructions introduced by the if-conversion process in the inner regions, such as new predicate merging instructions or new temporary pseudo registers, will be accounted for in the decision to if-convert the regions encompassing them.

\section{Conclusion} 

We presented in this chapter how an if-conversion algorithm can take advantage of the SSA properties to efficiently assign predicates and lay out the new control flow in an incremental, inner-outer process. As opposed to the alternative top-down approach, the region selection can be reevaluated at each nested transformation, using local analysis.
Basic block selection and if-conversion are performed as a single process, hyperblocks being created lazily, using well known techniques such as tail duplication or branch coalescing only when the benefit is established.
Predication and speculation are often presented as two different alternatives for if-conversion. While it is true that they both require different hardware support, they should coexist in an efficient if-conversion process such that every model of conditional execution is accepted. Thanks to conditional moves and $\psi$ transformations, they are now generated together in the same framework.

\section{Additional reading}

\cite{Rau:2003:IP:1074100.1074489}, exposes ILP in VLIW architectures using trace scheduling and local if-converted if-then-else regions using the $select$ and dismissible load operations. The idea behind was to enable the compiler to statically reorganize the instruction. In this respect, predictability \cite{Fisher:1992:PCB:143371.143493} becomes a major criteria for profitability.

To overcome the hard to predict profitability in conventional if-conversion algorithms, Reverse if-conversion was proposed in \cite{August:1999:PRI:326224.325595}, reconstructing the control flow at schedule time, after application of more aggressive region selection criteria.

Hyperblocks \cite{Mahlke:1992:ECS:144965.144998} was proposed as the primary if-converted scheduling framework, excluding basic blocks which do not justify their inclusion into the if-converted flow of control.

The duality between SSA like $\phi$s and predicate dependencies have been used in other works. In SSA-PS \cite{Jacome01clusteredvliw}, Predicated Switching operations are used to realize the conditional assignments using aggressive speculation techniques with conditional moves. Phi-Predication \cite{Chuang03phi-predicationfor} uses a modified version of the RK algorithm, to map phi-predication with phi-lists, holding guard and topological information. In both works, the use of SSA aims at solving the multiple definition problem exploiting variable renaming and join points, but they are based on speculation using conditional moves. 

In \cite{Stoutchinin_Gao_2004}, $\psi$ instructions are inserted while in SSA using a modified version of the classical Fang algorithm \cite{Fang:1996:CAI:645674.663446}, enabling support for a fully predicated ISA.
Those works established that standard if-conversion techniques can be applied to a SSA form using the $\psi$-SSA representation, or light weight $\phi$-SSA generation, but do not yet exploit the native SSA properties to build up the if-converted region.
A global SSA framework was presented \cite{odes_bruel} to support $select$ moves using aggressive speculation techniques, further extended to $\psi$-SSA \cite{ijes_bruel} allowing a mix of speculative and predicated techniques.

\cite{Mahlke95acomparison} evaluates how predicated operations can be performed using an equivalent sequences of speculative and conditional moves, starting from an if-converted region fully predicated. 










