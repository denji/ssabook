 \chapter{If-Conversion \Author{C. Bruel}}\label{chapter:if_conversion}
\inputprogress
\label{chapter:if_conversion}
\graphicspath{{img/}{if_conversion/img/}{part4/if_conversion/img/}}

\newcommand{\annotation}[1]{%
  \marginpar{\small\itshape\color{red}#1}}

%\setcounter{tocdepth}{3} 
%\tableofcontents

% TODO:
%   - 1.2.1.4 Insert example of non-convergent conjunctive iterative if-conversion
%   - 1.3.2   Insert example of a case where iteration on the algorithm is needed.

\section{Introduction}

Very Large Instruction Word (VLIW) or Explicitly Parallel Instruction Computing (EPIC) architectures make Instruction Level Parallelism (ILP) visible within the Instruction Set Architecture (ISA), relying on static schedulers to organize the compiler output such that multiple instructions can be issued in each cycle.
The imperative control flow representation of the program, inherently imposes an order of execution between instructions of different basic blocks. Yet, two different instructions, even ``connected'' by some control flow edges, could be executed in parallel. As explained in Chapter~\ref{chapter:vsdg}, this is the case whenever they are connected neither by a data nor a control dependence. 

If-conversion is the process of transforming a control flow region with conditional branches, into an equivalent predicated sequence of instructions (in a single basic-block). As for gated-SSA form (see Chapter~\ref{chapter:vsdg}), if-converted code replaces control dependencies by data dependencies, and thus exposes parallelism very naturally within the new merging basic-block. 
Removing branches improves performance in several ways: By removing the misprediction penalty, the instruction fetch throughput is increased and the instruction cache miss penalty reduced. Enlarging the size of basic blocks allows earlier execution of long latency operations and the merging of multiple control flow paths into a single flow of execution, than can later be exploited by scheduling frameworks such as VLIW scheduling, hyperblock scheduling or modulo scheduling.

Consider the simple example given in figure \ref{fig:example1}, that represents the execution of an \texttt{if-then-else-end} statement on a 4-issue processor. In this figure, $r=q\cond r_1:r_2$ stands for a select instruction where $r$ is assigned $r_1$ if $q$ is true, and $r_2$ otherwise. With standard basic-block ordering, assuming that all instructions have a one cycle latency, the schedule height goes from six cycles in the most optimistic case, to seven cycles. After if-conversion the execution path is reduced to six cycles with no branches, regardless of the test outcome, and assuming a very optimistic one cycle branch penalty. But the main benefit here is that it can be executed without branch disruption. 

\begin{figure}
  \subfloat[Control Flow] {
    \includegraphics[scale=0.9]{specul}
    \label{fig:orig}
  }
  \subfloat[with basic block ordering] {
    \includegraphics[scale=0.9]{specul-linear}
  }

  \subfloat[After if-conversion] {
    \includegraphics[scale=0.9]{specul-ifconverted}
  }
\caption{Static schedule of a simple region on a 4 issue processor. }
\label{fig:example1}
\end{figure}


From this introductory example, we can observe that:
\begin{itemize}
\item the two possible execution paths have been merged into a single execution path, implying a  better exploitation of the available resources;  
\item the schedule height have been reduced, because instructions can be control speculated before the branch;
\item the variables have been renamed, and a \textit{merge} pseudo-instruction have been introduced.
\end{itemize}

% \annotation{Reference join sets; phi-congruence already here?}
Thanks to SSA, the merging point is already materialized in the original control flow as a $\phi$ pseudo-instruction, and register renaming was performed by SSA construction. Given this, the transformation to generate if-converted code seems natural locally. Still exploiting those properties on larger scale control flow regions requires a framework that we will develop further.

\subsection{Architectural requirements}

The \textit{merge} pseudo operations needs to be mapped to a conditional form of execution in the target's architecture. As illustrated by Figure~\ref{fig:pred} we differentiate the three following models of conditional execution:
\begin{itemize}
\item \emph{Fully predicated execution}: Any instruction can be executed conditional on the value of a predicate operand.
\item \emph{(Control) speculative execution}: The instruction is executed unconditionally, and then committed using conditional moves (\texttt{cmov}) or \texttt{select} instructions. 
\item \emph{partially predicated execution}: Only a subset of the ISA is predicated, usually memory operations that are not easily speculated; other instructions are speculated.
\end{itemize}
In this figure, we use the notation $r = c \cond :r_1:r_2$, to represent a \texttt{select} like operation. Its semantic is identical to the gated $\phiif$-function presented in Chapter~\ref{chapter:vsdg}: $r$ takes the value of $r_1$ if $c$ is true, $r_2$ otherwise. Similarly, we also use the notation $r = c \cond op$ to represent the predicated execution of $op$ if the predicate $c$ is true; $r = \overline{p} \cond op$ if the predicate $c$ is false.

\begin{figure}[h]
\footnotesize
\subfloat[fully predicated]{
\begin{tabular}[t]{p{0.20\textwidth}}
$p \cond x = a + b $ \\
$\overline{p} \cond x = a * b $\\
~ 
\end{tabular}
}
\hfill
\subfloat[speculative using \emph{select}]{
\begin{tabular}[t]{p{0.27\textwidth}}
$t_1 = a + b $ \\
$t_2 = a * b $ \\
$x= p \cond t_1 : t_2 $ 
\end{tabular}
} \hfill
\subfloat[speculative using \emph{cmov}]{
\begin{tabular}[t]{p{0.27\textwidth}}
$x = a + b $ \\
$t = a * b $ \\
$x = \texttt{cmov}\ p,t$ 
\end{tabular}
}
\caption{Conditional execution using different models}
\label{fig:pred}
\end{figure}

To be speculated, an instruction must not have any possible side effects, or hazards. For instance a memory load must not trap because of an invalid address. 
Memory operations are  a major impediment to if-conversion. This is regrettable, because as any other long latency instructions, speculative loads can be very effective to fetching data earlier in the instruction stream, reducing stalls. Modern architectures provide architectural support to dismiss invalid address exceptions. Examples are the \texttt{ldw.d} dismissible load operation in the Multiflow Trace series of computers, or in the STMicroelectronics \textit{ST231} processor, but also the speculative load of the Intel IA64. The main difference is that with a dismissible model, invalid memory access exceptions are not delivered, which can be problematic in embedded or kernel environment that relies on memory exception for correct behavior. A speculative model allows to catch the exception thanks to the token bit check instruction. Some architectures, such as the \textit{IA64}, offer both speculative and predicated memory operations.
%
Stores can also be executed conditionally by speculating part of their address value, with additional constraints on the ordering on the memory operations due to possible alias between the two paths. Figure \ref{fig:spec} shows examples of various form of speculative memory operations.

Note that the \texttt{select} instruction is an architecture instruction that does not need to be replaced during the SSA destruction phase. If the target architecture does not provide such an gated instruction, it can be emulated using two conditional moves. This translation can be done afterward, and the \texttt{select} instruction still be used as an intermediate form. It allows the program to stay in full SSA form where all the data dependencies are made explicit, and can thus be feed to all SSA optimizers. 

\begin{figure}[h]
\subfloat[IA64 speculative load]{
\begin{minipage}[b]{0.30\textwidth}
$t = \texttt{ld.s(\textit{addr})} $ \\
$\texttt{chk.s} $\\
$p \cond x = t$ 
\end{minipage}
}\hfill
\subfloat[ST231 dismissible load]{
\begin{minipage}[b]{0.32\textwidth}
$t = \texttt{ldw.d(\textit{addr})} $ \\
$x = \texttt{select}\:p \cond t : x $ \\
~
\end{minipage}
    }\\
\subfloat[base store hoisting]{
\begin{minipage}[b]{0.33\textwidth}
$x=\texttt{select}\:p \cond \textit{addr} : \textit{dummy} $ \\
$\texttt{stw} (x, \textit{value}) $ 
\end{minipage}
}\hfill
\subfloat[index store hoisting]{
\begin{minipage}[b]{0.32\textwidth}
$\textit{index}=\texttt{select}\:p \cond i : j $ \\
$\texttt{stw} (x[\textit{index}], \textit{value}) $ 
\end{minipage}
}
\caption{Examples of speculated memory operations}
\label{fig:spec}
\end{figure}




This chapter is organized as follow: we start to describe the SSA techniques to transform a CFG region in SSA form to produce an if-converted SSA representation using speculation. We then describe how this framework is extended to use predicated instructions, using the $\psi$-SSA form presented in Chapter~\ref{chapter:psi_ssa}. Finally we propose a global framework to pull together those techniques, incrementally enlarging the scope of the if-converted region to its maximum beneficial size.

\section{Basic Transformations}
\label{sec:basic}

Global if-conversion approaches, identify a control-flow region and if-convert it in one shot. As opposed to global approaches, the technique described in this chapter is based on incremental reductions. To this end, we consider basic SSA transformations which goal is to isolate a simple diamond-DAG structure (informally an \texttt{if-then-else-end}) that can be easily if-convert. The complete framework, that identifies and incrementally performs the transformation will be described in Section~\ref{sec:if_conversion:hyperblock}. 

\subsection{SSA operations on Basic Blocks}



The basic transformation that actually if-converts the code is the \emph{$\phi$~removal} that takes a simple diamond-DAG  as an input, i.e. a single-entry-node/single-exit-node (SESE) DAG with only two distinct forward paths from its entry-node to its exit-node. The $\phi$-removal consists in (1) speculating the code of both branches in the entry basic-block; (2) then replacing the \phifun by a select; (3) finally simplifying the control-flow to a single basic-block. This transformation is illustrated by the example of Figure~\ref{fig:phi_rem}. 
\begin{figure}[h]
  \includegraphics[scale=0.9]{phi_removal}
  \caption{$\phi$~removal\label{fig:phi_rem}}
\end{figure}


The goal of the \emph{$\phi$ reduction} transformation is to isolate a diamond-DAG from a structure that resemble a diamond-DAG but has side entries to its exit block. This diamond-DAG can then be reduced using the $\phi$~removal transformation. Nested \texttt{if-then-else-end} in the original code can create such control flow. One can notice the similarity with the nested arity-two $\phiif$-functions used for gated-SSA (see Chapter~\ref{chapter:vsdg}). In the most general case, the join node of the considered region has $n$ predecessors with \phifuns of the form $B_0:r=\phi(B_1:r_1,B_2:r_2,\dots,B_n:r_n)$, and is such that removing edges from $B_3,\dots, B_n$ would give a diamond-DAG. After the transformation, $B_1$ and $B_2$ point to a freshly created basic-block, say $B_{12}$, that itself points to $B_0$; a new variable $B_{12}:r_{12}=\phi(B_1:r_1,B_2:r_2)$ is created in this new basic-block; the \phifun in $B_0$ is replaced by $B_0:r=\phi(B_{12}:r_{12},\dots,B_n:r_n)$. This is illustrated through the example of Figure~\ref{fig:phi_red}.
\begin{figure}[h]
  \includegraphics[scale=0.9]{phi_reduction}
  \caption{\label{fig:phi_red}$\phi$ reduction}
\end{figure}

The objective of \emph{path duplication} is to get rid of all side entry edges that avoid a single-exit-node region to be a diamond-DAG. Through path duplication, all edges that point to a node different than the exit node or to the willing entry node, are ``redirected'' to the exit node. $\phi$~reduction can then be applied to the obtained region. More formally, consider two distinguished nodes, named \textit{entry} and the single exit node of the region \textit{exit}, such that there are exactly two different control flow paths from \textit{entry} to \textit{exit}; consider (if exists), the first node $\textit{side}_i$ on one of the forward path $\textit{entry}\rightarrow \textit{side}_0\rightarrow\dots\textit{side}_p\rightarrow\textit{exit}$ that has at least two predecessors. The transformation duplicates the path $P=\textit{side}_i\rightarrow\dots\rightarrow\textit{side}_p\rightarrow\textit{exit}$ into $P'=\textit{side'}_i\rightarrow\dots\rightarrow\textit{side'}_p\rightarrow\textit{exit}$ and redirects $\textit{side}_{i-1}$ (or $\textit{entry}$ if $i=0$) to $\textit{side'}_i$. All the \phifuns that are along $P$ and $P'$  for which the number of predecessors have changed have to be updated accordingly. Hence, a $r=\phi(\textit{side}_p:r_1,B_2:r_2,\dots,B_n:r_n)$ in $\textit{exit}$  will be updated into $r=\phi(\textit{side}'_p:r_1,B_2:r_2,\dots,B_n:r_n,\textit{side}_p:r_1)$; a $r=\phi(\textit{side}_{i-1}:r_0, r_1, \dots, r_m)$ originally in $\textit{side}_i$ will be updated into $r=\phi(r_1, \dots, r_m)$ in $\textit{side}_i$ and into $r=\phi(r_0)$ i.e. $r=r_0$ in $\textit{side'}_i$. Variables renaming (see Chapter~\ref{chapter:repair_maintain_ssa_after_optimization}) along with copy-folding can then be performed on $P$ and $P'$. All steps are illustrated  through the example of Figure~\ref{fig:phi_aug}.


\begin{figure}[h]
  \subfloat[almost diamond-DAG]{
    \includegraphics[scale=0.9]{phi_augmentation_a}
  }
  \subfloat[after path-duplication]{
    \includegraphics[scale=0.9]{phi_augmentation_b}
  }

  \subfloat[after renaming/copy-folding]{
    \includegraphics[scale=0.9]{phi_augmentation_c}
  }
  \subfloat[after $\phi$ removal]{
    \includegraphics[scale=0.9]{phi_augmentation_d}
  }
  \caption{\label{fig:phi_aug}Path duplication}
\end{figure}


The last transformation, namely the \emph{Conjunctive predicate merging}, concerns the if-conversion of a control flow pattern that sometimes appears on codes to represent logical \texttt{and} or \texttt{or} conditional operations. As illustrated by Figure~\ref{fig:phi_merge} technically the goal is to get rid of side exit edges that avoid a single-entry-node region to be a diamond-DAG. As opposed to path duplication, the transformation is actually restricted to a very simple pattern highlighted in Figure~\ref{fig:phi_merge} made up of three distinct basic-block, \textit{entry}, that branches with predicate $p$ to \textit{side}, or \textit{exit}. \textit{side}, which is empty, branches itself with predicate $q$ to another basic-block outside of the region or to \textit{exit}. Conceptually the transformation can be understood has first isolating the outgoing path $p\rightarrow q$ and then if-converting the obtained diamond-DAG. Note that providing that $s=p\wedge q$ is true, $p\wedge \overline{q}$ simplifies to $\overline{q}$. Hence, $r_1$ can be written either way $p\cond t_1:t_2$ or  $\overline{q}\cond t_1:t_2$.

\begin{figure}[h]
  \includegraphics[scale=0.9]{phi_merge}
  \caption{\label{fig:phi_merge}convergent conjunctive merge} 
\end{figure}

Implementing the same framework on a non-SSA form program, would require more efforts: The $\phi$~reduction would require variable renaming, involving either a global data-flow analysis or the insertion of copies at the \textit{exit} node of the diamond-DAG; inferring the minimum amount of {select} operations would require having and updating liveness information. SSA form solves the renaming issue for free, and as illustrated by Figure~\ref{fig:pred_min} the minimality and the pruned flavor of the SSA form allows to avoid inserting useless {select} operations.  

\begin{figure}[h]
\centering
  \subfloat[if-conversion on minimal SSA] {
   \includegraphics[scale=0.9]{phi_min}
   \label{fig:phi_minimal}}\hfill
  \subfloat[if-conversion on pruned SSA] {
  \includegraphics[scale=0.9]{phi_pru}
  \label{fig:phi_pruned}}
\caption{\label{fig:pred_min} SSA predicate minimality}
\label{fig:minimality}
\end{figure}

\subsection{Handling of predicated execution model}

The $\phi$ removal transformation described above considered a speculative execution model. As we will illustrate hereafter, in the context of a predicated execution model, the choice of speculation versus predication is an optimization decision that should not be imposed by the intermediate representation. Also, transforming speculated code into predicated code can be viewed as a coalescing problem. The use of $\psi$-SSA (see Chapter~\ref{chapter:psi_ssa}), as the intermediate form of if-conversion, allows to postpone the decision of speculating some code, while the coalescing problem is naturally handled by the $\psi$-SSA destruction phase. 

Just as (control) speculating an operation on a control flow graph corresponds to ignore the control dependence with the conditional branch, speculating an operation on an if-converted code corresponds to remove the data dependence with the corresponding predicate. On the other-hand on register allocated code, speculation adds anti-dependencies. This trade-off can be illustrated through the example of Figure~\ref{fig:pred_versus_spec}: For the fully predicated version of the code, the computation of $p$ has to be done before the computations of $x_1$ and $x_2$; speculating the computation of $x_1$ removes the dependence with $p$ and allows to execute it in parallel with the test $(a<\hspace{-.6em}?\ b)$; if both the computation of $x_1$ and $x_2$ are speculated, they cannot be coalesced and when destruction $\psi$-SSA, the $\psi$-function will give rise to some select instruction; if only the computation of $x_1$ is speculated, then $x_1$ and $x_2$ can be coalesced to $x$, but then an anti-dependence from $x=a+b$ and $p\cond x = c $ appears that forbid its  execution in parallel.

\begin{figure}[h]
\subfloat[predicated code]{
\hspace{0.5cm}\begin{minipage}[b]{2.4cm}\label{subfig:pred_versus_spec:a}
\footnotesize
$ p = (a<\hspace{-.6em}?\ b) $ \\
$ p \cond x_1 = a + b $ \\
$ \overline{p} \cond x_2 = c $ \\
$ x = \psi(p \cond x_1, \overline{p} \cond x_2) $
\end{minipage}
}\hfill
\subfloat[fully speculated]{
\begin{minipage}[b]{2.4cm}\label{subfig:pred_versus_spec:b}
\footnotesize
$ p = (a<\hspace{-0.6em}?\ b) $ \\
$ x_1 = a + b $ \\
$ x_2 = c $ \\
$ x = \psi(p \cond x_1, \overline{p} \cond x_2) $ 
\end{minipage}
}\hfill
\subfloat[partially speculated]{
\begin{minipage}[b]{2.7cm}\label{subfig:pred_versus_spec:b}
\footnotesize
$ p = (a<\hspace{-0.6em}?\ b) $ \\
$ x_1 = a + b $ \\
$ \overline{p} \cond x_2 = c $ \\
$ x = \psi(x_1, \overline{p} \cond x_2) $ 
\end{minipage}
}\hfill
\subfloat[after coalescing]{
\begin{minipage}[b]{2.3cm}\label{subfig:pred_versus_spec:c}
\footnotesize
$ p = (a<\hspace{-0.6em}?\ b) $ \\
$ x = a + b $ \\
$ \overline{p} \cond x = c $ \\
~
\end{minipage}
}
\caption{\label{fig:pred_versus_spec}Speculation removes the dependency with the predicate but adds anti-dependencies between concurrent computations.}
\end{figure}

In practice, whenever it is considered as beneficial, speculation is performed during the $\phi$~removal transformation. Obviously only operations that do not have a side effect are speculated. 
If-conversion using fully speculated code is actually simpler than when dealing with predicated code: The code of each conditional basic-blocks of the diamond-DAG is moved to the merging basic-block, and the \phifun replaced by a $\psi$-function with the branching predicates (and its complement) as gates. When predicating an operation that is already predicated predicates are simply merged using logical \textrm{and}.

%% To identify the operation's results that need to be conditionally defined, we only need to look at the defining instructions of the $\phi$ instructions. All temporaries that do not have a join point within the considered region and that don't have a side effect can be unconditionally speculated during the SSA transformation processes. Only instructions with a side effect need to be guarded. 
%% Instruction predicates from the taken path are merged (using a logical and) with the branch parameter. Predicates in instructions from the fall trough path are merged with the complement of the branch parameter.


%% A $\psi$ operation exposes new data dependencies, by expressing the merge of two definitions. Note that the order of the partial definition is important, because a definition partially redefines the preceding ones. We use this property to speculate the first definitions, so it becomes speculated instead of disjoint. This local optimization allows the removal of predicate dependencies but also creates a new partial dependency when predicates are not disjoint. 


\subsubsection{$\psi$ speculation properties}

Since $\psi$ operations are part of the intermediate representation, they can be considered for inclusion in a candidate region for if-conversion. The conditional operations that they refer can in turn be speculated or predicated incrementally. We define here the promotion rules for $\psi$ operands, whereas the instructions defining the $\psi$ operands will be speculated.

Consider the instructions \ref{nested_psi} containing a sub region already processed. The $\psi$ operation can be safely speculated if all the instructions defining its operands can be speculated: They don't produce hazardous execution, they don't produce any side effects and there exists a conditional move instruction to merge the operands. Then the block can be executed regardless of the value of $c$. The use of the $\psi$ result is also unconditionally executed.

\subsubsection{$\psi$ predication properties}

If an instructions is not speculable, it must be predicated:
In \ref{nested_psi_predicated}, the $c$ condition merges with all conditions under which the $\psi$ operands are defined. Here a new predicate $p_1$ is created to hold the predicate definition for the instructions defined under $c$. 

Note that conceptually, the speculative $\psi$ execution allows a predicate definition domain larger that the original one, such that the predicative transformation exactly matches the initial definition domain, at the expense of more data dependencies and predicate computation.

We can see with this example that the decision to speculate or predicate can be done at the level of each joining definition, allowing a mix of both speculation and predication. The advantage of speculation over predication is a reduced dependency length. The disadvantage of speculation is that it increases register pressure until the merge point, and potentially moves long latency operations on the critical path.
 
\begin{figure}
\footnotesize
\subfloat[nested if] {
\begin{minipage}[b]{3.5cm}
$ if (c) $ \\
$ \{ $ \\
\hspace*{2mm}$ x_1 = a + b $ \\
\hspace*{2mm}$ \overline{p} \cond x_2 = c $ \\
\hspace*{2mm}$ x = \psi(T \cond x_1, \overline{p} \cond x_2) $ \\
\hspace*{2mm}$ d_1 = use (x) $ \\
$ \} $ \\
$ else $ \\
\hspace*{2mm}$ d_2 = 3 $ \\
$ d = \phi(d_1,d_2) $ \\
\end{minipage}
\label{nested_psi}}
\subfloat[speculated nested] {
\begin{minipage}[b]{3.5cm}
$ x_1 = a + b $ \\
$ \overline{p} \cond x_2 = c $ \\
$ x = \psi(T \cond x_1, \overline{p} \cond x_2) $ \\
$ d_1 = use (x) $ \\
$ \overline{c} \cond d_2 = 3 $ \\
$ d = \psi(T \cond d_1, \overline{c} \cond d_2) $ \\
\end{minipage}
\label{nested_psi_speculated}}
\subfloat[predicated nested] {
\begin{minipage}[b]{3.5cm}
$ q = \overline{p} \& {c} $ \\
$ c \cond x_1 = a + b $ \\
$ q \cond x_2 = c $ \\
$ x = \psi(c \cond x_1, q \cond x_2) $ \\
$ c \cond d_1 = use (x) $ \\
$ \overline{c} \cond d_2 = 3 $ \\
$ d = \psi(c \cond d_1, \overline{c} \cond d_2) $ \\
\end{minipage}
\label{nested_psi_predicated}}
\caption{Inner region $\psi$}
\end{figure}

\section{Global Analysis and Transformations}
\label{sec:if_conversion:hyperblock}
Critical regions are rarely just composed of simple if-then-else control flow regions and processors have limited resources. The number of registers will determine the acceptable level of data dependencies to minimize register pressure. The number of predicate registers will determine the depth of the if-conversion so that the number of conditions does not exceed the number of available predicates and the number of processing units will determine the number of instructions that can be executed simultaneously. 

For this reason, standard techniques are either limited to a single conditional branch using peephole-style pattern matching or intrinsic functions (conditional code is inlined by the compiler in the internal representation). A more sophisticated approach is to scope larger regions such as hyperblocks. 

\subsection{Hyperblock formation}

A Hyperblock is a region of straight code with a single entry and possibly multiple exits where inner branches have been removed by if-conversion.
Hyperblocks enlarge the scope for scheduling in two ways. First, since basic blocks swell as the scope of the predicated region grows, static scheduling can now be unconstrained. Second, instructions can be freely speculated without the need for compensation code. Conditional code to exit the hyperblock does not impact the execution flow, using a proper code basic block reordering to favor fall through execution.

Tail duplication is used to exclude from the Hyperblock basic blocks that cannot be if-converted, either because they contain hazardous instructions, or because of heuristic decisions. 

Standard approach to Hyperblock construction is to apply the following steps:

\begin{itemize}
\item Create a trace and select the blocks. A trace is a sequence of basic blocks that can be scheduled together. 
\item Remove the side entries with tail duplication. This step removes the scheduling constraints imposed by side entries.
\item Perform if-conversion while still in SSA.
\end{itemize}

\subsection{CLassical approach}

One way to perform if-conversion in SSA is to apply a classical if-conversion algorithm, such as Fang \cite{Fang:1996:CAI:645674.663446} or RK \cite{Schlansker-predicated}, on a conventional SSA representation. 
During the if-conversion process, instructions that are merged into the deleted $\phi$ operations are now expressed as $\psi$ operations, merging different values from their $\psi-congruence$ class. Assuming that all instructions are predicated, an additional pass is required during instruction selection to emit speculative instructions when a predicated variant is not available. Another pass is needed for predicate promotion, based on the predicate dependence graph.

To illustrate the differences between a traditional framework and a native SSA framework, we first start by describing the RK algorithm proposed by Park and Schlansker on the nested tests in figure \ref{fig:nested1}. We will then describe in detail the SSA based approach.

Standard if-conversion techniques address the problem in the following order: First, identify the region to be if-converted. An execution trace is selected based on profiling information to isolate the region. Then allocate boolean variables to the basic blocks, in the identified region. Then perform predicate initialization placement, instruction emissions and CFG restructuring into the final if-converted regions.

The RK algorithm starts by computing the Control Dependence Graph. It is necessary to associate each basic block with a guard condition, and to associate each guard width the set of basic blocks that need to set it. The algorithm creates a new guard, eventually initialized to false, for each condition. \ref{fig:nested2} shows the inner basic blocks with the new conditions initialized, and \ref{fig:RK} the RK mappings.

\begin{figure}
\centering
  \subfloat[Nested test] {
    \includegraphics[scale=0.8]{nested1}
    \label{fig:nested1}}
  \subfloat[Predicate Assignments] {
    \includegraphics[scale=0.8]{nested1_rk}
    \label{fig:nested2}}
  \subfloat[After Instruction layout] {
    \includegraphics[scale=0.8]{nested1_rk2}
    \label{fig:nested}}
\caption{Classical support for partial predication}
\label{fig:trad_part_pred}
\end{figure}

\begin{figure}
\footnotesize
  \subfloat[R mapping] {
     \begin{tabular}[t]{|l|cccccc|}
\hline
      Basic Block & BB1 & BB2 & BB3 & BB4 & BB5 & BB6  \\
\hline
      Guard       &  T  &  p   & q  & p4  &  p  &  p2 \\
\hline
     \end{tabular}}
  \subfloat[K mapping] {
     \begin{tabular}[t]{|l|cccc|}
\hline
      Guard          & p & q & p4  & p2  \\
\hline
      Basic Block    & 1 & 2,-0 & -2,-0 & -1 \\
\hline
     \end{tabular}}
\caption{Output from RK algorithm}
\label{fig:RK}
\end{figure}

By the end, the algorithm merges and reorder the basic blocks, emitting the predicated instructions and predicate setting operations, and removing the instruction branches. \ref{fig:nested}. 

This approach assumes that the region to if-convert is defined and that all instructions can be predicated, Without predicate support for comparison instructions, logical operations must be used to merge condition setting operations. For instance, An algorithm based on speculative transformations, necessary to support partially predicated architectures, is then penalized by this approach, since an additional pass is needed to convert a conditional instruction into a temporary register and a conditional move.

We describe next a native approach to build hyperblocks under SSA, in which the if-conversion process is inherently part of a single comprehensible framework.

\subsection{SSA Incremental if-conversion algorithm}

For each basic block considered within the region, a predicate must be computed and assigned to the corresponding instructions. Those predicate computations introduce new instructions and new data dependencies, that need to be controlled while the region is been if-converted.

The native If-conversion in SSA form is based on an incremental, incremental if-conversion construction, unifying region selection and region transformation. 

The algorithm takes as input a structured region in SSA form and produces a valid SSA representation using conditional move instructions to realize join points, Incrementally building-up the if-converted region during a control flow traversal maintaining the SSA representation.

We create the list of candidate conditional blocks of the control flow sorted in post-order. Each one designates the head of a sub-graph that can be if-converted using the techniques described in chapter-\ref{sec:basic}. Post-order traversal guarantees that each inner region will be processed before regions of larger scope. No sub-graph needs to be selected at this point, because the decision to if-convert will be retaken incrementally from inner to outer regions as the hyperblock grows. When the region cannot grow anymore because of resources, or because a basic block cannot be if-converted, then another region is considered in the post-order list until all the CFG is explored

During this incremental process, since nested regions are already predicated when evaluating the if-conversion of a branch, all the side effects, such as new predicate merging instructions, new conditional moves merging flow or new data dependencies, will be accounted for locally. Furthermore, the predicate assignment is simplified, since new predicates are mechanically inserted when merging inner regions containing conditional code. The prevalent idea is that the inner region once predicated will be viewed as a single basic region by the outer scopes evaluation engines.

As the algorithm processes the control flow in post-order traversal, the dominator tree does not change, and it is possible to maintain the SSA locally to the inner region. By recurrence the if-converted region can in turn be optimized out if its head belongs to the dominance frontier of an outer region.

Consider for example the CFG from the $wc$ program (figure \ref{fig:wc1}). The exit node is $BB7$, and $BB3$ contains a function call, so will be excluded from the hyperblock. This control-flow contains three back edges and form a good example how a cyclic graph can be if-converted.

 The post-order list of the basic nested regions is ($BB11$, $BB17$, $BB16$, $BB14$, $BB10$, $BB9$, $BB6$, $BB2$). The head of nested hammocks is represented by the circle nodes.
The first hammock region starting at $BB11$ to $BB2$ contains only $BB12$. The fact that $BB2$ dominates the region to if-convert is not a problem, since what matters is the merging SSA $\phi$ operations. The instructions in $BB12$ are then predicated and $BB2$ becomes the single successor of $BB11$. 
Considering next the region pointed by $BB17$, $BB19$ cannot yet be promoted because of the side entries coming from $BB15$ and $BB16$, so it is duplicated into a $BB19'$ with $BB2$ as successor. $BB19$ can then be predicated into $BB17$
$BB16$ is the start of a region containing $BB17$ and $BB19'$. $BB17$ can be promoted into $BB16$. $BB19'$ already contains predicated operations from the previous transformation, So a new merging predicate is introduced at this point. $BB16$ unique successor is now a back edge to $BB2$.
$BB14$ is the head of the newly created region where $BB15$, $BB16$ can be promoted. Again since $BB16$ contains predicated and predicate setting operations, a newly predicate must be created to hold the merged conditions.
$BB10$ points to a cyclic hammock consisting from $BB10$-$BB14$-$BB11$-$BB2$. $BB14$ needs to be duplicated to $BB14'$ and $BB11$ to $BB11'$. The process finished with the hammock formed by $BB9$-$BB10$-$BB14'$.
The region is now if-converted, leaving a single back-edge, removing 7 branches inside the body loop.

\begin{figure}
  \subfloat[Before if-conversion] {
    \includegraphics[scale=0.6]{graph1}
    \label{fig:wc1}}
  \subfloat[After if-conversion] {
    \includegraphics[scale=0.6]{graph7}
    \label{fig:wc2}}
\label{fig:wc example}
\end{figure}

\subsection{Tail Duplication}

Consider the example of Hyperblock formation given in figure \ref{fig:hyper1}. This loop contains two branches, and so if-converting it would be profitable. However, block selection has excluded BB2 and has integrated BB5, because heuristics have determined that the schedule of BB5 inside BB4 would be beneficial. The trace contains {BB1, BB3, BB4, BB5, BB6}. Since BB4 has a side entry, it must be removed by tail duplication. Figure \ref{fig:hyper2} shows the control flow after block duplication. Notice that a new node, BB7, has been added after the tail duplication by a process called branch coalescing. Finally figure \ref{fig:hyper3} shows the code once if-converted.

\begin{figure}[h]
  \subfloat[loop] {
    \includegraphics[scale=0.7]{hyper1}
    \label{fig:hyper1}}
  \subfloat[standard tail-duplication] {
    \includegraphics[scale=0.7]{hyper2}
    \label{fig:hyper2}}
  \subfloat[after SSA if-conversion] {
    \includegraphics[scale=0.7]{hyper4}
    \label{fig:hyper4}}
  \subfloat[final flow] {
    \includegraphics[scale=0.7]{hyper3}
    \label{fig:hyper3}}
\end{figure}

When using this standard decomposition, the if-conversion is performed after tail-duplication. Hyperblock formation, if-conversion and speculation introduce a major phase ordering problem. 

Consider in contrast how tail-duplication can be performed lazily using the SSA incremental transformations, after the code has been if-converted. Figure \ref{fig:hyper4} shows the same loop body where the second $if$ region has been SSA if-converted. The decision to if-convert the region formed by {BB1, BB2, BB3} is now local and can be taken conservatively. Only at this stage, tail-duplication can be performed if necessary to remove the side entry coming from BB2. Duplicating a single predicated block is now a very simple operation that is described in the next paragraph. Note that if BB2 can be included in the region, the whole loop region would have been if-converted.

\subsection{Basic Block duplication}

Basic Block duplication is used to remove side entries and to obviate the constraints on control dependencies. Unless applied carefully, basic block duplication could be the cause of code bloating without performance improvement. However experience has shown that when applied carefully it can be efficient, enabling further scalar optimizations.
Consider figure \ref{fig:bbdup}. Since we are if-converting from the inner most regions, the algorithm first considers the region $BB3$, $BB4$, $BB5$, $BB6$ and $BB7$, and discards the edge coming from BB2 by duplicating BB6 into BB8. The $\phi$ becomes a move in the duplicated block with a renamed definition. The new $\phi$ operands are updated from the new edge. Note that in the implementation the block does not need to be created during this intermediate step because it will next be promoted into $BB3$. We have two nested hammocks and the process can be applied incrementally. The dependency that has been removed in the control flow is now expressed as a data dependency between the two $select$ instructions.
Naturally, any number of incoming edges into the duplicated basic block are allowed.

The algorithm to perform SSA basic block duplication is decomposed into:
\begin{itemize}
\item Extract the $\phi$s definitions to be conditionalized from the duplicated block creating a $move$ instruction and a new reduced $\phi$ (or two $move$ instructions if the duplicated block had only two incoming edges).
\item Then the $\phi$s in the tail basic block are augmented with the new definition created by the new repair instruction. If the $\phi$ was live-out after the tail block a move must be inserted to avoid propagating renaming outside of the region considered. 
\item The last step consists of renaming the new definitions to keep the region in SSA form.
\end{itemize}

\begin{figure}[h]
\centering
  \subfloat[Original] {
    \includegraphics[scale=0.75]{side1}
    \label{fig:side1}}
  \subfloat[After duplication] {
    \includegraphics[scale=0.75]{side2}
    \label{fig:side2}}
  \subfloat[inner branch] {
    \includegraphics[scale=0.75]{side3}
    \label{fig:side3}}
  \subfloat[outer branch] {
    \includegraphics[scale=0.75]{side4}
    \label{fig:side4}}
\caption{Side entry removal using basic block duplication}
\label{fig:bbdup}
\end{figure}

\subsection{Profitability}

Fusing execution paths can over commit the architectural ability to execute in parallel the multiple instructions: Data dependencies and register renaming introduce new register constraints. Moving operations earlier in the instruction stream increases live-ranges. 
Aggressive if-conversion can easily exceed the processor resources, leading to excessive register pressure or moving infrequently used long latencies instructions into the critical path. In a SSA incremental approach, the decision to if-convert or not the inner region will propagate recurrently to the outer regions.

The prevalent idea is that a region can be if-converted if the cost of the resulting if-converted basic block is smaller than the cost of each region taken separately weighted by the branch frequencies.

We compare the cost of the region before if-conversion. So the cost of a path is the schedule estimation of all the blocks in the path pondered by the execution profile:
\begin{align*}
Cost(path)=Freq(path)*\sum_{k=1}^n(Cost(bb_{k}))
\end{align*}
The cost of the region starting at basic block $head$ before if-conversion is therefore the cost of all the basic blocks in the considered region, on each path.
\begin{align*}
Cost(BBs_{before:\ ifc})=Cost(bb_{head}) + branchlat + Cost(bbs_{taken:\ path}) + Cost(bbs_{fall-though:\ path})
\end{align*}
The Cost after if-conversion is estimated with:
\begin{align*}
Cost(BB_{after:\ ifc})=Cost(bb_{head} \circ bb_{taken:\ path} \circ bb_{fall-though:\ path})
\end{align*}
Where $\circ$ is the composition function that merges basic blocks together, removes associated branches and creates the predicate operations. The resulting $Cost$ applied to the new basic block represents the estimated schedule after if-conversion, that will be effective only if
\begin{align*}
Cost(BBs_{before:\ ifc}) > Cost(BB_{after:\ ifc})
\end{align*}

The estimated cost of the if-converted region is the schedule height estimation of the instructions without the branches. When the schedule height of the if-converted region is smaller than the profiled estimation, then it is profitable.

The objective function needs the target machine description to derive the instruction latencies, resource usage and scheduling constraints. The local dependencies computed between instructions are used to compute the dependence height. The branch frequency is obtained either from static branch prediction heuristics, profile information or user inserted directives. Naturally, this heuristic can be either pessimistic, because it does not take into account new optimization opportunities introduced by the branch removals or explicit new dependencies, or optimistic because of bad register pressure estimation leading to register spilling on the critical path, or uncertainty in the branch prediction. But since the SSA incremental restructuring framework reduces the scope for the decision function to a localized part of the CFG, the size and complexity of the inner region under consideration makes the profitability a comprehensive process. Consequently, this cost function must be fast enough to be reapplied to each hammock during the incremental processing, with the advantage that all the instructions introduced by the if-conversion process in the inner regions, such as new predicate merging instructions or new temporary pseudo registers, will be accounted for in the decision to if-convert the regions encompassing them.

\section{Conclusion} 

We presented in this chapter how an if-conversion algorithm can take advantage of the SSA properties to efficiently assign predicates and lay out the new control flow in an incremental, inner-outer process. As opposed to the alternative top-down approach, the region selection can be reevaluated at each nested transformation, using local analysis.
Basic block selection and if-conversion are performed as a single process, hyperblocks being created lazily, using well known techniques such as tail-duplication or branch coalescing only when the benefit is established.
Predication and speculation are often presented as two different alternatives for if-conversion. While it is true than they both require different hardware support, they should coexist in an efficient if-conversion process such that every model of conditional execution is accepted. Thanks to conditional moves and $\psi$ transformations, they are now generated together in the same framework.

\section{Additional reading}

\cite{Rau:2003:IP:1074100.1074489}, exposes ILP in VLIW architectures using trace scheduling and local if-converted if-then-else regions using the $select$ and dismissible load operations. The idea behind was to enable the compiler to statically reorganize the instruction. In this respect, predictability \cite{Fisher:1992:PCB:143371.143493} becomes a major criteria for profitability.

To overcome the hard to predict profitability in conventional if-conversion algorithms, Reverse if-conversion was proposed in \cite{August:1999:PRI:326224.325595}, reconstructing the control flow at schedule time, after application of more aggressive region selection criteria.

Hyperblocks \cite{Mahlke:1992:ECS:144965.144998} was proposed as the primary if-converted scheduling framework, excluding basic blocks which do not justify their inclusion into the if-converted flow of control.

The duality between SSA like $\phi$s and predicate dependencies have been used in other works. In SSA-PS \cite{Jacome01clusteredvliw}, Predicated Switching operations are used to realize the conditional assignments using aggressive speculation techniques with conditional moves. Phi-Predication \cite{Chuang03phi-predicationfor} uses a modified version of the RK algorithm, to map phi-predication with phi-lists, holding guard and topological information. In both works, the use of SSA aims at solving the multiple definition problem exploiting variable renaming and join points, but they are based on speculation using conditional moves. 

In \cite{Stoutchinin_Gao_2004}, $\psi$ instructions are inserted while in SSA using a modified version of the classical Fang algorithm \cite{Fang:1996:CAI:645674.663446}, enabling support for a fully predicated ISA.
Those works established that standard if-conversion techniques can be applied to a SSA form using the $\psi$-SSA representation, or light weight $\phi$-SSA generation, but do not yet exploit the native SSA properties to build up the if-converted region.
A global SSA framework was presented \cite{odes_bruel} to support $select$ moves using aggressive speculation techniques, further extended to $\psi$-SSA \cite{ijes_bruel} allowing a mix of speculative and predicated techniques.

\cite{Mahlke95acomparison} evaluates how predicated operations can be performed using an equivalent sequences of speculative and conditional moves, starting from an if-converted region fully predicated. 










